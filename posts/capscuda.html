<!DOCTYPE html>
<html lang="en">

<head>

    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
    <meta name="description" content="">
    <meta name="author" content="">

    <title>Jos van de Wolfshaar - Blog</title>

    <!-- Bootstrap core CSS -->
    <link href="../vendor/bootstrap/css/bootstrap.min.css" rel="stylesheet">

    <!-- Custom fonts for this template -->
    <link href="../vendor/font-awesome/css/font-awesome.min.css" rel="stylesheet" type="text/css">
    <link href='https://fonts.googleapis.com/css?family=Lora:400,700,400italic,700italic' rel='stylesheet' type='text/css'>
    <link href='https://fonts.googleapis.com/css?family=Open+Sans:300italic,400italic,600italic,700italic,800italic,400,300,600,700,800' rel='stylesheet' type='text/css'>

    <!-- Custom styles for this template -->
    <link href="../css/clean-blog.min.css" rel="stylesheet">

</head>

<body>

<!-- Navigation -->
<nav class="navbar navbar-expand-lg navbar-light fixed-top" id="mainNav">
    <div class="container">
        <a class="navbar-brand" href="../index.html">Home</a>
        <button class="navbar-toggler navbar-toggler-right" type="button" data-toggle="collapse" data-target="#navbarResponsive" aria-controls="navbarResponsive" aria-expanded="false" aria-label="Toggle navigation">
            Menu
            <i class="fa fa-bars"></i>
        </button>
        <div class="collapse navbar-collapse" id="navbarResponsive">
            <ul class="navbar-nav ml-auto">
                <li class="nav-item">
                    <a class="nav-link" href="https://www.github.com/jostosh">GitHub</a>
                </li>
                <!--
                <li class="nav-item">
                    <a class="nav-link" href="../contact.html">Contact</a>
                </li>-->
            </ul>
        </div>
    </div>
</nav>


<!-- Page Header -->
<header class="masthead" style="background-image: url('../img/parallel.jpg')">
    <div class="overlay"></div>
    <div class="container">
        <div class="row">
            <div class="col-lg-10 col-md-10 mx-auto">
                <div class="post-heading">
                    <h1>TensorFlow custom Op with CUDA</h1>
                    <h2 class="subheading">A first look at GPU programming for TensorFlow.</h2>
                    <span class="meta">Posted
                on November 23, 2017. Latest edit: December 15, 2017.</span>
                </div>
            </div>
        </div>
    </div>
</header>

<!-- Post Content -->
<article>
    <div class="container">
        <div class="row">
            <div class="col-lg-10 col-md-10 mx-auto">
                <h2>
                    CUDA, TensorFlow and Capsule Networks
                </h2>
                <p>
                    Recently I've spent some time on CUDA programming and implementing custom Ops for TensorFlow. As an exercise, I decided to take a shot at implementing a custom Op for one of the operations in capsule networks that would normally require some <a href="capsnet.html">reshape hacking</a> or at least a <a href="https://github.com/Sarasra/models/tree/master/research/capsules">couple of intermediate TensorFlow Ops</a>. If you're not familiar with Capsule Networks, have a look at <a href="">the article</a> or my <a href="capsnet.html">previous post</a>.
                </p>
                <h2>
                Capsule matrix multiplication
                </h2>
                <p>
                The operation of interest in this blog post is the one that computes:
    $$ \hat{\boldsymbol u}_{j|i} = \boldsymbol W_{ij} \boldsymbol u_i, $$
                where $\hat{\boldsymbol u}_{j|i}$ is the activation vector of capsule $j$ as 'predicted' by capsule $i$ through the matrix vector multiplication $\boldsymbol W_{ij} \boldsymbol u_i$. The matrix $\boldsymbol W_{ij}$ is of shape <code>[out_dim, in_dim]</code> and the vector $\boldsymbol u_i$ denotes the output vector of capsule $i$. A single capsule layer computes this for all pairs $i,j$ and for all images in a batch. Hence, the tensors <code>W_ij</code> and <code>u_i</code> are actually of shape <code>[batch_size, in_caps, in_dim]</code> and <code>[in_caps, out_caps, out_dim, in_dim]</code> respectively. This means that we will build an op that just takes in these two tensors and computes an output tensor <code>u_hat_ji</code> of shape <code>[batch_size, in_caps, out_caps, out_dim]</code>. In other words, for all batch indices <code>[0,1,...,batch_size-1]</code> and for all combinations of in capsules <code>[0,1,...,in_caps-1]</code> and out capsules <code>[0,1,...,out_caps-1]</code> we have to compute a matrix-vector product.
                </p>

                <h2>
                TensorFlow kernel implementation
                </h2>
                <p>
                    Our custom Op will be most valuable if we implement it for a GPU. In the <a href="https://www.tensorflow.org/extend/adding_an_op">TensorFlow documentation</a>, you can find the necessary material to get you started on your own C++ kernels for TensorFlow. Then, you can read up on how to empower your algorithms with massively parallel GPU capabilities <a href="https://developer.nvidia.com/cuda-example">in this book</a> or <a href="https://devblogs.nvidia.com/even-easier-introduction-cuda/">online</a>. I will not repeat the details that you can find there, but I will provide a practical example that hopefully helps to understand how you can use CUDA for your own TensorFlow Ops. For me, getting to know some CUDA was easier than I thought, but squeezing out all performance can be tricky and I will leave further optimization of the kernel in this post for future work.
</p>
<h3>
    Op registration
</h3>
                <p>
                Let's do the forward pass of the Op. First, we will register the Op:
<pre><code class="cpp">
REGISTER_OP("CapsulePrediction")
.Input("input: T")
.Input("weights: T")
.Output("output: T")
.Attr("T: type")
.SetShapeFn([](InferenceContext* ctx) {
    // Get shapes and ensure correct dimensionality
    ShapeHandle in_shape;
    ShapeHandle weights_shape;
    TF_RETURN_IF_ERROR(ctx->WithRank(ctx->input(0), 3, &in_shape));
    TF_RETURN_IF_ERROR(ctx->WithRank(ctx->input(1), 4, &weights_shape));

    // Construct and set the output shape
    DimensionHandle out_d0, out_d1, out_d2, out_d3;
    std::vector&lt;DimensionHandle&gt; out_dims;
    out_dims.push_back(ctx->MakeDim(ctx->Dim(ctx->input(0), 0)));
    out_dims.push_back(ctx->MakeDim(ctx->Dim(ctx->input(1), 0)));
    out_dims.push_back(ctx->MakeDim(ctx->Dim(ctx->input(1), 1)));
    out_dims.push_back(ctx->MakeDim(ctx->Dim(ctx->input(1), 2)));
    ShapeHandle out_shape = ctx->MakeShape(out_dims);
    ctx->set_output(0, out_shape);

    return Status::OK();
});
</code></pre>
                For now, I have defined this so that we could later specify different TensorFlow kernels for different <code>dtype</code>s by adding the <code>"..: T"</code> specification. Other than that, the code implements a shape function that is given by the lambda function which first ensures <code>ctx-&gt;input(0)</code> (the input $\boldsymbol u_i$) and <code>ctx-&gt;input(1)</code> (the weights $\boldsymbol W_{ij}$) have the correct rank. Then we determine the dimensions of the output tensor which we can obtain from the input tensors. The dimension of the Op's output is <code>[batch_size, in_caps, out_caps, out_dim]</code>, so we take <code>batch_size</code> and <code>in_caps</code> from the $\boldsymbol u_i$ tensor and <code>out_caps</code> and <code>out_dim</code> from the $W_{ij}$ tensor.
                </p>
                <h3>
                    Forward capsule prediction
                </h3>
                <p>
                    Now, let's look at the Op's 'kernel'. A 'kernel' is the TensorFlow terminology for the device-specific implementation of an Op. When defining a custom kernel, it should inherit from TensorFlow's <code>OpKernel</code> and it should implement the <code>Compute</code> method:
                    <pre><code class="cpp">
class CapsulePredictionOp : public OpKernel
{
 public:
  explicit CapsulePredictionOp(OpKernelConstruction* ctx) : OpKernel(ctx) { }

  void Compute(OpKernelContext* ctx) override
  {
    // Get inputs
    const Tensor& input = ctx->input(0);
    const Tensor& weights = ctx->input(1);

    // Setup output shape
    const TensorShape& input_shape(input.shape());
    TensorShape output_shape(weights.shape());
    output_shape.InsertDim(0, input_shape.dim_size(0));
    output_shape.RemoveDim(4);

    // Allocate output tensor
    Tensor* output = nullptr;
    OP_REQUIRES_OK(ctx, ctx->allocate_output(0, output_shape, &output));

    // Get the Eigen tensors and pass them on the launcher
    auto input_tensor   = input.tensor&lt;float, 3&gt;();
    auto weights_tensor = weights.tensor&lt;float, 4&gt;();
    auto output_tensor  = output-&gt;tensor&lt;float, 4&gt;();
    launch(ctx-&gt;eigen_device<GPUDevice>(), input_tensor, weights_tensor,
      output_tensor);
  }
};
            </code></pre>
            In the implementation above we haven't done anything with CUDA yet, but we'll get there so don't worry. The code merely initializes the output shape from the input shapes and allocates the memory. The <code>OpKernelContext</code> object that is provided as a parameter makes sure to allocate the memory on the currently used device. In our case, this will be the GPU. Then we obtain the <code>Eigen</code> tensors through the <code>tensor</code> method and pass them on the the <code>launch</code> function, where the actual magic happens.
                </p>
                <h3>
                Launching the kernel
                </h3>
                <p>
                    The <code>launch</code> function literally (at least in CUDA terminology) launches code on the GPU. Perhaps a little confusing, but CUDA refers to functions that run code on the 'device' as <i>kernels</i>. In TensorFlow terminology, a kernel is not necessarily a GPU implementation, while in CUDA terminology it is. Let's not get too wrapped up in terminology and just get to the code:
                </p>
                <pre><code class="cpp">
void launch(
  const GPUDevice& d,
  typename TTypes&lt;float, 3&gt;::ConstTensor x,
  typename TTypes&lt;float, 4&gt;::ConstTensor weights,
  typename TTypes&lt;float, 4&gt;::Tensor out)
{
  // Get the dimensions
  const int64 batch_size  = x.dimension(0);
  const int64 in_caps     = x.dimension(1);
  const int64 in_dim      = x.dimension(2);
  const int64 out_dim     = weights.dimension(2);
  const int64 out_caps    = weights.dimension(1);

  // Size first dim
  const int64 w_d0 = out_caps * out_dim * in_dim;
  const int64 x_d0 = in_caps * in_dim;
  const int64 o_d0 = in_caps * out_caps * out_dim;

  // Second dim
  const int64 w_d1 = out_dim * in_dim;
  const int64 x_d1 = in_dim;
  const int64 o_d1 = out_caps * out_dim;

  // Third dim
  const int64 w_d2 = in_dim;
  const int64 o_d2 = out_dim;

  // Launch CUDA kernel for forward operation
  CudaLaunchConfig config = GetCudaLaunchConfig(out.size(), d);
  capsulePredictionKernel
    &lt;&lt;&lt;config.block_count, config.thread_per_block, 0, d.stream()&gt;&gt;&gt;(
      x.data(), weights.data(), out.data(),
      o_d0, o_d1, o_d2, x_d0, x_d1, w_d0, w_d1, w_d2,
      in_dim, out.size());
}
                </code></pre>
                <p>
The <code>TTypes</code> templates that you can see in the function arguments and the <code>int64</code> types are defined in the <code>tensorflow</code> namespace. The next part on the dimensions should be pretty self-explanatory. Because we are passing our arrays as one-dimensional arrays to the actual CUDA kernel, we need to figure out what the memory sizes are for each dimension and each kernel. Note that when I say 'memory sizes', I just refer to the number of floats for each axis and not the byte size. Let's consider the memory sizes of the first axis of each tensor:
                </p>
                <pre><code class="cpp">
// Size first dim
const int64 w_d0 = out_caps * out_dim * in_dim;
const int64 x_d0 = in_caps * in_dim;
const int64 o_d0 = in_caps * out_caps * out_dim;
                </code></pre>
                <p>
                Awesome, so we can simply get these using the dimensions we determined already. The code tells us that <code>w_d0</code> is just the product of <code>out_caps</code>, <code>out_dim</code> and <code>in_dim</code>. So if we want to jump from one index $\mathsf{W}_{\mathbf{i},j,k,l}$ to $\mathsf{W}_{\mathbf{i+1},j,k,l}$ we should add <code>w_d0</code> to the one-dimensional index. The same goes for index $j$ and <code>w_d1</code> as you might already expect. If you're familiar with Matlab, then it might help to remind you of the <code>ind2sub</code> function that concerns the very same thing.
                </p>
                <p>
                    The actual CUDA kernel launch is given at the bottom of the function and repeated here:
                </p>
                <pre><code>
// Launch CUDA kernel for forward operation
CudaLaunchConfig config = GetCudaLaunchConfig(out.size(), d);
capsulePredictionKernel
&lt;&lt;&lt;config.block_count, config.thread_per_block, 0, d.stream()&gt;&gt;&gt;(
  x.data(), weights.data(), out.data(),
  o_d0, o_d1, o_d2, x_d0, x_d1, w_d0, w_d1, w_d2,
  in_dim, out.size());
                </code></pre>
                <p>
                Both statements involve quite a few new concepts. The first statement uses a <code>GetCudaLaunchConfig</code> instance as a way to determine the number of <b>blocks</b> and the number of <b>threads</b> per block. The <code>cudaPredictionKernel</code> is the function that uses CUDA parallelism on the GPU It is launched by using the triple-fold delimiters: <code>&lt;&lt;&lt;config.block_count, config.thread_per_block, 0, d.stream()&gt;&gt;&gt;</code>. When you launch a kernel, you must specify the number of blocks and threads per block, as is done here. The third zero is not relevant for now and will often be zero if you were to implement your own kernels. The CUDA stream <code>d.stream()</code> is a separate pipeline of GPU instructions. Whenever you add your kernel to the stream, the stream will make sure the kernel ends before the next kernel on the stream is called. If you want to do two independent tasks in parallel, you could use two streams and launch one task on each.
                </p>
                <h3>
                    Threads and blocks
                </h3>
                <p>
                    All <b>blocks</b> that are assigned to a call can be run in parallel. If you launch a kernel with <code>N</code> blocks, then you could think of it as running <code>N</code> separate instances of the kernel function. That's pretty convenient! The <code>nvcc</code> compiler will make sure that the kernel function has access to the exact block index, so that the specific block-instance of the kernel knows which parts of the incoming arrays it should process.
                </p>
                <p>
                    A block can contain multiple <b>threads</b> itself. Threads are just an additional layer of parallelism, so they run in parallel. Why another layer of parallelism, you ask? Well, threads can do things that blocks cannot. Threads can share their memory which is typically useful when you want to use the same value of some input array in the same block multiple times. The shared memory access is much faster and it is one of the many ways you might optimize your final CUDA implementation.
                </p>
                <h3>
                    The CUDA kernel
                </h3>
                <p>
                    After a quick recap on threads and blocks in CUDA, we finally get to see the CUDA implementation of the forward capsule prediction:
                </p>
                <pre><code class="cpp">
__global__ void capsulePredictionKernel(
    const float* in, const float* weights, float* out,
    const int64 o_d0, const int64 o_d1, const int64 o_d2,
    const int64 x_d0, const int64 x_d1,
    const int64 w_d0, const int64 w_d1, const int64 w_d2,
    const int64 in_dim, const int64 output_size)
{
  CUDA_1D_KERNEL_LOOP(i, output_size)
  {
    // So here we have out[b,ci,cj,e]
    const int64 b     = i / o_d0;
    const int64 ci    = (i % o_d0) / o_d1;
    const int64 cj    = (i % o_d1) / o_d2;
    const int64 e_out  = i % o_d2;

    // Then, we can have a look at computing the array indices for in and W
    int64 in_idx = b * x_d0 + ci * x_d1;
    int64 w_idx = ci * w_d0 + cj * w_d1 + e_out * w_d2;

    // Initialize result
    float result = 0.0;
    for (int64 v = 0; v &lt; in_dim; ++v)
      // For both in and weights, the subsequent elements of the forward
      // computation are also subsequent in memory
      result += ldg(in + in_idx++) * ldg(weights + w_idx++);
    // Write result
    out[i] = result;
  }
}
                </code></pre>
                <p>
                The first thing you might notice is the <code>__global__</code> qualifier that precedes the function definition. This is what the <code>nvcc</code> compiler uses to make the function available <i>globally</i>, meaning that it can be launched from CPU. The arguments inherit their names from the <code>launch</code> function so they should not cause too much confusion. The <code>CUDA_1D_KERNEL_LOOP</code> is a macro defined in <code>tensorflow/core/util/cuda_kernel_helper.h</code>. It replaces this line of code with:
                </p>
                <pre><code class="cpp">
for (int i = blockIdx.x * blockDim.x + threadIdx.x; i &lt; output_size;
     i += blockDim.x * gridDim.x)
            </code></pre>
                <p>
                    This macro enforces us to think in a very convenient abstract way: it gives us some index <code>i</code> that correspond to the i-th element of the output array <code>out</code>. Now all we have to do is figure out what the indices are of our additional arrays <code>in</code> and <code>weights</code>. In order to do that, we determine the batch index <code>b</code>, the input capsule index <code>ci</code>, the output capsule index <code>cj</code> and the out capsule element index <code>e_out</code>:
                </p>
                <pre><code>
// So here we have out[b,ci,cj,e]
const int64 b     = i / o_d0;
const int64 ci    = (i % o_d0) / o_d1;
const int64 cj    = (i % o_d1) / o_d2;
const int64 e_out = i % o_d2;
                </code></pre>
                <p>
                    Determining these becomes easy once we know the number of elements contained in each axis. In fact, we have given the memory sizes as arguments to the function. For the other arrays, we can then convert <code>b</code>, <code>ci</code>, <code>cj</code> and <code>e_out</code> to their respective one-dimensional indices:
                </p>
                <pre><code class="cpp">
// Then, we can have a look at computing the array indices for in and W
int64 in_idx = b * x_d0 + ci * x_d1;
int64 w_idx  = ci * w_d0 + cj * w_d1 + e_out * w_d2;
            </code></pre>
                <p>
Again, we use the already provided memory sizes for each of the axes to get our one-dimensional indices. These are the indices for the <i>first input capsule element</i> of (i) input capsule <code>ci</code> at batch index <code>b</code> and (ii) the weights of the input capsule <code>ci</code>, the output capsule <code>cj</code> and the output capsule element <code>e_out</code>.
                </p>
                <p>
                    We assume that the last axis of both <code>in</code> and <code>W</code> are the input capsule elements. This means that they are consequent in memory and it is therefore straightforward to construct the loop that goes over the individual input capsule elements:
                </p>
                <pre><code class="cpp">
// Initialize result
float result = 0.0;
for (int64 v = 0; v < in_dim; ++v)
  // For both in and weights, the subsequent elements of the forward
  // computation are also subsequent in memory
  result += ldg(in + in_idx++) * ldg(weights + w_idx++);
// Write result
out[i] = result;
                </code></pre>
                <p>
                    The <code>ldg</code> function is a <a href="https://stackoverflow.com/questions/26603188/what-is-the-difference-between-ldg-intrinsic-and-a-normal-execution">Read-Only Data Cache Load Function</a>. It just receives a pointer to the actual element to read. Remember that we are computing matrix-vector products, which are just sets of inner products. A potential improvement here is to use <i>shared memory</i> since a single input capsule value is used many times, but we will leave out further optimization for now.
                </p>
                <p>
                    It is worth noting that this post assumes prior knowledge of TensorFlow and its most common operators. I should also add that it is really intended as supplementary material. There are quite some concepts and observations that I do not discuss here, but which you can easily acquaint by reading <a href="https://arxiv.org/abs/1710.09829">the article</a>.
                </p>

                <h2>
                    What is a capsule?
                </h2>
                <p>
                    Before we get to capsules, we will revisit a few terminological primers for neural networks. Usually, we say that the perceptrons in our neural networks represent features. For example, in image classification with deep convolutional neural networks, the neurons in the last few hidden layers typically represent abstract concepts, such as 'I see two eyes' and the 'the creature has two legs'. This interpretation is of course a bit anthropomorphistic. Nevertheless, it helps to get an intuitive understanding of what neural networks do when they interpret high-dimensional data. The presence of a certain feature in an image is reflected by a nonzero value of the corresponding hidden neuron. Again, with a healthy dose of anthropomorphism, we could say that the greater the neuron's activity, the more `confident' the neuron is of observing the feature.
                </p>

                <p>
                    Hinton and his team rethought the common approach of using perceptrons to build neural networks. Instead, they advocate the use of capsules. A single capsule is just a group of neurons. Just like regular neurons, capsules reside in layers. The numeric properties of neurons such as preactivations and outputs are represented by scalars. In contrast and perhaps not surprisingly, the same properties of capsules are represented by vectors. A single capsule now represents the presence of a feature in an image. In the paper, the word `entity' is used to indicate the same thing as what I refer to as a feature.
                </p>

                <p>
                    In classification, the class of a data instance can be seen as a special kind of entity. In most deep neural networks, class membership is encoded in the output layer as a set of probabilities. This is accomplished by a softmax layer that contains exactly one neuron per output class. In capsule networks, the presence of a class is encoded by a capsule. More specifically, the length of the activity vector of a capsule encodes the degree of confidence towards the presence of the class entity.
                </p>

                <p>
                    This means that capsule networks no longer use a softmax operator to obtain their output distribution. As far as I understand, capsule networks have no explicit notion of class probabilities. They rather encode entities, meaning that multiple entities can be present at the same time, which is exactly the property the authors use to separate the two overlapping digits in section 6 of the paper.
                </p>

                <h2>
                    The activation of a capsule
                </h2>

                <p>
                    Capsules have two ways of encoding information: length and direction. To be able to interpret the length of a capsule as a probability, it must be `squashed' so that the length is always between 0 and 1. This is accomplished with a squashing nonlinearity:
                    $$\boldsymbol v_j = {\|\boldsymbol s_j\|^2 \over 1 + \|\boldsymbol s_j\|^2}{\boldsymbol s_j \over \| \boldsymbol s_j \|},$$

                which squashes short vectors to near-zero length and long vectors to unit length.
                </p>

                <h2>
                    Multiple capsule layers
                </h2>

                <p>
                    Apart from the output layer, hidden layers might also be built up out of capsules. These capsules will represent simpler entities than class labels, e.g. pose, deformation, texture etc.
                </p>

                <p>
                    How to go from one capsule layer to another? In regular MLPs, the activation of a whole layer is represented as a vector. In capsule networks, the activation of a single capsule is represented as a vector. While ordinary MLPs can suffice with a single matrix-vector product to compute the preactivation of the next layer $l+1$, capsule layers need such a product for each pair of capsules between the two layers $(\boldsymbol v_i^{(l)}, \boldsymbol v_j^{(l+1)})$, where $l$ and $l+1$ are layer indices and $i$ and $j$ are capsule indices.
                </p>

                <p>
                    Following the terminology in the paper, the results of these matrix-vector products are seen as predictions of the output of the capsules in layer $l+1$. The predictions are linearly combined using coupling coefficients to form the current output. Initially, the coupling coefficients are equal and sum to 1. To compute a single forward pass in the network, the coefficients are re-estimated a few times by a process referred to as dynamic routing. This promotes the coupling coefficients between a capsule in $l$ to another in $l+1$ whenever the activity is predicted well. The degree to which these activities agree is determined through an inner product of the actual output and the prediction. As far as I understand, the routing mechanism more-or-less simplifies the interaction between the two layers by inhibiting many matrix-vector products and only promoting a few through a softmax procedure, similar to attention mechanisms as found in machine translation models using RNNs. After a few iterations of dynamic routing, these coupling coefficients converge to a situation where one could regard the higher level entities in $l+1$ to be encoded by only a few lower level entities in layer $l$. To really understand what's going on, I suggest you have a look at section 2 in the paper.
                </p>

                <h2>
                    Implementation in TensorFlow
                </h2>
                <p>
                    Perhaps a new kind of neural network is best explained by looking at a possible implementation. But first, let's have a look at the architecture we need to implement. We will be looking at the MNIST dataset. If you want to skip the explanations here and you just wanna get the code, see <a href="https://www.github.com/jostosh/capsnet">the repository</a>.
                </p>
                <h3>
                    Architecture
                </h3>
                <p>
                    We will have to combine 3 layers:
                </p>
                    <ul>
                <li>A regular convolutional layer with ReLU activations, stride 1, $9\times 9$ kernels and 256 filters </li>
                <li>A convolutional capsule layer with activations as described above, stride 2, $9\times 9$ kernels and 32 capsules per spatial output with a dimensionality of 8.</li>
                <li>A fully connected capsule layer with activations as described above, 10 capsules with a dimensionality of 16.</li>
            </ul>
                <h4>
                    Inputs and the first layer
                </h4>
                <p>
                Nowadays, TensorFlow is already shipped with predefined layers, let's save ourselves some time then.
                </p>
                <pre><code class="python">
# Define placeholders
x = tf.placeholder(tf.float32, [None, 28, 28, 1])
labels = tf.placeholder(tf.int64, [None])

# Define capsule network
conv1_out = tf.layers.conv2d(x, 256, 9, activation=tf.nn.relu)
pc = primary_caps(conv1_out, kernel_size=9, strides=(2, 2), capsules=32, dim=8)
v_j = digit_caps(pc, n_digit_caps=10, dim_digit_caps=16)
digit_norms = tf.norm(v_j, axis=-1)
                </code></pre>
<p>
    So we've set up our first convolutional layer and the placeholders that will contain externally provided labels and images.

</p>
                <h4>
                    Primary capsule layer
                </h4>
                <p>
                Good, now the interesting part. The first capsule layer is referred to as the primary capsule layer. It performs a convolution operation followed by the squashing nonlinearity as discussed above. We'll implement the layer as a function. In my implementation, I ended up with the following:
</p>
                <pre><code class="python">
def primary_caps(x, kernel_size, strides, capsules, dim, name="PrimaryCaps"):
    """ Primary capsule layer. Linear convolution with reshaping to account for capsules. """
    preactivation = tf.layers.conv2d(
        x, capsules * dim, kernel_size, strides=strides, activation=tf.identity, name=name,
        kernel_initializer=ConcatInitializer('glorot_uniform', axis=3, splits=capsules)
    )
    _, w, h, _ = preactivation.shape.as_list()
    out = tf.reshape(preactivation, (-1, w * h * capsules, dim))
    return squash(out)
                </code></pre>
                <p>
                    Let's break this down. The first statement to compute the preactivation seems obvious, but there is something peculiar going on. Note that we have put <code>capsules * dim</code> as the number of filters. This is because each capsule in this layer will have its vector along the depth dimension of the convolutional output volume. The width and height dimensions are only indices for capsules that focus on different parts of the input. Rather than computing '<code class>capsules</code>' convolutions separately, we do everything with a single convolution Op, which is simply more efficient.
                </p>
                <p>
                    The next statement is where we determine the spatial dimensions (the height and the width) of the output volume. The number of capsules in this would then be <code>h * w * capsules</code>. We can use this to reshape the output so that the new tensor has a shape of <code>[batch_size, n_capsules, capsule_dimension]</code>. We'll create a squashing function that assumes that the capsule elements are indexed on the last axis.
                </p>

                <h4>
                    Squashing function
                </h4>
                <p>
                    Our squashing function is straightforward given the equation as given in the paper.
                </p>
                <pre><code class="python">
def squash(s_j):
    """ Squashing function as given in the paper """
    squared_norms = tf.reduce_sum(tf.square(s_j), axis=-1, keep_dims=True)
    return s_j * squared_norms / (1 + squared_norms) / tf.sqrt(squared_norms + 1e-8)
                </code></pre>
                <p>
                    In the first statement, we compute the squared norms along the last axis of the incoming tensor. This works fine with our <code>primary caps</code> implementation as shown above, where we have reshaped the tensor so that the capsule's elements are indexed on the last axis. The second statement is a simply a translation from the earlier equation to code. Just to ensure numerical stability in case of small vectors, we add a nonzero constant to the denominator on the right.
                </p>
                <h4>
                    Digit capsule layer
                </h4>
                <p>
                    Building the next layer is the most difficult challenge of this reimplementation. In this layer, we have 10 capsules with dimensionality 16, one capsule for each digit. The main difficulty is in the computation of all predicted capsule vectors. In the paper, they express this as follows:
                    $$ \hat{\boldsymbol u}_{j|i} = \boldsymbol W_{ij} \boldsymbol u_i, $$
                    where the $i$ index corresponds to the capsules in the primary capsule layer and the $j$ index corresponds to capsules in the digit capsule layer. This means that we need <code>n_primary_capsules * n_digit_capsules</code> matrices. It's probably a good idea to have a weight tensor with rank 4, with shape <code>[n_primary_caps, n_digit_caps, dim_primary_caps, dim_digit_caps]</code>.
                </p>
                    <p>
                I have spent some time thinking of what might be the most efficient way of computing so many matrix vector products with TensorFlow. TensorFlow's <code>matmul</code> operator is intended for (batched) matrix multiplication. At first sight, our tensors $\boldsymbol W_{ij}$ and $\boldsymbol u_i$ don't have the right dimensions to do anything like that. However, I came up with some reshape-hacking that does the trick. Note that the result of a matrix multiplication would be a series of vectors. Each of these vectors would be the predicted activation of a digit capsule. So for a single batch, we would be able to compute the activations of all samples for a single index pair $(i, j)$ by having a matrix with <code>batch_size</code> columns and <code>dim_primary_caps</code> rows. This would be premultiplied by a matrix with <code>dim_digit_caps</code> rows and <code>dim_primary_caps</code> columns:
                        $$ \boldsymbol W_{ij} \left[\begin{array}{cccc} \boldsymbol u^{(1)}_i & \boldsymbol u^{(2)}_i & \cdots & \boldsymbol u^{(n)}_i \end{array}\right], $$
                        where the superscripts are the batch indices.
                    </p>
                <p>
                    Now it is easy to do the same thing for all $j$ and a single $i$. We can just use a block matrix where the individual matrices for all $j$ are stacked:
                    $$ \left[\begin{array}{c} \boldsymbol W_{i1}\\ \boldsymbol W_{i2} \\ \vdots \\ \boldsymbol W_{im} \end{array}\right] \left[\begin{array}{cccc} \boldsymbol u^{(1)}_i & \boldsymbol u^{(2)}_i & \cdots & \boldsymbol u^{(n)}_i \end{array}\right], $$
                    so that the result of the matrix vector product will contain the predicted activations for all $j$, but only a single $i$. I can hear you say it: what to do with the <code>n_primary_caps</code> dimension? Well, this dimension is both in the weight tensor, as well as in the primary caps output tensor. This can now be our 'batch' dimension for the <code>matmul</code> operator. This is how the above translates to code:
                </p>
                <pre><code class="python">
# Get number of capsules and dimensionality of previous layer
in_shape = incoming.shape.as_list()
n_primary_caps = in_shape[capsule_axis]
dim_primary_caps = in_shape[neuron_axis]
# Initialize all weight matrices
W_ij = tf.get_variable(
    "weights",
    shape=[n_primary_caps, n_digit_caps * dim_digit_caps, dim_primary_caps],
    initializer=glorot_uniform()
)
# Initialize routing logits, the leading axis with size 1 is added for
# convenience.
b_ij = tf.get_variable(
    "logits",
    shape=[1, n_primary_caps, n_digit_caps],
    initializer=tf.zeros_initializer(),
    trainable=args.logits_trainable
)
# Reshape and transpose hacking
u_i = tf.transpose(incoming, (1, 2, 0))
u_hat = tf.matmul(W_ij, u_i)
u_hat = tf.reshape(
    tf.transpose(u_hat, (2, 0, 1)),
    (-1, n_primary_caps, n_digit_caps, dim_digit_caps)
)
                </code></pre>
                <h5>
                    Dynamic routing
                </h5>
                <p>
                    Now that we have our $\hat{\boldsymbol u}$ tensor, we can perform dynamic routing. This is simply a matter of translating the pseudo-code in the paper to Python code, so that the <code>digit_caps</code> function can be implemented as follows:
                </p>
                <pre><code class="python">
def digit_caps(
    incoming, n_digit_caps, dim_digit_caps, name="DigitCaps",
    neuron_axis=-1, capsule_axis=-2, routing_iters=3
):
    """ Digit capsule layer """
    with tf.variable_scope(name):
        # Get number of capsules and dimensionality of previous layer
        in_shape = incoming.shape.as_list()
        n_primary_caps = in_shape[capsule_axis]
        dim_primary_caps = in_shape[neuron_axis]
        # Initialize all weight matrices
        W_ij = tf.get_variable(
            "weights",
            shape=[n_primary_caps, n_digit_caps * dim_digit_caps, dim_primary_caps],
            initializer=glorot_uniform()
        )
        # Initialize routing logits, the leading axis with size 1 is added for
        # convenience.
        b_ij = tf.get_variable(
            "logits",
            shape=[1, n_primary_caps, n_digit_caps],
            initializer=tf.zeros_initializer(),
            trainable=args.logits_trainable
        )
        # Reshape and transpose hacking
        u_i = tf.transpose(incoming, (1, 2, 0))
        u_hat = tf.matmul(W_ij, u_i)
        u_hat = tf.reshape(
            tf.transpose(u_hat, (2, 0, 1)),
            (-1, n_primary_caps, n_digit_caps, dim_digit_caps)
        )

        def capsule_out(b_ij):
            """ Given the logits b_ij, computes the output of this layer. """
            c_ij = tf.nn.softmax(b_ij, dim=2)
            s_j = tf.reduce_sum(
                tf.reshape(c_ij, (-1, n_primary_caps, n_digit_caps, 1)) * u_hat,
                axis=1
            )
            v_j = squash(s_j)
            return v_j

        def routing_iteration(iter, logits):
            """
            Given a set of logits, computes the new logits using the definition
            of routing from the paper.
            """
            v_j = capsule_out(logits)
            a_ij = tf.reduce_sum(tf.expand_dims(v_j, axis=1) * u_hat, axis=3)
            logits = tf.reshape(logits + a_ij, (-1, n_primary_caps, n_digit_caps))
            return [iter + 1, logits]

        # Compute routing
        i = tf.constant(0)
        routing_result = tf.while_loop(
            lambda i, logits: tf.less(i, routing_iters),
            routing_iteration,
            [i, tf.tile(b_ij, tf.stack([tf.shape(incoming)[0], 1, 1]))]
        )
        # Second element of the result contains our final logits
        v_j = capsule_out(routing_result[1])

    return v_j
                </code></pre>
                <p>
                    A relatively uncommon operator in this piece of code is the <code>while_loop</code>, which takes a looping condition, a function to execute in a single iteration and the initial values for the loop. The values given here are the index and the logits $b_{ij}$ as in the paper. Note that these arguments are passed to both the condition function as well as the loop body function.
                </p>
                <h2>
                    Results
                </h2>
                <p>
                    I thought it would be nice to look at trainable logits vs. constant logits. A comparison can be seen in the graph below. Note that this is only after a single training run, so we cannot reliably conclude anything from this yet. The accuracy seems to be convincingly high.

                <div>
                    <a href="https://plot.ly/~jostosh/1/?share_key=QkOrfTwcJZYHBnYtCRXrLE" target="_blank" title="Plot 1" style="display: block; text-align: center;"><img src="https://plot.ly/~jostosh/1.png?share_key=QkOrfTwcJZYHBnYtCRXrLE" alt="Plot 1" style="max-width: 100%;width: 600px;"  width="600" onerror="this.onerror=null;this.src='https://plot.ly/404.png';" /></a>
                    <script data-plotly="jostosh:1" sharekey-plotly="QkOrfTwcJZYHBnYtCRXrLE" src="https://plot.ly/embed.js" async></script>
                </div>

                </p>

                <h2>
                    Rounding up
                </h2>
                <p>
                    The full implementation is available in this <a href="https://github.com/jostosh/capsnet">repository</a>. Thanks for reading this and I hope you enjoyed! I am currently working on a commenting system so that you can ask questions right here soon. If you feel you can't wait, hit me at <code>jos dot vandewolfshaar at gmail dot com.</code>
                </p>
            </div>
        </div>
    </div>
</article>

<hr>

<!--
<div class="container">

    <div class="row">
        <div class="col-lg-8 col-md-10 mx-auto">
            <h2>Comments</h2>
            <!-- Contact Form - Enter your email address on line 19 of the mail/contact_me.php file to make this form work. -->
            <!-- WARNING: Some web hosts do not allow emails to be sent through forms to common mail hosts like Gmail or Yahoo. It's recommended that you use a private domain email address! -->
            <!-- To use the contact form, your site must be on a live web host with PHP! The form will not work locally! -->

<!--
            <form name="commentForm" id="contactForm" method="POST" action="https://api.staticman.net/v2/entry/jostosh/jostosh.github.io/master" novalidate>
                <input name="options[slug]" type="hidden" value="{{ page.slug }}">
                <div class="control-group">
                    <div class="form-group floating-label-form-group controls">
                        <label>Name</label>
                        <input type="text" class="form-control" placeholder="Name" id="name" name="fields[name]" required data-validation-required-message="Please enter your name.">
                        <p class="help-block text-danger"></p>
                    </div>
                </div>
                <div class="control-group">
                    <div class="form-group floating-label-form-group controls">
                        <label>Email Address</label>
                        <input type="email" class="form-control" placeholder="Email Address" id="email" name="fields[email]" required data-validation-required-message="Please enter your email address.">
                        <p class="help-block text-danger"></p>
                    </div>
                </div>
                <div class="control-group">
                    <div class="form-group floating-label-form-group controls">
                        <label>Message</label>
                        <textarea rows="5" class="form-control" placeholder="Message" id="message" name="fields[message]" required data-validation-required-message="Please enter a message."></textarea>
                        <p class="help-block text-danger"></p>
                    </div>
                </div>
                <br>
                <div id="success"></div>
                <div class="form-group">
                    <button type="submit" class="btn btn-primary" id="sendMessageButton">Send</button>
                </div>
            </form>
        </div>
    </div>
</div>

<!-- Footer -->
<footer>
    <div class="container">
        <div class="row">
            <div class="col-lg-8 col-md-10 mx-auto">
                <ul class="list-inline text-center">
                    <li class="list-inline-item">
                        <a href="https://www.linkedin.com/in/jos-van-de-wolfshaar-75950768/">
                  <span class="fa-stack fa-lg">
                    <i class="fa fa-circle fa-stack-2x"></i>
                    <i class="fa fa-linkedin fa-stack-1x fa-inverse"></i>
                  </span>
                        </a>
                    </li>
                    <li class="list-inline-item">
                        <a href="https://github.com/jostosh">
                  <span class="fa-stack fa-lg">
                    <i class="fa fa-circle fa-stack-2x"></i>
                    <i class="fa fa-github fa-stack-1x fa-inverse"></i>
                  </span>
                        </a>
                    </li>
                </ul>
                <p class="copyright text-muted">Jos van de Wolfshaar</p>
            </div>
        </div>
    </div>
</footer>

<!-- Bootstrap core JavaScript -->
<script src="../vendor/jquery/jquery.min.js"></script>
<script src="../vendor/bootstrap/js/bootstrap.bundle.min.js"></script>

<!-- Custom scripts for this template -->
<script src="../js/clean-blog.min.js"></script>

<script type="text/javascript" async
        src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/MathJax.js?config=TeX-MML-AM_CHTML">
</script>

<script type="text/x-mathjax-config">
MathJax.Hub.Config({
  TeX: { equationNumbers: { autoNumber: "AMS" }, extensions: ["AMSmath.js", "AMSsymbols.js"] },
  tex2jax: {
    inlineMath: [['$','$'], ['\\(','\\)']],
    processEscapes: true
  }
});
</script>

<link rel="stylesheet"
      href="//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/styles/default.min.css">
<script src="//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/highlight.min.js"></script>
<script>hljs.initHighlightingOnLoad();</script>

</body>

</html>
