{"data":{"site":{"siteMetadata":{"title":"Machine Learning Blog - JvdW","author":"Jos van de Wolfshaar"}},"markdownRemark":{"id":"82e1e556-6bc1-5bf8-9963-41cd9b94e491","excerpt":"This will hopefully be the first of many posts on various concepts in ML. I love working with on tensor-based models such as neural networks given the vast…","html":"<p>This will hopefully be the first of many posts on various concepts in ML. I love working with on tensor-based models such as neural networks given the vast amount of applications, incredible performance, the room for new ideas and an immense community around it.</p>\n<h2>Capsule Networks</h2>\n<p>In this post I will be looking at an interesting paper that was recently made available <a href=\"https://arxiv.org/abs/1710.09829\">on Arxiv</a>. This paper is co-authored by Geoffrey Hinton, a leading expert in the field of neural networks and deep learning who is currently employed at Google, Toronto. His recent paper on capsule networks has made a serious impression on the community.</p>\n<p>It is worth noting that this post assumes prior knowledge of TensorFlow and its most common operators. I should also add that it is really intended as supplementary material. There are quite some concepts and observations that I do not discuss here, but which you can easily acquaint by reading <a href=\"https://arxiv.org/abs/1710.09829\">the article</a>.</p>\n<h2>What is a capsule?</h2>\n<p>Before we get to capsules, we will revisit a few terminological primers for neural networks. Usually, we say that the perceptrons in our neural networks represent features. For example, in image classification with deep convolutional neural networks, the neurons in the last few hidden layers typically represent abstract concepts, such as ‘I see two eyes’ and the ‘the creature has two legs’. This interpretation is of course a bit anthropomorphistic. Nevertheless, it helps to get an intuitive understanding of what neural networks do when they interpret high-dimensional data. The presence of a certain feature in an image is reflected by a nonzero value of the corresponding hidden neuron. Again, with a healthy dose of anthropomorphism, we could say that the greater the neuron’s activity, the more `confident’ the neuron is of observing the feature.</p>\n<p>Hinton and his team rethought the common approach of using perceptrons to build neural networks. Instead, they advocate the use of capsules. A single capsule is just a group of neurons. Just like regular neurons, capsules reside in layers. The numeric properties of neurons such as preactivations and outputs are represented by scalars. In contrast and perhaps not surprisingly, the same properties of capsules are represented by vectors. A single capsule now represents the presence of a feature in an image. In the paper, the word `entity’ is used to indicate the same thing as what I refer to as a feature.</p>\n<p>In classification, the class of a data instance can be seen as a special kind of entity. In most deep neural networks, class membership is encoded in the output layer as a set of probabilities. This is accomplished by a softmax layer that contains exactly one neuron per output class. In capsule networks, the presence of a class is encoded by a capsule. More specifically, the length of the activity vector of a capsule encodes the degree of confidence towards the presence of the class entity.</p>\n<p>This means that capsule networks no longer use a softmax operator to obtain their output distribution. As far as I understand, capsule networks have no explicit notion of class probabilities. They rather encode entities, meaning that multiple entities can be present at the same time, which is exactly the property the authors use to separate the two overlapping digits in section 6 of the paper.</p>\n<h2>The Activation Of A Capsule</h2>\n<p>Capsules have two ways of encoding information: length and direction. To be able to interpret the length of a capsule as a probability, it must be `squashed’ so that the length is always between 0 and 1. This is accomplished with a squashing nonlinearity:</p>\n<span class=\"katex-display\"><span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><msub><mi mathvariant=\"bold-italic\">v</mi><mi>j</mi></msub><mo>=</mo><mfrac><mrow><mi mathvariant=\"normal\">∥</mi><msub><mi mathvariant=\"bold-italic\">s</mi><mi>j</mi></msub><msup><mi mathvariant=\"normal\">∥</mi><mn>2</mn></msup></mrow><mrow><mn>1</mn><mo>+</mo><mi mathvariant=\"normal\">∥</mi><msub><mi mathvariant=\"bold-italic\">s</mi><mi>j</mi></msub><msup><mi mathvariant=\"normal\">∥</mi><mn>2</mn></msup></mrow></mfrac><mfrac><msub><mi mathvariant=\"bold-italic\">s</mi><mi>j</mi></msub><mrow><mi mathvariant=\"normal\">∥</mi><msub><mi mathvariant=\"bold-italic\">s</mi><mi>j</mi></msub><mi mathvariant=\"normal\">∥</mi></mrow></mfrac></mrow><annotation encoding=\"application/x-tex\">\\boldsymbol v_j = \\frac{\\| \\boldsymbol s_j \\|^2}{1 + \\| \\boldsymbol s_j \\|^2} \\frac{\\boldsymbol s_j}{\\| \\boldsymbol s_j \\|}</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.730548em;vertical-align:-0.286108em;\"></span><span class=\"mord\"><span class=\"mord\"><span class=\"mord boldsymbol\" style=\"margin-right:0.03704em;\">v</span></span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.311664em;\"><span style=\"top:-2.5500000000000003em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mathdefault mtight\" style=\"margin-right:0.05724em;\">j</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.286108em;\"><span></span></span></span></span></span></span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span><span class=\"mrel\">=</span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:2.463216em;vertical-align:-0.972108em;\"></span><span class=\"mord\"><span class=\"mopen nulldelimiter\"></span><span class=\"mfrac\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:1.4911079999999999em;\"><span style=\"top:-2.314em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span class=\"mord\"><span class=\"mord\">1</span><span class=\"mspace\" style=\"margin-right:0.2222222222222222em;\"></span><span class=\"mbin\">+</span><span class=\"mspace\" style=\"margin-right:0.2222222222222222em;\"></span><span class=\"mord\">∥</span><span class=\"mord\"><span class=\"mord\"><span class=\"mord boldsymbol\">s</span></span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.311664em;\"><span style=\"top:-2.5500000000000003em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mathdefault mtight\" style=\"margin-right:0.05724em;\">j</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.286108em;\"><span></span></span></span></span></span></span><span class=\"mord\"><span class=\"mord\">∥</span><span class=\"msupsub\"><span class=\"vlist-t\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.740108em;\"><span style=\"top:-2.9890000000000003em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\">2</span></span></span></span></span></span></span></span></span></span><span style=\"top:-3.23em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span class=\"frac-line\" style=\"border-bottom-width:0.04em;\"></span></span><span style=\"top:-3.677em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span class=\"mord\"><span class=\"mord\">∥</span><span class=\"mord\"><span class=\"mord\"><span class=\"mord boldsymbol\">s</span></span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.311664em;\"><span style=\"top:-2.5500000000000003em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mathdefault mtight\" style=\"margin-right:0.05724em;\">j</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.286108em;\"><span></span></span></span></span></span></span><span class=\"mord\"><span class=\"mord\">∥</span><span class=\"msupsub\"><span class=\"vlist-t\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.8141079999999999em;\"><span style=\"top:-3.063em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\">2</span></span></span></span></span></span></span></span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.972108em;\"><span></span></span></span></span></span><span class=\"mclose nulldelimiter\"></span></span><span class=\"mord\"><span class=\"mopen nulldelimiter\"></span><span class=\"mfrac\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:1.12144em;\"><span style=\"top:-2.314em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span class=\"mord\"><span class=\"mord\">∥</span><span class=\"mord\"><span class=\"mord\"><span class=\"mord boldsymbol\">s</span></span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.311664em;\"><span style=\"top:-2.5500000000000003em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mathdefault mtight\" style=\"margin-right:0.05724em;\">j</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.286108em;\"><span></span></span></span></span></span></span><span class=\"mord\">∥</span></span></span><span style=\"top:-3.23em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span class=\"frac-line\" style=\"border-bottom-width:0.04em;\"></span></span><span style=\"top:-3.677em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span class=\"mord\"><span class=\"mord\"><span class=\"mord\"><span class=\"mord boldsymbol\">s</span></span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.311664em;\"><span style=\"top:-2.5500000000000003em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mathdefault mtight\" style=\"margin-right:0.05724em;\">j</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.286108em;\"><span></span></span></span></span></span></span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.972108em;\"><span></span></span></span></span></span><span class=\"mclose nulldelimiter\"></span></span></span></span></span></span>\n<p>which squashes short vectors to near-zero length and long vectors to unit length.</p>\n<h2>Multiple Capsule Layers</h2>\n<p>Apart from the output layer, hidden layers might also be built up out of capsules. These capsules will represent simpler entities than class labels, e.g. pose, deformation, texture etc.</p>\n<p>How to go from one capsule layer to another? In regular MLPs, the activation of a whole layer is represented as a vector. In capsule networks, the activation of a single capsule is represented as a vector. While ordinary MLPs can suffice with a single matrix-vector product to compute the preactivation of the next layer <span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mi>l</mi><mo>+</mo><mn>1</mn></mrow><annotation encoding=\"application/x-tex\">l+1</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.77777em;vertical-align:-0.08333em;\"></span><span class=\"mord mathdefault\" style=\"margin-right:0.01968em;\">l</span><span class=\"mspace\" style=\"margin-right:0.2222222222222222em;\"></span><span class=\"mbin\">+</span><span class=\"mspace\" style=\"margin-right:0.2222222222222222em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:0.64444em;vertical-align:0em;\"></span><span class=\"mord\">1</span></span></span></span>, capsule layers need such a product for each pair of capsules between the two layers <span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mo stretchy=\"false\">(</mo><msubsup><mi mathvariant=\"bold-italic\">v</mi><mi>j</mi><mrow><mo stretchy=\"false\">(</mo><mi>l</mi><mo stretchy=\"false\">)</mo></mrow></msubsup><mo separator=\"true\">,</mo><msubsup><mi mathvariant=\"bold-italic\">v</mi><mi>j</mi><mrow><mo stretchy=\"false\">(</mo><mi>l</mi><mo>+</mo><mn>1</mn><mo stretchy=\"false\">)</mo></mrow></msubsup><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">(\\boldsymbol v_j ^{(l)}, \\boldsymbol v_j^{(l+1)})</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:1.4577719999999998em;vertical-align:-0.4129719999999999em;\"></span><span class=\"mopen\">(</span><span class=\"mord\"><span class=\"mord\"><span class=\"mord boldsymbol\" style=\"margin-right:0.03704em;\">v</span></span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:1.0448em;\"><span style=\"top:-2.4231360000000004em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mathdefault mtight\" style=\"margin-right:0.05724em;\">j</span></span></span><span style=\"top:-3.2198em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\"><span class=\"mopen mtight\">(</span><span class=\"mord mathdefault mtight\" style=\"margin-right:0.01968em;\">l</span><span class=\"mclose mtight\">)</span></span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.4129719999999999em;\"><span></span></span></span></span></span></span><span class=\"mpunct\">,</span><span class=\"mspace\" style=\"margin-right:0.16666666666666666em;\"></span><span class=\"mord\"><span class=\"mord\"><span class=\"mord boldsymbol\" style=\"margin-right:0.03704em;\">v</span></span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:1.0448em;\"><span style=\"top:-2.4231360000000004em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mathdefault mtight\" style=\"margin-right:0.05724em;\">j</span></span></span><span style=\"top:-3.2198em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\"><span class=\"mopen mtight\">(</span><span class=\"mord mathdefault mtight\" style=\"margin-right:0.01968em;\">l</span><span class=\"mbin mtight\">+</span><span class=\"mord mtight\">1</span><span class=\"mclose mtight\">)</span></span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.4129719999999999em;\"><span></span></span></span></span></span></span><span class=\"mclose\">)</span></span></span></span>, where <span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mi>l</mi></mrow><annotation encoding=\"application/x-tex\">l</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.69444em;vertical-align:0em;\"></span><span class=\"mord mathdefault\" style=\"margin-right:0.01968em;\">l</span></span></span></span> and <span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mi>l</mi><mo>+</mo><mn>1</mn></mrow><annotation encoding=\"application/x-tex\">l + 1</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.77777em;vertical-align:-0.08333em;\"></span><span class=\"mord mathdefault\" style=\"margin-right:0.01968em;\">l</span><span class=\"mspace\" style=\"margin-right:0.2222222222222222em;\"></span><span class=\"mbin\">+</span><span class=\"mspace\" style=\"margin-right:0.2222222222222222em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:0.64444em;vertical-align:0em;\"></span><span class=\"mord\">1</span></span></span></span> are layer indices and <span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mi>i</mi></mrow><annotation encoding=\"application/x-tex\">i</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.65952em;vertical-align:0em;\"></span><span class=\"mord mathdefault\">i</span></span></span></span> and <span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mi>j</mi></mrow><annotation encoding=\"application/x-tex\">j</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.85396em;vertical-align:-0.19444em;\"></span><span class=\"mord mathdefault\" style=\"margin-right:0.05724em;\">j</span></span></span></span> are capsule indices.</p>\n<p>Following the terminology in the paper, the results of these matrix-vector products are seen as predictions of the output of the capsules in layer <span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mi>l</mi><mo>+</mo><mn>1</mn></mrow><annotation encoding=\"application/x-tex\">l+1</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.77777em;vertical-align:-0.08333em;\"></span><span class=\"mord mathdefault\" style=\"margin-right:0.01968em;\">l</span><span class=\"mspace\" style=\"margin-right:0.2222222222222222em;\"></span><span class=\"mbin\">+</span><span class=\"mspace\" style=\"margin-right:0.2222222222222222em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:0.64444em;vertical-align:0em;\"></span><span class=\"mord\">1</span></span></span></span>. The predictions are linearly combined using coupling coefficients to form the current output. Initially, the coupling coefficients are equal and sum to 1. To compute a single forward pass in the network, the coefficients are re-estimated a few times by a process referred to as dynamic routing. This promotes the coupling coefficients between a capsule in <span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mi>l</mi></mrow><annotation encoding=\"application/x-tex\">l</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.69444em;vertical-align:0em;\"></span><span class=\"mord mathdefault\" style=\"margin-right:0.01968em;\">l</span></span></span></span> to another in <span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mi>l</mi><mo>+</mo><mn>1</mn></mrow><annotation encoding=\"application/x-tex\">l+1</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.77777em;vertical-align:-0.08333em;\"></span><span class=\"mord mathdefault\" style=\"margin-right:0.01968em;\">l</span><span class=\"mspace\" style=\"margin-right:0.2222222222222222em;\"></span><span class=\"mbin\">+</span><span class=\"mspace\" style=\"margin-right:0.2222222222222222em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:0.64444em;vertical-align:0em;\"></span><span class=\"mord\">1</span></span></span></span> whenever the activity is predicted well. The degree to which these activities agree is determined through an inner product of the actual output and the prediction. As far as I understand, the routing mechanism more-or-less simplifies the interaction between the two layers by inhibiting many matrix-vector products and only promoting a few through a softmax procedure, similar to attention mechanisms as found in machine translation models using RNNs. After a few iterations of dynamic routing, these coupling coefficients converge to a situation where one could regard the higher level entities in <span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mi>l</mi><mo>+</mo><mn>1</mn></mrow><annotation encoding=\"application/x-tex\">l+1</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.77777em;vertical-align:-0.08333em;\"></span><span class=\"mord mathdefault\" style=\"margin-right:0.01968em;\">l</span><span class=\"mspace\" style=\"margin-right:0.2222222222222222em;\"></span><span class=\"mbin\">+</span><span class=\"mspace\" style=\"margin-right:0.2222222222222222em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:0.64444em;vertical-align:0em;\"></span><span class=\"mord\">1</span></span></span></span> to be encoded by only a few lower level entities in layer <span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mi>l</mi></mrow><annotation encoding=\"application/x-tex\">l</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.69444em;vertical-align:0em;\"></span><span class=\"mord mathdefault\" style=\"margin-right:0.01968em;\">l</span></span></span></span>. To gain a deeper understanding of what’s going on, I suggest you have a look at section 2 in the paper.</p>\n<h2>Implementation In TensorFlow</h2>\n<p>Perhaps a new kind of neural network is best explained by looking at a possible implementation. But first, let’s have a look at the architecture we need to implement. We will be looking at the MNIST dataset. If you want to skip the explanations here and you just wanna get the code, see the <a href=\"https://www.github.com/jostosh/capsnet\">repository</a>.</p>\n<h3>Architecture</h3>\n<p>We will have to combine 3 layers:</p>\n<ul>\n<li>A regular convolutional layer with ReLU activations, stride 1, <span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mn>9</mn><mo>×</mo><mn>9</mn></mrow><annotation encoding=\"application/x-tex\">9\\times 9</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.72777em;vertical-align:-0.08333em;\"></span><span class=\"mord\">9</span><span class=\"mspace\" style=\"margin-right:0.2222222222222222em;\"></span><span class=\"mbin\">×</span><span class=\"mspace\" style=\"margin-right:0.2222222222222222em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:0.64444em;vertical-align:0em;\"></span><span class=\"mord\">9</span></span></span></span> kernels and 256 filters</li>\n<li>A convolutional capsule layer with activations as described above, stride 2, <span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mn>9</mn><mo>×</mo><mn>9</mn></mrow><annotation encoding=\"application/x-tex\">9\\times 9</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.72777em;vertical-align:-0.08333em;\"></span><span class=\"mord\">9</span><span class=\"mspace\" style=\"margin-right:0.2222222222222222em;\"></span><span class=\"mbin\">×</span><span class=\"mspace\" style=\"margin-right:0.2222222222222222em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:0.64444em;vertical-align:0em;\"></span><span class=\"mord\">9</span></span></span></span> kernels kernels and 32 capsules per spatial output with a dimensionality of 8.</li>\n<li>A fully connected capsule layer with activations as described above, 10 capsules with a dimensionality of 16.</li>\n</ul>\n<h3>Inputs And The First Layer</h3>\n<p>Nowadays, TensorFlow is already shipped with predefined layers, let’s save ourselves some time then.</p>\n<div class=\"gatsby-highlight\" data-language=\"python\"><pre class=\"language-python\"><code class=\"language-python\"><span class=\"token comment\"># Define placeholders</span>\nx <span class=\"token operator\">=</span> tf<span class=\"token punctuation\">.</span>placeholder<span class=\"token punctuation\">(</span>tf<span class=\"token punctuation\">.</span>float32<span class=\"token punctuation\">,</span> <span class=\"token punctuation\">[</span><span class=\"token boolean\">None</span><span class=\"token punctuation\">,</span> <span class=\"token number\">28</span><span class=\"token punctuation\">,</span> <span class=\"token number\">28</span><span class=\"token punctuation\">,</span> <span class=\"token number\">1</span><span class=\"token punctuation\">]</span><span class=\"token punctuation\">)</span>\nlabels <span class=\"token operator\">=</span> tf<span class=\"token punctuation\">.</span>placeholder<span class=\"token punctuation\">(</span>tf<span class=\"token punctuation\">.</span>int64<span class=\"token punctuation\">,</span> <span class=\"token punctuation\">[</span><span class=\"token boolean\">None</span><span class=\"token punctuation\">]</span><span class=\"token punctuation\">)</span>\n\n<span class=\"token comment\"># Define capsule network</span>\nconv1_out <span class=\"token operator\">=</span> tf<span class=\"token punctuation\">.</span>layers<span class=\"token punctuation\">.</span>conv2d<span class=\"token punctuation\">(</span>x<span class=\"token punctuation\">,</span> <span class=\"token number\">256</span><span class=\"token punctuation\">,</span> <span class=\"token number\">9</span><span class=\"token punctuation\">,</span> activation<span class=\"token operator\">=</span>tf<span class=\"token punctuation\">.</span>nn<span class=\"token punctuation\">.</span>relu<span class=\"token punctuation\">)</span>\npc <span class=\"token operator\">=</span> primary_caps<span class=\"token punctuation\">(</span>conv1_out<span class=\"token punctuation\">,</span> kernel_size<span class=\"token operator\">=</span><span class=\"token number\">9</span><span class=\"token punctuation\">,</span> strides<span class=\"token operator\">=</span><span class=\"token punctuation\">(</span><span class=\"token number\">2</span><span class=\"token punctuation\">,</span> <span class=\"token number\">2</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">,</span> capsules<span class=\"token operator\">=</span><span class=\"token number\">32</span><span class=\"token punctuation\">,</span> dim<span class=\"token operator\">=</span><span class=\"token number\">8</span><span class=\"token punctuation\">)</span>\nv_j <span class=\"token operator\">=</span> digit_caps<span class=\"token punctuation\">(</span>pc<span class=\"token punctuation\">,</span> n_digit_caps<span class=\"token operator\">=</span><span class=\"token number\">10</span><span class=\"token punctuation\">,</span> dim_digit_caps<span class=\"token operator\">=</span><span class=\"token number\">16</span><span class=\"token punctuation\">)</span>\ndigit_norms <span class=\"token operator\">=</span> tf<span class=\"token punctuation\">.</span>norm<span class=\"token punctuation\">(</span>v_j<span class=\"token punctuation\">,</span> axis<span class=\"token operator\">=</span><span class=\"token operator\">-</span><span class=\"token number\">1</span><span class=\"token punctuation\">)</span></code></pre></div>\n<p>So we’ve set up our first convolutional layer and the placeholders that will contain externally provided labels and images.</p>\n<h3>Primary Capsule Layer</h3>\n<p>Good, now the interesting part. The first capsule layer is referred to as the primary capsule layer. It performs a convolution operation followed by the squashing nonlinearity as discussed above. We’ll implement the layer as a function. In my implementation, I ended up with the following:</p>\n<div class=\"gatsby-highlight\" data-language=\"python\"><pre class=\"language-python\"><code class=\"language-python\"><span class=\"token keyword\">def</span> <span class=\"token function\">primary_caps</span><span class=\"token punctuation\">(</span>x<span class=\"token punctuation\">,</span> kernel_size<span class=\"token punctuation\">,</span> strides<span class=\"token punctuation\">,</span> capsules<span class=\"token punctuation\">,</span> dim<span class=\"token punctuation\">,</span> name<span class=\"token operator\">=</span><span class=\"token string\">\"PrimaryCaps\"</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">:</span>\n    <span class=\"token triple-quoted-string string\">\"\"\" Primary capsule layer. Linear convolution with reshaping to account for capsules. \"\"\"</span>\n    preactivation <span class=\"token operator\">=</span> tf<span class=\"token punctuation\">.</span>layers<span class=\"token punctuation\">.</span>conv2d<span class=\"token punctuation\">(</span>\n        x<span class=\"token punctuation\">,</span> capsules <span class=\"token operator\">*</span> dim<span class=\"token punctuation\">,</span> kernel_size<span class=\"token punctuation\">,</span> strides<span class=\"token operator\">=</span>strides<span class=\"token punctuation\">,</span> activation<span class=\"token operator\">=</span>tf<span class=\"token punctuation\">.</span>identity<span class=\"token punctuation\">,</span> name<span class=\"token operator\">=</span>name<span class=\"token punctuation\">,</span>\n        kernel_initializer<span class=\"token operator\">=</span>ConcatInitializer<span class=\"token punctuation\">(</span><span class=\"token string\">'glorot_uniform'</span><span class=\"token punctuation\">,</span> axis<span class=\"token operator\">=</span><span class=\"token number\">3</span><span class=\"token punctuation\">,</span> splits<span class=\"token operator\">=</span>capsules<span class=\"token punctuation\">)</span>\n    <span class=\"token punctuation\">)</span>\n    _<span class=\"token punctuation\">,</span> w<span class=\"token punctuation\">,</span> h<span class=\"token punctuation\">,</span> _ <span class=\"token operator\">=</span> preactivation<span class=\"token punctuation\">.</span>shape<span class=\"token punctuation\">.</span>as_list<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span>\n    out <span class=\"token operator\">=</span> tf<span class=\"token punctuation\">.</span>reshape<span class=\"token punctuation\">(</span>preactivation<span class=\"token punctuation\">,</span> <span class=\"token punctuation\">(</span><span class=\"token operator\">-</span><span class=\"token number\">1</span><span class=\"token punctuation\">,</span> w <span class=\"token operator\">*</span> h <span class=\"token operator\">*</span> capsules<span class=\"token punctuation\">,</span> dim<span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span>\n    <span class=\"token keyword\">return</span> squash<span class=\"token punctuation\">(</span>out<span class=\"token punctuation\">)</span></code></pre></div>\n<p>Let’s break this down. The first statement to compute the preactivation seems obvious, but there is something peculiar going on. Note that we have put <code class=\"language-text\">capsules * dim</code> as the number of filters. This is because each capsule in this layer will have its vector along the depth dimension of the convolutional output volume. The width and height dimensions are only indices for capsules that focus on different parts of the input. Rather than computing <code class=\"language-text\">#capsules</code> convolutions separately, we do everything with a single convolution Op, which is simply more efficient.</p>\n<p>The next statement is where we determine the spatial dimensions (the height and the width) of the output volume. The number of capsules in this would then be <code class=\"language-text\">h * w * capsules</code>. We can use this to reshape the output so that the new tensor has a shape of <code class=\"language-text\">[batch_size, n_capsules, capsule_dimension]</code>. We’ll create a squashing function that assumes that the capsule elements are indexed on the last axis.</p>\n<h3>Squashing Function</h3>\n<p>Our squashing function is straightforward given the equation from the paper.</p>\n<div class=\"gatsby-highlight\" data-language=\"python\"><pre class=\"language-python\"><code class=\"language-python\"><span class=\"token keyword\">def</span> <span class=\"token function\">squash</span><span class=\"token punctuation\">(</span>s_j<span class=\"token punctuation\">)</span><span class=\"token punctuation\">:</span>\n    <span class=\"token triple-quoted-string string\">\"\"\" Squashing function as given in the paper \"\"\"</span>\n    squared_norms <span class=\"token operator\">=</span> tf<span class=\"token punctuation\">.</span>reduce_sum<span class=\"token punctuation\">(</span>tf<span class=\"token punctuation\">.</span>square<span class=\"token punctuation\">(</span>s_j<span class=\"token punctuation\">)</span><span class=\"token punctuation\">,</span> axis<span class=\"token operator\">=</span><span class=\"token operator\">-</span><span class=\"token number\">1</span><span class=\"token punctuation\">,</span> keep_dims<span class=\"token operator\">=</span><span class=\"token boolean\">True</span><span class=\"token punctuation\">)</span>\n    <span class=\"token keyword\">return</span> s_j <span class=\"token operator\">*</span> squared_norms <span class=\"token operator\">/</span> <span class=\"token punctuation\">(</span><span class=\"token number\">1</span> <span class=\"token operator\">+</span> squared_norms<span class=\"token punctuation\">)</span> <span class=\"token operator\">/</span> tf<span class=\"token punctuation\">.</span>sqrt<span class=\"token punctuation\">(</span>squared_norms <span class=\"token operator\">+</span> <span class=\"token number\">1e</span><span class=\"token operator\">-</span><span class=\"token number\">8</span><span class=\"token punctuation\">)</span></code></pre></div>\n<p>In the first statement, we compute the squared norms along the last axis of the incoming tensor. This works fine with our <code class=\"language-text\">primary caps</code> implementation as shown above, where we have reshaped the tensor so that the capsule’s elements are indexed on the last axis. The second statement is a simply a translation from the earlier equation to code. Just to ensure numerical stability in case of small vectors, we add a nonzero constant to the denominator on the right.</p>\n<h3>Digit Capsule Layer</h3>\n<p>Building the next layer is the most difficult part of this reimplementation. In this layer, we have 10 capsules with dimensionality 16, one capsule for each digit. The main difficulty is in the computation of all predicted capsule vectors. In the paper, they express this as follows:</p>\n<span class=\"katex-display\"><span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><msub><mover accent=\"true\"><mi mathvariant=\"bold-italic\">u</mi><mo>^</mo></mover><mrow><mi>j</mi><mi mathvariant=\"normal\">∣</mi><mi>i</mi></mrow></msub><mo>=</mo><msub><mi mathvariant=\"bold-italic\">W</mi><mrow><mi>i</mi><mi>j</mi></mrow></msub><msub><mi mathvariant=\"bold-italic\">u</mi><mi>i</mi></msub></mrow><annotation encoding=\"application/x-tex\">\\hat{\\boldsymbol u}_{j|i} = \\boldsymbol W_{ij} \\boldsymbol u_i</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:1.0630799999999998em;vertical-align:-0.3551999999999999em;\"></span><span class=\"mord\"><span class=\"mord accent\"><span class=\"vlist-t\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.70788em;\"><span style=\"top:-3em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span class=\"mord\"><span class=\"mord\"><span class=\"mord boldsymbol\">u</span></span></span></span><span style=\"top:-3.01344em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span class=\"accent-body\" style=\"left:-0.25em;\">^</span></span></span></span></span></span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.34480000000000005em;\"><span style=\"top:-2.5198em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\"><span class=\"mord mathdefault mtight\" style=\"margin-right:0.05724em;\">j</span><span class=\"mord mtight\">∣</span><span class=\"mord mathdefault mtight\">i</span></span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.3551999999999999em;\"><span></span></span></span></span></span></span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span><span class=\"mrel\">=</span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:0.972218em;vertical-align:-0.286108em;\"></span><span class=\"mord\"><span class=\"mord\"><span class=\"mord boldsymbol\" style=\"margin-right:0.15972em;\">W</span></span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.311664em;\"><span style=\"top:-2.5500000000000003em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\"><span class=\"mord mathdefault mtight\">i</span><span class=\"mord mathdefault mtight\" style=\"margin-right:0.05724em;\">j</span></span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.286108em;\"><span></span></span></span></span></span></span><span class=\"mord\"><span class=\"mord\"><span class=\"mord boldsymbol\">u</span></span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.31166399999999994em;\"><span style=\"top:-2.5500000000000003em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mathdefault mtight\">i</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span></span></span></span></span>\n<p>where the <span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mi>i</mi></mrow><annotation encoding=\"application/x-tex\">i</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.65952em;vertical-align:0em;\"></span><span class=\"mord mathdefault\">i</span></span></span></span> index corresponds to the capsules in the primary capsule layer and the <span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mi>j</mi></mrow><annotation encoding=\"application/x-tex\">j</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.85396em;vertical-align:-0.19444em;\"></span><span class=\"mord mathdefault\" style=\"margin-right:0.05724em;\">j</span></span></span></span> index corresponds to capsules in the digit capsule layer. This means that we need <code class=\"language-text\">n_primary_capsules * n_digit_capsules</code> matrices. It’s probably a good idea to have a weight tensor with rank 4, with shape <code class=\"language-text\">[n_primary_caps, n_digit_caps, dim_primary_caps, dim_digit_caps]</code>.</p>\n<p>I have spent some time thinking of what might be the most efficient way of computing so many matrix vector products with TensorFlow. TensorFlow’s matmul operator is intended for (batched) matrix multiplication. At first sight, our tensors <code class=\"language-text\">W_{ij}</code>and <span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><msub><mi mathvariant=\"bold-italic\">u</mi><mi>i</mi></msub></mrow><annotation encoding=\"application/x-tex\">\\boldsymbol u_i</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.59444em;vertical-align:-0.15em;\"></span><span class=\"mord\"><span class=\"mord\"><span class=\"mord boldsymbol\">u</span></span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.31166399999999994em;\"><span style=\"top:-2.5500000000000003em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mathdefault mtight\">i</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span></span></span></span> don’t have the right dimensions to do anything like that. However, we can do some reshape hacking that does the trick. Note that the result of a matrix multiplication would be a series of vectors. Each of these vectors would be the predicted activation of a digit capsule. So for a single batch, we would be able to compute the activations of all samples for a single index pair <span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mo stretchy=\"false\">(</mo><mi>i</mi><mo separator=\"true\">,</mo><mi>j</mi><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">(i,j)</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:1em;vertical-align:-0.25em;\"></span><span class=\"mopen\">(</span><span class=\"mord mathdefault\">i</span><span class=\"mpunct\">,</span><span class=\"mspace\" style=\"margin-right:0.16666666666666666em;\"></span><span class=\"mord mathdefault\" style=\"margin-right:0.05724em;\">j</span><span class=\"mclose\">)</span></span></span></span> by having a matrix with <code class=\"language-text\">batch_size</code> columns and <code class=\"language-text\">dim_primary_caps</code> rows. This would be premultiplied by a matrix with <code class=\"language-text\">dim_digit_caps</code> rows and <code class=\"language-text\">dim_primary_caps</code> columns:</p>\n<span class=\"katex-display\"><span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><msub><mi mathvariant=\"bold-italic\">W</mi><mrow><mi>i</mi><mi>j</mi></mrow></msub><mrow><mo fence=\"true\">[</mo><mtable rowspacing=\"0.15999999999999992em\" columnalign=\"center center center center\" columnspacing=\"1em\"><mtr><mtd><mstyle scriptlevel=\"0\" displaystyle=\"false\"><msubsup><mi mathvariant=\"bold-italic\">u</mi><mi>i</mi><mrow><mo stretchy=\"false\">(</mo><mn>1</mn><mo stretchy=\"false\">)</mo></mrow></msubsup></mstyle></mtd><mtd><mstyle scriptlevel=\"0\" displaystyle=\"false\"><msubsup><mi mathvariant=\"bold-italic\">u</mi><mi>i</mi><mrow><mo stretchy=\"false\">(</mo><mn>2</mn><mo stretchy=\"false\">)</mo></mrow></msubsup></mstyle></mtd><mtd><mstyle scriptlevel=\"0\" displaystyle=\"false\"><mo>⋯</mo></mstyle></mtd><mtd><mstyle scriptlevel=\"0\" displaystyle=\"false\"><msubsup><mi mathvariant=\"bold-italic\">u</mi><mi>i</mi><mrow><mo stretchy=\"false\">(</mo><mi>n</mi><mo stretchy=\"false\">)</mo></mrow></msubsup></mstyle></mtd></mtr></mtable><mo fence=\"true\">]</mo></mrow></mrow><annotation encoding=\"application/x-tex\">\\boldsymbol W_{ij} \\left[\\begin{array}{cccc} \\boldsymbol u^{(1)}_i &amp; \\boldsymbol u^{(2)}_i &amp; \\cdots &amp; \\boldsymbol u^{(n)}_i \\end{array}\\right]</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:1.80002em;vertical-align:-0.65002em;\"></span><span class=\"mord\"><span class=\"mord\"><span class=\"mord boldsymbol\" style=\"margin-right:0.15972em;\">W</span></span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.311664em;\"><span style=\"top:-2.5500000000000003em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\"><span class=\"mord mathdefault mtight\">i</span><span class=\"mord mathdefault mtight\" style=\"margin-right:0.05724em;\">j</span></span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.286108em;\"><span></span></span></span></span></span></span><span class=\"mspace\" style=\"margin-right:0.16666666666666666em;\"></span><span class=\"minner\"><span class=\"mopen delimcenter\" style=\"top:0em;\"><span class=\"delimsizing size2\">[</span></span><span class=\"mord\"><span class=\"mtable\"><span class=\"arraycolsep\" style=\"width:0.5em;\"></span><span class=\"col-align-c\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.9524000000000001em;\"><span style=\"top:-2.9524em;\"><span class=\"pstrut\" style=\"height:3.0448em;\"></span><span class=\"mord\"><span class=\"mord\"><span class=\"mord\"><span class=\"mord boldsymbol\">u</span></span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:1.0448em;\"><span style=\"top:-2.4231360000000004em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mathdefault mtight\">i</span></span></span><span style=\"top:-3.2198em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\"><span class=\"mopen mtight\">(</span><span class=\"mord mtight\">1</span><span class=\"mclose mtight\">)</span></span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.27686399999999994em;\"><span></span></span></span></span></span></span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.45239999999999997em;\"><span></span></span></span></span></span><span class=\"arraycolsep\" style=\"width:0.5em;\"></span><span class=\"arraycolsep\" style=\"width:0.5em;\"></span><span class=\"col-align-c\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.9524000000000001em;\"><span style=\"top:-2.9524em;\"><span class=\"pstrut\" style=\"height:3.0448em;\"></span><span class=\"mord\"><span class=\"mord\"><span class=\"mord\"><span class=\"mord boldsymbol\">u</span></span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:1.0448em;\"><span style=\"top:-2.4231360000000004em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mathdefault mtight\">i</span></span></span><span style=\"top:-3.2198em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\"><span class=\"mopen mtight\">(</span><span class=\"mord mtight\">2</span><span class=\"mclose mtight\">)</span></span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.27686399999999994em;\"><span></span></span></span></span></span></span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.45239999999999997em;\"><span></span></span></span></span></span><span class=\"arraycolsep\" style=\"width:0.5em;\"></span><span class=\"arraycolsep\" style=\"width:0.5em;\"></span><span class=\"col-align-c\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.9524000000000001em;\"><span style=\"top:-2.9524em;\"><span class=\"pstrut\" style=\"height:3.0448em;\"></span><span class=\"mord\"><span class=\"minner\">⋯</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.45239999999999997em;\"><span></span></span></span></span></span><span class=\"arraycolsep\" style=\"width:0.5em;\"></span><span class=\"arraycolsep\" style=\"width:0.5em;\"></span><span class=\"col-align-c\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.9524000000000001em;\"><span style=\"top:-2.9524em;\"><span class=\"pstrut\" style=\"height:3.0448em;\"></span><span class=\"mord\"><span class=\"mord\"><span class=\"mord\"><span class=\"mord boldsymbol\">u</span></span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:1.0448em;\"><span style=\"top:-2.4231360000000004em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mathdefault mtight\">i</span></span></span><span style=\"top:-3.2198em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\"><span class=\"mopen mtight\">(</span><span class=\"mord mathdefault mtight\">n</span><span class=\"mclose mtight\">)</span></span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.27686399999999994em;\"><span></span></span></span></span></span></span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.45239999999999997em;\"><span></span></span></span></span></span><span class=\"arraycolsep\" style=\"width:0.5em;\"></span></span></span><span class=\"mclose delimcenter\" style=\"top:0em;\"><span class=\"delimsizing size2\">]</span></span></span></span></span></span></span>\n<p>where the superscripts are the batch indices.</p>\n<p>Now it is easy to do the same thing for all <span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mi>j</mi></mrow><annotation encoding=\"application/x-tex\">j</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.85396em;vertical-align:-0.19444em;\"></span><span class=\"mord mathdefault\" style=\"margin-right:0.05724em;\">j</span></span></span></span> and a single <span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mi>i</mi></mrow><annotation encoding=\"application/x-tex\">i</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.65952em;vertical-align:0em;\"></span><span class=\"mord mathdefault\">i</span></span></span></span>. We can just use a block matrix where the individual matrices for all <span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mi>j</mi></mrow><annotation encoding=\"application/x-tex\">j</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.85396em;vertical-align:-0.19444em;\"></span><span class=\"mord mathdefault\" style=\"margin-right:0.05724em;\">j</span></span></span></span> are stacked:</p>\n<span class=\"katex-display\"><span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mrow><mo fence=\"true\">[</mo><mtable rowspacing=\"0.15999999999999992em\" columnalign=\"center\" columnspacing=\"1em\"><mtr><mtd><mstyle scriptlevel=\"0\" displaystyle=\"false\"><msub><mi mathvariant=\"bold-italic\">W</mi><mrow><mi>i</mi><mn>1</mn></mrow></msub></mstyle></mtd></mtr><mtr><mtd><mstyle scriptlevel=\"0\" displaystyle=\"false\"><msub><mi mathvariant=\"bold-italic\">W</mi><mrow><mi>i</mi><mn>2</mn></mrow></msub></mstyle></mtd></mtr><mtr><mtd><mstyle scriptlevel=\"0\" displaystyle=\"false\"><mi mathvariant=\"normal\">⋮</mi><mpadded height=\"+0em\" voffset=\"0em\"><mspace mathbackground=\"black\" width=\"0em\" height=\"1.5em\"></mspace></mpadded></mstyle></mtd></mtr><mtr><mtd><mstyle scriptlevel=\"0\" displaystyle=\"false\"><msub><mi mathvariant=\"bold-italic\">W</mi><mrow><mi>i</mi><mi>m</mi></mrow></msub></mstyle></mtd></mtr></mtable><mo fence=\"true\">]</mo></mrow><mrow><mo fence=\"true\">[</mo><mtable rowspacing=\"0.15999999999999992em\" columnalign=\"center center center center\" columnspacing=\"1em\"><mtr><mtd><mstyle scriptlevel=\"0\" displaystyle=\"false\"><msubsup><mi mathvariant=\"bold-italic\">u</mi><mi>i</mi><mrow><mo stretchy=\"false\">(</mo><mn>1</mn><mo stretchy=\"false\">)</mo></mrow></msubsup></mstyle></mtd><mtd><mstyle scriptlevel=\"0\" displaystyle=\"false\"><msubsup><mi mathvariant=\"bold-italic\">u</mi><mi>i</mi><mrow><mo stretchy=\"false\">(</mo><mn>2</mn><mo stretchy=\"false\">)</mo></mrow></msubsup></mstyle></mtd><mtd><mstyle scriptlevel=\"0\" displaystyle=\"false\"><mo>⋯</mo></mstyle></mtd><mtd><mstyle scriptlevel=\"0\" displaystyle=\"false\"><msubsup><mi mathvariant=\"bold-italic\">u</mi><mi>i</mi><mrow><mo stretchy=\"false\">(</mo><mi>n</mi><mo stretchy=\"false\">)</mo></mrow></msubsup></mstyle></mtd></mtr></mtable><mo fence=\"true\">]</mo></mrow></mrow><annotation encoding=\"application/x-tex\">\\left[\\begin{array}{c} \\boldsymbol W_{i1}\\\\ \\boldsymbol W_{i2} \\\\ \\vdots \\\\ \\boldsymbol W_{im} \\end{array}\\right] \\left[\\begin{array}{cccc} \\boldsymbol u^{(1)}_i &amp; \\boldsymbol u^{(2)}_i &amp; \\cdots &amp; \\boldsymbol u^{(n)}_i \\end{array}\\right]</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:5.459999999999999em;vertical-align:-2.4799999999999995em;\"></span><span class=\"minner\"><span class=\"mopen\"><span class=\"delimsizing mult\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:2.953005em;\"><span style=\"top:-1.3499850000000007em;\"><span class=\"pstrut\" style=\"height:3.1550000000000002em;\"></span><span class=\"delimsizinginner delim-size4\"><span>⎣</span></span></span><span style=\"top:-2.5049850000000005em;\"><span class=\"pstrut\" style=\"height:3.1550000000000002em;\"></span><span class=\"delimsizinginner delim-size4\"><span>⎢</span></span></span><span style=\"top:-3.1059850000000004em;\"><span class=\"pstrut\" style=\"height:3.1550000000000002em;\"></span><span class=\"delimsizinginner delim-size4\"><span>⎢</span></span></span><span style=\"top:-3.7069850000000004em;\"><span class=\"pstrut\" style=\"height:3.1550000000000002em;\"></span><span class=\"delimsizinginner delim-size4\"><span>⎢</span></span></span><span style=\"top:-4.953005em;\"><span class=\"pstrut\" style=\"height:3.1550000000000002em;\"></span><span class=\"delimsizinginner delim-size4\"><span>⎡</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:2.4500349999999997em;\"><span></span></span></span></span></span></span><span class=\"mord\"><span class=\"mtable\"><span class=\"arraycolsep\" style=\"width:0.5em;\"></span><span class=\"col-align-c\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:2.9799999999999995em;\"><span style=\"top:-5.8275em;\"><span class=\"pstrut\" style=\"height:3.6875em;\"></span><span class=\"mord\"><span class=\"mord\"><span class=\"mord\"><span class=\"mord boldsymbol\" style=\"margin-right:0.15972em;\">W</span></span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.31166399999999994em;\"><span style=\"top:-2.5500000000000003em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\"><span class=\"mord mathdefault mtight\">i</span><span class=\"mord mtight\">1</span></span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span></span></span><span style=\"top:-4.6275em;\"><span class=\"pstrut\" style=\"height:3.6875em;\"></span><span class=\"mord\"><span class=\"mord\"><span class=\"mord\"><span class=\"mord boldsymbol\" style=\"margin-right:0.15972em;\">W</span></span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.31166399999999994em;\"><span style=\"top:-2.5500000000000003em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\"><span class=\"mord mathdefault mtight\">i</span><span class=\"mord mtight\">2</span></span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span></span></span><span style=\"top:-2.7674999999999996em;\"><span class=\"pstrut\" style=\"height:3.6875em;\"></span><span class=\"mord\"><span class=\"mord\"><span class=\"mord\">⋮</span><span class=\"mord rule\" style=\"border-right-width:0em;border-top-width:1.5em;bottom:0em;\"></span></span></span></span><span style=\"top:-1.5675000000000006em;\"><span class=\"pstrut\" style=\"height:3.6875em;\"></span><span class=\"mord\"><span class=\"mord\"><span class=\"mord\"><span class=\"mord boldsymbol\" style=\"margin-right:0.15972em;\">W</span></span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.31166399999999994em;\"><span style=\"top:-2.5500000000000003em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\"><span class=\"mord mathdefault mtight\">i</span><span class=\"mord mathdefault mtight\">m</span></span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:2.4799999999999995em;\"><span></span></span></span></span></span><span class=\"arraycolsep\" style=\"width:0.5em;\"></span></span></span><span class=\"mclose\"><span class=\"delimsizing mult\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:2.953005em;\"><span style=\"top:-1.3499850000000007em;\"><span class=\"pstrut\" style=\"height:3.1550000000000002em;\"></span><span class=\"delimsizinginner delim-size4\"><span>⎦</span></span></span><span style=\"top:-2.5049850000000005em;\"><span class=\"pstrut\" style=\"height:3.1550000000000002em;\"></span><span class=\"delimsizinginner delim-size4\"><span>⎥</span></span></span><span style=\"top:-3.1059850000000004em;\"><span class=\"pstrut\" style=\"height:3.1550000000000002em;\"></span><span class=\"delimsizinginner delim-size4\"><span>⎥</span></span></span><span style=\"top:-3.7069850000000004em;\"><span class=\"pstrut\" style=\"height:3.1550000000000002em;\"></span><span class=\"delimsizinginner delim-size4\"><span>⎥</span></span></span><span style=\"top:-4.953005em;\"><span class=\"pstrut\" style=\"height:3.1550000000000002em;\"></span><span class=\"delimsizinginner delim-size4\"><span>⎤</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:2.4500349999999997em;\"><span></span></span></span></span></span></span></span><span class=\"mspace\" style=\"margin-right:0.16666666666666666em;\"></span><span class=\"minner\"><span class=\"mopen delimcenter\" style=\"top:0em;\"><span class=\"delimsizing size2\">[</span></span><span class=\"mord\"><span class=\"mtable\"><span class=\"arraycolsep\" style=\"width:0.5em;\"></span><span class=\"col-align-c\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.9524000000000001em;\"><span style=\"top:-2.9524em;\"><span class=\"pstrut\" style=\"height:3.0448em;\"></span><span class=\"mord\"><span class=\"mord\"><span class=\"mord\"><span class=\"mord boldsymbol\">u</span></span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:1.0448em;\"><span style=\"top:-2.4231360000000004em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mathdefault mtight\">i</span></span></span><span style=\"top:-3.2198em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\"><span class=\"mopen mtight\">(</span><span class=\"mord mtight\">1</span><span class=\"mclose mtight\">)</span></span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.27686399999999994em;\"><span></span></span></span></span></span></span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.45239999999999997em;\"><span></span></span></span></span></span><span class=\"arraycolsep\" style=\"width:0.5em;\"></span><span class=\"arraycolsep\" style=\"width:0.5em;\"></span><span class=\"col-align-c\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.9524000000000001em;\"><span style=\"top:-2.9524em;\"><span class=\"pstrut\" style=\"height:3.0448em;\"></span><span class=\"mord\"><span class=\"mord\"><span class=\"mord\"><span class=\"mord boldsymbol\">u</span></span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:1.0448em;\"><span style=\"top:-2.4231360000000004em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mathdefault mtight\">i</span></span></span><span style=\"top:-3.2198em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\"><span class=\"mopen mtight\">(</span><span class=\"mord mtight\">2</span><span class=\"mclose mtight\">)</span></span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.27686399999999994em;\"><span></span></span></span></span></span></span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.45239999999999997em;\"><span></span></span></span></span></span><span class=\"arraycolsep\" style=\"width:0.5em;\"></span><span class=\"arraycolsep\" style=\"width:0.5em;\"></span><span class=\"col-align-c\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.9524000000000001em;\"><span style=\"top:-2.9524em;\"><span class=\"pstrut\" style=\"height:3.0448em;\"></span><span class=\"mord\"><span class=\"minner\">⋯</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.45239999999999997em;\"><span></span></span></span></span></span><span class=\"arraycolsep\" style=\"width:0.5em;\"></span><span class=\"arraycolsep\" style=\"width:0.5em;\"></span><span class=\"col-align-c\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.9524000000000001em;\"><span style=\"top:-2.9524em;\"><span class=\"pstrut\" style=\"height:3.0448em;\"></span><span class=\"mord\"><span class=\"mord\"><span class=\"mord\"><span class=\"mord boldsymbol\">u</span></span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:1.0448em;\"><span style=\"top:-2.4231360000000004em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mathdefault mtight\">i</span></span></span><span style=\"top:-3.2198em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\"><span class=\"mopen mtight\">(</span><span class=\"mord mathdefault mtight\">n</span><span class=\"mclose mtight\">)</span></span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.27686399999999994em;\"><span></span></span></span></span></span></span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.45239999999999997em;\"><span></span></span></span></span></span><span class=\"arraycolsep\" style=\"width:0.5em;\"></span></span></span><span class=\"mclose delimcenter\" style=\"top:0em;\"><span class=\"delimsizing size2\">]</span></span></span></span></span></span></span>\n<p>so that the result of the matrix vector product will contain the predicted activations for all <span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mi>j</mi></mrow><annotation encoding=\"application/x-tex\">j</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.85396em;vertical-align:-0.19444em;\"></span><span class=\"mord mathdefault\" style=\"margin-right:0.05724em;\">j</span></span></span></span>, but only a single <span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mi>i</mi></mrow><annotation encoding=\"application/x-tex\">i</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.65952em;vertical-align:0em;\"></span><span class=\"mord mathdefault\">i</span></span></span></span>. I can hear you say it: what to do with the <code class=\"language-text\">n_primary_caps</code> dimension? Well, this dimension is both in the weight tensor, as well as in the primary caps output tensor. This can now be our ‘batch’ dimension for the matmul operator. This is how the above translates to code:</p>\n<div class=\"gatsby-highlight\" data-language=\"python\"><pre class=\"language-python\"><code class=\"language-python\"><span class=\"token comment\"># Get number of capsules and dimensionality of previous layer</span>\nin_shape <span class=\"token operator\">=</span> incoming<span class=\"token punctuation\">.</span>shape<span class=\"token punctuation\">.</span>as_list<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span>\nn_primary_caps <span class=\"token operator\">=</span> in_shape<span class=\"token punctuation\">[</span>capsule_axis<span class=\"token punctuation\">]</span>\ndim_primary_caps <span class=\"token operator\">=</span> in_shape<span class=\"token punctuation\">[</span>neuron_axis<span class=\"token punctuation\">]</span>\n<span class=\"token comment\"># Initialize all weight matrices</span>\nW_ij <span class=\"token operator\">=</span> tf<span class=\"token punctuation\">.</span>get_variable<span class=\"token punctuation\">(</span>\n    <span class=\"token string\">\"weights\"</span><span class=\"token punctuation\">,</span>\n    shape<span class=\"token operator\">=</span><span class=\"token punctuation\">[</span>n_primary_caps<span class=\"token punctuation\">,</span> n_digit_caps <span class=\"token operator\">*</span> dim_digit_caps<span class=\"token punctuation\">,</span> dim_primary_caps<span class=\"token punctuation\">]</span><span class=\"token punctuation\">,</span>\n    initializer<span class=\"token operator\">=</span>glorot_uniform<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span>\n<span class=\"token punctuation\">)</span>\n<span class=\"token comment\"># Initialize routing logits, the leading axis with size 1 is added for</span>\n<span class=\"token comment\"># convenience.</span>\nb_ij <span class=\"token operator\">=</span> tf<span class=\"token punctuation\">.</span>get_variable<span class=\"token punctuation\">(</span>\n    <span class=\"token string\">\"logits\"</span><span class=\"token punctuation\">,</span>\n    shape<span class=\"token operator\">=</span><span class=\"token punctuation\">[</span><span class=\"token number\">1</span><span class=\"token punctuation\">,</span> n_primary_caps<span class=\"token punctuation\">,</span> n_digit_caps<span class=\"token punctuation\">]</span><span class=\"token punctuation\">,</span>\n    initializer<span class=\"token operator\">=</span>tf<span class=\"token punctuation\">.</span>zeros_initializer<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">,</span>\n    trainable<span class=\"token operator\">=</span>args<span class=\"token punctuation\">.</span>logits_trainable\n<span class=\"token punctuation\">)</span>\n<span class=\"token comment\"># Reshape and transpose hacking</span>\nu_i <span class=\"token operator\">=</span> tf<span class=\"token punctuation\">.</span>transpose<span class=\"token punctuation\">(</span>incoming<span class=\"token punctuation\">,</span> <span class=\"token punctuation\">(</span><span class=\"token number\">1</span><span class=\"token punctuation\">,</span> <span class=\"token number\">2</span><span class=\"token punctuation\">,</span> <span class=\"token number\">0</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span>\nu_hat <span class=\"token operator\">=</span> tf<span class=\"token punctuation\">.</span>matmul<span class=\"token punctuation\">(</span>W_ij<span class=\"token punctuation\">,</span> u_i<span class=\"token punctuation\">)</span>\nu_hat <span class=\"token operator\">=</span> tf<span class=\"token punctuation\">.</span>reshape<span class=\"token punctuation\">(</span>\n    tf<span class=\"token punctuation\">.</span>transpose<span class=\"token punctuation\">(</span>u_hat<span class=\"token punctuation\">,</span> <span class=\"token punctuation\">(</span><span class=\"token number\">2</span><span class=\"token punctuation\">,</span> <span class=\"token number\">0</span><span class=\"token punctuation\">,</span> <span class=\"token number\">1</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">,</span>\n    <span class=\"token punctuation\">(</span><span class=\"token operator\">-</span><span class=\"token number\">1</span><span class=\"token punctuation\">,</span> n_primary_caps<span class=\"token punctuation\">,</span> n_digit_caps<span class=\"token punctuation\">,</span> dim_digit_caps<span class=\"token punctuation\">)</span>\n<span class=\"token punctuation\">)</span></code></pre></div>\n<h3>Dynamic Routing</h3>\n<p>Now that we have our <span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mover accent=\"true\"><mi mathvariant=\"bold-italic\">u</mi><mo>^</mo></mover></mrow><annotation encoding=\"application/x-tex\">\\hat{\\boldsymbol u}</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.70788em;vertical-align:0em;\"></span><span class=\"mord accent\"><span class=\"vlist-t\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.70788em;\"><span style=\"top:-3em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span class=\"mord\"><span class=\"mord\"><span class=\"mord boldsymbol\">u</span></span></span></span><span style=\"top:-3.01344em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span class=\"accent-body\" style=\"left:-0.25em;\">^</span></span></span></span></span></span></span></span></span> tensor, we can perform dynamic routing. This is simply a matter of translating the pseudo-code in the paper to Python code, so that the <code class=\"language-text\">digit_caps</code> function can be implemented as follows:</p>\n<div class=\"gatsby-highlight\" data-language=\"python\"><pre class=\"language-python\"><code class=\"language-python\"><span class=\"token keyword\">def</span> <span class=\"token function\">digit_caps</span><span class=\"token punctuation\">(</span>\n    incoming<span class=\"token punctuation\">,</span> n_digit_caps<span class=\"token punctuation\">,</span> dim_digit_caps<span class=\"token punctuation\">,</span> name<span class=\"token operator\">=</span><span class=\"token string\">\"DigitCaps\"</span><span class=\"token punctuation\">,</span>\n    neuron_axis<span class=\"token operator\">=</span><span class=\"token operator\">-</span><span class=\"token number\">1</span><span class=\"token punctuation\">,</span> capsule_axis<span class=\"token operator\">=</span><span class=\"token operator\">-</span><span class=\"token number\">2</span><span class=\"token punctuation\">,</span> routing_iters<span class=\"token operator\">=</span><span class=\"token number\">3</span>\n<span class=\"token punctuation\">)</span><span class=\"token punctuation\">:</span>\n    <span class=\"token triple-quoted-string string\">\"\"\" Digit capsule layer \"\"\"</span>\n    <span class=\"token keyword\">with</span> tf<span class=\"token punctuation\">.</span>variable_scope<span class=\"token punctuation\">(</span>name<span class=\"token punctuation\">)</span><span class=\"token punctuation\">:</span>\n        <span class=\"token comment\"># Get number of capsules and dimensionality of previous layer</span>\n        in_shape <span class=\"token operator\">=</span> incoming<span class=\"token punctuation\">.</span>shape<span class=\"token punctuation\">.</span>as_list<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span>\n        n_primary_caps <span class=\"token operator\">=</span> in_shape<span class=\"token punctuation\">[</span>capsule_axis<span class=\"token punctuation\">]</span>\n        dim_primary_caps <span class=\"token operator\">=</span> in_shape<span class=\"token punctuation\">[</span>neuron_axis<span class=\"token punctuation\">]</span>\n        <span class=\"token comment\"># Initialize all weight matrices</span>\n        W_ij <span class=\"token operator\">=</span> tf<span class=\"token punctuation\">.</span>get_variable<span class=\"token punctuation\">(</span>\n            <span class=\"token string\">\"weights\"</span><span class=\"token punctuation\">,</span>\n            shape<span class=\"token operator\">=</span><span class=\"token punctuation\">[</span>n_primary_caps<span class=\"token punctuation\">,</span> n_digit_caps <span class=\"token operator\">*</span> dim_digit_caps<span class=\"token punctuation\">,</span> dim_primary_caps<span class=\"token punctuation\">]</span><span class=\"token punctuation\">,</span>\n            initializer<span class=\"token operator\">=</span>glorot_uniform<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span>\n        <span class=\"token punctuation\">)</span>\n        <span class=\"token comment\"># Initialize routing logits, the leading axis with size 1 is added for</span>\n        <span class=\"token comment\"># convenience.</span>\n        b_ij <span class=\"token operator\">=</span> tf<span class=\"token punctuation\">.</span>get_variable<span class=\"token punctuation\">(</span>\n            <span class=\"token string\">\"logits\"</span><span class=\"token punctuation\">,</span>\n            shape<span class=\"token operator\">=</span><span class=\"token punctuation\">[</span><span class=\"token number\">1</span><span class=\"token punctuation\">,</span> n_primary_caps<span class=\"token punctuation\">,</span> n_digit_caps<span class=\"token punctuation\">]</span><span class=\"token punctuation\">,</span>\n            initializer<span class=\"token operator\">=</span>tf<span class=\"token punctuation\">.</span>zeros_initializer<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">,</span>\n            trainable<span class=\"token operator\">=</span>args<span class=\"token punctuation\">.</span>logits_trainable\n        <span class=\"token punctuation\">)</span>\n        <span class=\"token comment\"># Reshape and transpose hacking</span>\n        u_i <span class=\"token operator\">=</span> tf<span class=\"token punctuation\">.</span>transpose<span class=\"token punctuation\">(</span>incoming<span class=\"token punctuation\">,</span> <span class=\"token punctuation\">(</span><span class=\"token number\">1</span><span class=\"token punctuation\">,</span> <span class=\"token number\">2</span><span class=\"token punctuation\">,</span> <span class=\"token number\">0</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span>\n        u_hat <span class=\"token operator\">=</span> tf<span class=\"token punctuation\">.</span>matmul<span class=\"token punctuation\">(</span>W_ij<span class=\"token punctuation\">,</span> u_i<span class=\"token punctuation\">)</span>\n        u_hat <span class=\"token operator\">=</span> tf<span class=\"token punctuation\">.</span>reshape<span class=\"token punctuation\">(</span>\n            tf<span class=\"token punctuation\">.</span>transpose<span class=\"token punctuation\">(</span>u_hat<span class=\"token punctuation\">,</span> <span class=\"token punctuation\">(</span><span class=\"token number\">2</span><span class=\"token punctuation\">,</span> <span class=\"token number\">0</span><span class=\"token punctuation\">,</span> <span class=\"token number\">1</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">,</span>\n            <span class=\"token punctuation\">(</span><span class=\"token operator\">-</span><span class=\"token number\">1</span><span class=\"token punctuation\">,</span> n_primary_caps<span class=\"token punctuation\">,</span> n_digit_caps<span class=\"token punctuation\">,</span> dim_digit_caps<span class=\"token punctuation\">)</span>\n        <span class=\"token punctuation\">)</span>\n\n        <span class=\"token keyword\">def</span> <span class=\"token function\">capsule_out</span><span class=\"token punctuation\">(</span>b_ij<span class=\"token punctuation\">)</span><span class=\"token punctuation\">:</span>\n            <span class=\"token triple-quoted-string string\">\"\"\" Given the logits b_ij, computes the output of this layer. \"\"\"</span>\n            c_ij <span class=\"token operator\">=</span> tf<span class=\"token punctuation\">.</span>nn<span class=\"token punctuation\">.</span>softmax<span class=\"token punctuation\">(</span>b_ij<span class=\"token punctuation\">,</span> dim<span class=\"token operator\">=</span><span class=\"token number\">2</span><span class=\"token punctuation\">)</span>\n            s_j <span class=\"token operator\">=</span> tf<span class=\"token punctuation\">.</span>reduce_sum<span class=\"token punctuation\">(</span>\n                tf<span class=\"token punctuation\">.</span>reshape<span class=\"token punctuation\">(</span>c_ij<span class=\"token punctuation\">,</span> <span class=\"token punctuation\">(</span><span class=\"token operator\">-</span><span class=\"token number\">1</span><span class=\"token punctuation\">,</span> n_primary_caps<span class=\"token punctuation\">,</span> n_digit_caps<span class=\"token punctuation\">,</span> <span class=\"token number\">1</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span> <span class=\"token operator\">*</span> u_hat<span class=\"token punctuation\">,</span>\n                axis<span class=\"token operator\">=</span><span class=\"token number\">1</span>\n            <span class=\"token punctuation\">)</span>\n            v_j <span class=\"token operator\">=</span> squash<span class=\"token punctuation\">(</span>s_j<span class=\"token punctuation\">)</span>\n            <span class=\"token keyword\">return</span> v_j\n\n        <span class=\"token keyword\">def</span> <span class=\"token function\">routing_iteration</span><span class=\"token punctuation\">(</span><span class=\"token builtin\">iter</span><span class=\"token punctuation\">,</span> logits<span class=\"token punctuation\">)</span><span class=\"token punctuation\">:</span>\n            <span class=\"token triple-quoted-string string\">\"\"\"\n            Given a set of logits, computes the new logits using the definition\n            of routing from the paper.\n            \"\"\"</span>\n            v_j <span class=\"token operator\">=</span> capsule_out<span class=\"token punctuation\">(</span>logits<span class=\"token punctuation\">)</span>\n            a_ij <span class=\"token operator\">=</span> tf<span class=\"token punctuation\">.</span>reduce_sum<span class=\"token punctuation\">(</span>tf<span class=\"token punctuation\">.</span>expand_dims<span class=\"token punctuation\">(</span>v_j<span class=\"token punctuation\">,</span> axis<span class=\"token operator\">=</span><span class=\"token number\">1</span><span class=\"token punctuation\">)</span> <span class=\"token operator\">*</span> u_hat<span class=\"token punctuation\">,</span> axis<span class=\"token operator\">=</span><span class=\"token number\">3</span><span class=\"token punctuation\">)</span>\n            logits <span class=\"token operator\">=</span> tf<span class=\"token punctuation\">.</span>reshape<span class=\"token punctuation\">(</span>logits <span class=\"token operator\">+</span> a_ij<span class=\"token punctuation\">,</span> <span class=\"token punctuation\">(</span><span class=\"token operator\">-</span><span class=\"token number\">1</span><span class=\"token punctuation\">,</span> n_primary_caps<span class=\"token punctuation\">,</span> n_digit_caps<span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span>\n            <span class=\"token keyword\">return</span> <span class=\"token punctuation\">[</span><span class=\"token builtin\">iter</span> <span class=\"token operator\">+</span> <span class=\"token number\">1</span><span class=\"token punctuation\">,</span> logits<span class=\"token punctuation\">]</span>\n\n        <span class=\"token comment\"># Compute routing</span>\n        i <span class=\"token operator\">=</span> tf<span class=\"token punctuation\">.</span>constant<span class=\"token punctuation\">(</span><span class=\"token number\">0</span><span class=\"token punctuation\">)</span>\n        routing_result <span class=\"token operator\">=</span> tf<span class=\"token punctuation\">.</span>while_loop<span class=\"token punctuation\">(</span>\n            <span class=\"token keyword\">lambda</span> i<span class=\"token punctuation\">,</span> logits<span class=\"token punctuation\">:</span> tf<span class=\"token punctuation\">.</span>less<span class=\"token punctuation\">(</span>i<span class=\"token punctuation\">,</span> routing_iters<span class=\"token punctuation\">)</span><span class=\"token punctuation\">,</span>\n            routing_iteration<span class=\"token punctuation\">,</span>\n            <span class=\"token punctuation\">[</span>i<span class=\"token punctuation\">,</span> tf<span class=\"token punctuation\">.</span>tile<span class=\"token punctuation\">(</span>b_ij<span class=\"token punctuation\">,</span> tf<span class=\"token punctuation\">.</span>stack<span class=\"token punctuation\">(</span><span class=\"token punctuation\">[</span>tf<span class=\"token punctuation\">.</span>shape<span class=\"token punctuation\">(</span>incoming<span class=\"token punctuation\">)</span><span class=\"token punctuation\">[</span><span class=\"token number\">0</span><span class=\"token punctuation\">]</span><span class=\"token punctuation\">,</span> <span class=\"token number\">1</span><span class=\"token punctuation\">,</span> <span class=\"token number\">1</span><span class=\"token punctuation\">]</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">]</span>\n        <span class=\"token punctuation\">)</span>\n        <span class=\"token comment\"># Second element of the result contains our final logits</span>\n        v_j <span class=\"token operator\">=</span> capsule_out<span class=\"token punctuation\">(</span>routing_result<span class=\"token punctuation\">[</span><span class=\"token number\">1</span><span class=\"token punctuation\">]</span><span class=\"token punctuation\">)</span>\n\n    <span class=\"token keyword\">return</span> v_j</code></pre></div>\n<p>A relatively uncommon operator in this piece of code is the while<em>loop, which takes a looping condition, a function to execute in a single iteration and the initial values for the loop. The values given here are the index and the logits $b</em>{ij}$ as in the paper. Note that these arguments are passed to both the condition function as well as the loop body function.</p>","frontmatter":{"title":"Capsule Networks","date":"November 23, 2017","description":"Neural networks with high-dimensional building blocks."}}},"pageContext":{"isCreatedByStatefulCreatePages":false,"slug":"/capsnet/","previous":null,"next":{"fields":{"slug":"/capsnet-cuda/"},"frontmatter":{"title":"Cuda, TensorFlow and capsule networks"}}}}