{"data":{"site":{"siteMetadata":{"title":"Machine Learning Blog - JvdW","author":"Jos van de Wolfshaar"}},"markdownRemark":{"id":"82e1e556-6bc1-5bf8-9963-41cd9b94e491","excerpt":"Recently I’ve spent some time on CUDA programming and implementing custom Ops for TensorFlow. As an exercise, I decided to take a shot at implementing a custom…","html":"<p>Recently I’ve spent some time on CUDA programming and implementing custom Ops for TensorFlow. As an exercise, I decided to take a shot at implementing a custom Op for one of the operations in capsule networks that would normally require some reshape hacking or at least a <a href=\"https://github.com/Sarasra/models/tree/master/research/capsules\">couple of intermediate TensorFlow Ops</a>. If you’re not familiar with Capsule Networks, have a look at the <a href=\"https://arxiv.org/abs/1710.09829\">original paper</a> or my previous post. The open-sourced capsule network code by the paper’s authors can be found <a href=\"https://github.com/Sarasra/models/tree/master/research/capsules\">here</a>. The code that we discuss in this blog post can be found <a href=\"https://github.com/jostosh/capsnet\">here</a>.</p>\n<p>Many of the concepts that are covered in this post (CUDA, TensorFlow custom Ops, gradient testing) can be learned by going through their corresponding documentations, but I always think it is enlightening to see how such separate elements come together. This is the main motivation behind my blog post: showcasing the development of a custom TensorFlow Op with CUDA from start to end. So let’s commence.</p>\n<p>There also exists a Chinese translation of this post by <a href=\"https://www.jqr.com/article/000055\">Jakukyo Friel</a>.</p>\n<h2>Capsule prediction</h2>\n<p>The operation of interest in this blog post is the one that computes:</p>\n<span class=\"katex-display\"><span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><msub><mover accent=\"true\"><mi mathvariant=\"bold-italic\">u</mi><mo>^</mo></mover><mrow><mi>j</mi><mi mathvariant=\"normal\">∣</mi><mi>i</mi></mrow></msub><mo>=</mo><msub><mi>W</mi><mrow><mi>i</mi><mi>j</mi></mrow></msub><msub><mi mathvariant=\"bold-italic\">u</mi><mrow><mi>i</mi><mi>j</mi></mrow></msub></mrow><annotation encoding=\"application/x-tex\">\\hat{\\boldsymbol u}_{j|i} = W_{ij} \\boldsymbol u_{ij}</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:1.0630799999999998em;vertical-align:-0.3551999999999999em;\"></span><span class=\"mord\"><span class=\"mord accent\"><span class=\"vlist-t\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.70788em;\"><span style=\"top:-3em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span class=\"mord\"><span class=\"mord\"><span class=\"mord boldsymbol\">u</span></span></span></span><span style=\"top:-3.01344em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span class=\"accent-body\" style=\"left:-0.25em;\">^</span></span></span></span></span></span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.34480000000000005em;\"><span style=\"top:-2.5198em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\"><span class=\"mord mathdefault mtight\" style=\"margin-right:0.05724em;\">j</span><span class=\"mord mtight\">∣</span><span class=\"mord mathdefault mtight\">i</span></span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.3551999999999999em;\"><span></span></span></span></span></span></span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span><span class=\"mrel\">=</span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:0.969438em;vertical-align:-0.286108em;\"></span><span class=\"mord\"><span class=\"mord mathdefault\" style=\"margin-right:0.13889em;\">W</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.311664em;\"><span style=\"top:-2.5500000000000003em;margin-left:-0.13889em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\"><span class=\"mord mathdefault mtight\">i</span><span class=\"mord mathdefault mtight\" style=\"margin-right:0.05724em;\">j</span></span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.286108em;\"><span></span></span></span></span></span></span><span class=\"mord\"><span class=\"mord\"><span class=\"mord boldsymbol\">u</span></span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.311664em;\"><span style=\"top:-2.5500000000000003em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\"><span class=\"mord mathdefault mtight\">i</span><span class=\"mord mathdefault mtight\" style=\"margin-right:0.05724em;\">j</span></span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.286108em;\"><span></span></span></span></span></span></span></span></span></span></span>\n<p>where <span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><msub><mi mathvariant=\"bold-italic\">u</mi><mrow><mi>i</mi><mi>j</mi></mrow></msub></mrow><annotation encoding=\"application/x-tex\">\\boldsymbol u_{ij}</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.730548em;vertical-align:-0.286108em;\"></span><span class=\"mord\"><span class=\"mord\"><span class=\"mord boldsymbol\">u</span></span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.311664em;\"><span style=\"top:-2.5500000000000003em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\"><span class=\"mord mathdefault mtight\">i</span><span class=\"mord mathdefault mtight\" style=\"margin-right:0.05724em;\">j</span></span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.286108em;\"><span></span></span></span></span></span></span></span></span></span> is the activation vector of capsule <span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mi>j</mi></mrow><annotation encoding=\"application/x-tex\">j</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.85396em;vertical-align:-0.19444em;\"></span><span class=\"mord mathdefault\" style=\"margin-right:0.05724em;\">j</span></span></span></span> as ‘predicted’ by capsule <span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mi>i</mi></mrow><annotation encoding=\"application/x-tex\">i</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.65952em;vertical-align:0em;\"></span><span class=\"mord mathdefault\">i</span></span></span></span> through the matrix-vector multiplication\n<span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><msub><mi>W</mi><mrow><mi>i</mi><mi>j</mi></mrow></msub><msub><mi mathvariant=\"bold-italic\">u</mi><mi>i</mi></msub></mrow><annotation encoding=\"application/x-tex\">W_{ij} \\boldsymbol u_i</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.969438em;vertical-align:-0.286108em;\"></span><span class=\"mord\"><span class=\"mord mathdefault\" style=\"margin-right:0.13889em;\">W</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.311664em;\"><span style=\"top:-2.5500000000000003em;margin-left:-0.13889em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\"><span class=\"mord mathdefault mtight\">i</span><span class=\"mord mathdefault mtight\" style=\"margin-right:0.05724em;\">j</span></span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.286108em;\"><span></span></span></span></span></span></span><span class=\"mord\"><span class=\"mord\"><span class=\"mord boldsymbol\">u</span></span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.31166399999999994em;\"><span style=\"top:-2.5500000000000003em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mathdefault mtight\">i</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span></span></span></span>. The matrix <span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><msub><mi>W</mi><mrow><mi>i</mi><mi>j</mi></mrow></msub></mrow><annotation encoding=\"application/x-tex\">W_{ij}</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.969438em;vertical-align:-0.286108em;\"></span><span class=\"mord\"><span class=\"mord mathdefault\" style=\"margin-right:0.13889em;\">W</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.311664em;\"><span style=\"top:-2.5500000000000003em;margin-left:-0.13889em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\"><span class=\"mord mathdefault mtight\">i</span><span class=\"mord mathdefault mtight\" style=\"margin-right:0.05724em;\">j</span></span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.286108em;\"><span></span></span></span></span></span></span></span></span></span> is of shape <code class=\"language-text\">[out_dim, in_dim]</code> and the vector <span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><msub><mi mathvariant=\"bold-italic\">u</mi><mi>i</mi></msub></mrow><annotation encoding=\"application/x-tex\">\\boldsymbol u_i</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.59444em;vertical-align:-0.15em;\"></span><span class=\"mord\"><span class=\"mord\"><span class=\"mord boldsymbol\">u</span></span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.31166399999999994em;\"><span style=\"top:-2.5500000000000003em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mathdefault mtight\">i</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span></span></span></span> denotes the output vector of capsule <span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mi>i</mi></mrow><annotation encoding=\"application/x-tex\">i</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.65952em;vertical-align:0em;\"></span><span class=\"mord mathdefault\">i</span></span></span></span>. A single capsule layer computes this for all pairs <span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mi>i</mi><mo separator=\"true\">,</mo><mi>j</mi></mrow><annotation encoding=\"application/x-tex\">i,j</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.85396em;vertical-align:-0.19444em;\"></span><span class=\"mord mathdefault\">i</span><span class=\"mpunct\">,</span><span class=\"mspace\" style=\"margin-right:0.16666666666666666em;\"></span><span class=\"mord mathdefault\" style=\"margin-right:0.05724em;\">j</span></span></span></span> and for all samples in a batch. Hence, the tensors <code class=\"language-text\">W_ij</code> and <code class=\"language-text\">u_i</code> are actually of shape <code class=\"language-text\">[batch_size, in_caps, in_dim]</code> and <code class=\"language-text\">[in_caps, out_caps, out_dim, in_dim]</code> respectively. This means that we will build an op that just takes in these two tensors and computes an output tensor <code class=\"language-text\">u_hat_ji</code> of shape <code class=\"language-text\">[batch_size, in_caps, out_caps, out_dim]</code>. In other words, for all batch indices <code class=\"language-text\">[0,1,...,batch_size-1]</code> and for all combinations of in capsules <code class=\"language-text\">[0,1,...,in_caps-1]</code> and out capsules <code class=\"language-text\">[0,1,...,out_caps-1]</code> we have to compute a matrix-vector product.</p>\n<h2>TensorFlow kernel implementation</h2>\n<p>Our custom Op will be most valuable if we implement it for a GPU. In the <a href=\"https://www.tensorflow.org/extend/adding_an_op\">TensorFlow documentation</a>, you can find the necessary material to get you started on your own C++ kernels for TensorFlow. Then, you can read up on how to empower your algorithms with massively parallel GPU capabilities in <a href=\"https://developer.nvidia.com/cuda-example\">this book</a> or <a href=\"https://devblogs.nvidia.com/even-easier-introduction-cuda/\">online</a>. I will not repeat the details that you can find there, but I will provide a practical example that hopefully helps to understand how you can use CUDA for your own TensorFlow Ops. For me, getting to know some CUDA was easier than I thought, but squeezing out all performance can be tricky and I will leave further optimization of the kernel in this post for future work.</p>\n<h2>Op registration</h2>\n<p>Let’s do the forward pass of the Op. First, we will register the Op:</p>\n<div class=\"gatsby-highlight\" data-language=\"cpp\"><pre class=\"language-cpp\"><code class=\"language-cpp\"><span class=\"token function\">REGISTER_OP</span><span class=\"token punctuation\">(</span><span class=\"token string\">\"CapsulePrediction\"</span><span class=\"token punctuation\">)</span>\n<span class=\"token punctuation\">.</span><span class=\"token function\">Input</span><span class=\"token punctuation\">(</span><span class=\"token string\">\"input: T\"</span><span class=\"token punctuation\">)</span>\n<span class=\"token punctuation\">.</span><span class=\"token function\">Input</span><span class=\"token punctuation\">(</span><span class=\"token string\">\"weights: T\"</span><span class=\"token punctuation\">)</span>\n<span class=\"token punctuation\">.</span><span class=\"token function\">Output</span><span class=\"token punctuation\">(</span><span class=\"token string\">\"output: T\"</span><span class=\"token punctuation\">)</span>\n<span class=\"token punctuation\">.</span><span class=\"token function\">Attr</span><span class=\"token punctuation\">(</span><span class=\"token string\">\"T: type\"</span><span class=\"token punctuation\">)</span>\n<span class=\"token punctuation\">.</span><span class=\"token function\">SetShapeFn</span><span class=\"token punctuation\">(</span><span class=\"token punctuation\">[</span><span class=\"token punctuation\">]</span><span class=\"token punctuation\">(</span>InferenceContext<span class=\"token operator\">*</span> ctx<span class=\"token punctuation\">)</span> <span class=\"token punctuation\">{</span>\n    <span class=\"token comment\">// Get shapes and ensure correct dimensionality</span>\n    ShapeHandle in_shape<span class=\"token punctuation\">;</span>\n    ShapeHandle weights_shape<span class=\"token punctuation\">;</span>\n    <span class=\"token function\">TF_RETURN_IF_ERROR</span><span class=\"token punctuation\">(</span>ctx<span class=\"token operator\">-></span><span class=\"token function\">WithRank</span><span class=\"token punctuation\">(</span>ctx<span class=\"token operator\">-></span><span class=\"token function\">input</span><span class=\"token punctuation\">(</span><span class=\"token number\">0</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">,</span> <span class=\"token number\">3</span><span class=\"token punctuation\">,</span> <span class=\"token operator\">&amp;</span>in_shape<span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">;</span>\n    <span class=\"token function\">TF_RETURN_IF_ERROR</span><span class=\"token punctuation\">(</span>ctx<span class=\"token operator\">-></span><span class=\"token function\">WithRank</span><span class=\"token punctuation\">(</span>ctx<span class=\"token operator\">-></span><span class=\"token function\">input</span><span class=\"token punctuation\">(</span><span class=\"token number\">1</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">,</span> <span class=\"token number\">4</span><span class=\"token punctuation\">,</span> <span class=\"token operator\">&amp;</span>weights_shape<span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">;</span>\n\n    <span class=\"token comment\">// Construct and set the output shape</span>\n    DimensionHandle out_d0<span class=\"token punctuation\">,</span> out_d1<span class=\"token punctuation\">,</span> out_d2<span class=\"token punctuation\">,</span> out_d3<span class=\"token punctuation\">;</span>\n    std<span class=\"token operator\">::</span>vector<span class=\"token operator\">&lt;</span>DimensionHandle<span class=\"token operator\">></span> out_dims<span class=\"token punctuation\">;</span>\n    out_dims<span class=\"token punctuation\">.</span><span class=\"token function\">push_back</span><span class=\"token punctuation\">(</span>ctx<span class=\"token operator\">-></span><span class=\"token function\">MakeDim</span><span class=\"token punctuation\">(</span>ctx<span class=\"token operator\">-></span><span class=\"token function\">Dim</span><span class=\"token punctuation\">(</span>ctx<span class=\"token operator\">-></span><span class=\"token function\">input</span><span class=\"token punctuation\">(</span><span class=\"token number\">0</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">,</span> <span class=\"token number\">0</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">;</span>\n    out_dims<span class=\"token punctuation\">.</span><span class=\"token function\">push_back</span><span class=\"token punctuation\">(</span>ctx<span class=\"token operator\">-></span><span class=\"token function\">MakeDim</span><span class=\"token punctuation\">(</span>ctx<span class=\"token operator\">-></span><span class=\"token function\">Dim</span><span class=\"token punctuation\">(</span>ctx<span class=\"token operator\">-></span><span class=\"token function\">input</span><span class=\"token punctuation\">(</span><span class=\"token number\">1</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">,</span> <span class=\"token number\">0</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">;</span>\n    out_dims<span class=\"token punctuation\">.</span><span class=\"token function\">push_back</span><span class=\"token punctuation\">(</span>ctx<span class=\"token operator\">-></span><span class=\"token function\">MakeDim</span><span class=\"token punctuation\">(</span>ctx<span class=\"token operator\">-></span><span class=\"token function\">Dim</span><span class=\"token punctuation\">(</span>ctx<span class=\"token operator\">-></span><span class=\"token function\">input</span><span class=\"token punctuation\">(</span><span class=\"token number\">1</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">,</span> <span class=\"token number\">1</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">;</span>\n    out_dims<span class=\"token punctuation\">.</span><span class=\"token function\">push_back</span><span class=\"token punctuation\">(</span>ctx<span class=\"token operator\">-></span><span class=\"token function\">MakeDim</span><span class=\"token punctuation\">(</span>ctx<span class=\"token operator\">-></span><span class=\"token function\">Dim</span><span class=\"token punctuation\">(</span>ctx<span class=\"token operator\">-></span><span class=\"token function\">input</span><span class=\"token punctuation\">(</span><span class=\"token number\">1</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">,</span> <span class=\"token number\">2</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">;</span>\n    ShapeHandle out_shape <span class=\"token operator\">=</span> ctx<span class=\"token operator\">-></span><span class=\"token function\">MakeShape</span><span class=\"token punctuation\">(</span>out_dims<span class=\"token punctuation\">)</span><span class=\"token punctuation\">;</span>\n    ctx<span class=\"token operator\">-></span><span class=\"token function\">set_output</span><span class=\"token punctuation\">(</span><span class=\"token number\">0</span><span class=\"token punctuation\">,</span> out_shape<span class=\"token punctuation\">)</span><span class=\"token punctuation\">;</span>\n\n    <span class=\"token keyword\">return</span> Status<span class=\"token operator\">::</span><span class=\"token function\">OK</span><span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">;</span>\n<span class=\"token punctuation\">}</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">;</span></code></pre></div>\n<p>For now, I have defined this so that we could later specify different TensorFlow kernels for different dtypes by adding the <code class=\"language-text\">&quot;..: T&quot;</code> specification. Some of the classes that you see here such as <code class=\"language-text\">ShapeHandle</code>, <code class=\"language-text\">DimensionHandle</code> and <code class=\"language-text\">InferenceContext</code> are defined in the <code class=\"language-text\">tensorflow</code> namespace. The code shows a <em>shape function</em> that is implemented as a lambda function which first ensures <code class=\"language-text\">ctx-&gt;input(0)</code> (the input <span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><msub><mi mathvariant=\"bold-italic\">u</mi><mi>i</mi></msub></mrow><annotation encoding=\"application/x-tex\">\\boldsymbol u_i</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.59444em;vertical-align:-0.15em;\"></span><span class=\"mord\"><span class=\"mord\"><span class=\"mord boldsymbol\">u</span></span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.31166399999999994em;\"><span style=\"top:-2.5500000000000003em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mathdefault mtight\">i</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span></span></span></span>) and <code class=\"language-text\">ctx-&gt;input(1)</code> (the weights <span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><msub><mi>W</mi><mrow><mi>i</mi><mi>j</mi></mrow></msub></mrow><annotation encoding=\"application/x-tex\">W_{ij}</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.969438em;vertical-align:-0.286108em;\"></span><span class=\"mord\"><span class=\"mord mathdefault\" style=\"margin-right:0.13889em;\">W</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.311664em;\"><span style=\"top:-2.5500000000000003em;margin-left:-0.13889em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\"><span class=\"mord mathdefault mtight\">i</span><span class=\"mord mathdefault mtight\" style=\"margin-right:0.05724em;\">j</span></span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.286108em;\"><span></span></span></span></span></span></span></span></span></span>) have the correct rank. Then, we determine the dimensions of the output tensor which we can obtain from the input tensors. The dimension of the Op’s output is <code class=\"language-text\">[batch_size, in_caps, out_caps, out_dim]</code>, so we take batch<em>size and in</em>caps from the <span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><msub><mi mathvariant=\"bold-italic\">u</mi><mi>i</mi></msub></mrow><annotation encoding=\"application/x-tex\">\\boldsymbol u_i</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.59444em;vertical-align:-0.15em;\"></span><span class=\"mord\"><span class=\"mord\"><span class=\"mord boldsymbol\">u</span></span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.31166399999999994em;\"><span style=\"top:-2.5500000000000003em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mathdefault mtight\">i</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span></span></span></span> tensor and out<em>caps and out</em>dim from the $W_{ij} tensor.</p>\n<h2>Forward Capsule Prediction</h2>\n<p>Now, let’s look at the Op’s kernel. The word ‘kernel’ is TensorFlow terminology for the device-specific implementation of an Op. When defining a custom kernel, it should inherit from TensorFlow’s <code class=\"language-text\">OpKernel</code> and it shall implement the <code class=\"language-text\">Compute</code> method:</p>\n<div class=\"gatsby-highlight\" data-language=\"cpp\"><pre class=\"language-cpp\"><code class=\"language-cpp\"><span class=\"token keyword\">class</span> <span class=\"token class-name\">CapsulePredictionOp</span> <span class=\"token operator\">:</span> <span class=\"token keyword\">public</span> OpKernel\n<span class=\"token punctuation\">{</span>\n <span class=\"token keyword\">public</span><span class=\"token operator\">:</span>\n  <span class=\"token keyword\">explicit</span> <span class=\"token function\">CapsulePredictionOp</span><span class=\"token punctuation\">(</span>OpKernelConstruction<span class=\"token operator\">*</span> ctx<span class=\"token punctuation\">)</span> <span class=\"token operator\">:</span> <span class=\"token function\">OpKernel</span><span class=\"token punctuation\">(</span>ctx<span class=\"token punctuation\">)</span> <span class=\"token punctuation\">{</span> <span class=\"token punctuation\">}</span>\n\n  <span class=\"token keyword\">void</span> <span class=\"token function\">Compute</span><span class=\"token punctuation\">(</span>OpKernelContext<span class=\"token operator\">*</span> ctx<span class=\"token punctuation\">)</span> override\n  <span class=\"token punctuation\">{</span>\n    <span class=\"token comment\">// Get inputs</span>\n    <span class=\"token keyword\">const</span> Tensor<span class=\"token operator\">&amp;</span> input <span class=\"token operator\">=</span> ctx<span class=\"token operator\">-></span><span class=\"token function\">input</span><span class=\"token punctuation\">(</span><span class=\"token number\">0</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">;</span>\n    <span class=\"token keyword\">const</span> Tensor<span class=\"token operator\">&amp;</span> weights <span class=\"token operator\">=</span> ctx<span class=\"token operator\">-></span><span class=\"token function\">input</span><span class=\"token punctuation\">(</span><span class=\"token number\">1</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">;</span>\n\n    <span class=\"token comment\">// Setup output shape</span>\n    <span class=\"token keyword\">const</span> TensorShape<span class=\"token operator\">&amp;</span> <span class=\"token function\">input_shape</span><span class=\"token punctuation\">(</span>input<span class=\"token punctuation\">.</span><span class=\"token function\">shape</span><span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">;</span>\n    TensorShape <span class=\"token function\">output_shape</span><span class=\"token punctuation\">(</span>weights<span class=\"token punctuation\">.</span><span class=\"token function\">shape</span><span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">;</span>\n    output_shape<span class=\"token punctuation\">.</span><span class=\"token function\">InsertDim</span><span class=\"token punctuation\">(</span><span class=\"token number\">0</span><span class=\"token punctuation\">,</span> input_shape<span class=\"token punctuation\">.</span><span class=\"token function\">dim_size</span><span class=\"token punctuation\">(</span><span class=\"token number\">0</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">;</span>\n    output_shape<span class=\"token punctuation\">.</span><span class=\"token function\">RemoveDim</span><span class=\"token punctuation\">(</span><span class=\"token number\">4</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">;</span>\n\n    <span class=\"token comment\">// Allocate output tensor</span>\n    Tensor<span class=\"token operator\">*</span> output <span class=\"token operator\">=</span> <span class=\"token keyword\">nullptr</span><span class=\"token punctuation\">;</span>\n    <span class=\"token function\">OP_REQUIRES_OK</span><span class=\"token punctuation\">(</span>ctx<span class=\"token punctuation\">,</span> ctx<span class=\"token operator\">-></span><span class=\"token function\">allocate_output</span><span class=\"token punctuation\">(</span><span class=\"token number\">0</span><span class=\"token punctuation\">,</span> output_shape<span class=\"token punctuation\">,</span> <span class=\"token operator\">&amp;</span>output<span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">;</span>\n\n    <span class=\"token comment\">// Get the Eigen tensors and pass them on the launcher</span>\n    <span class=\"token keyword\">auto</span> input_tensor   <span class=\"token operator\">=</span> input<span class=\"token punctuation\">.</span>tensor<span class=\"token operator\">&lt;</span><span class=\"token keyword\">float</span><span class=\"token punctuation\">,</span> <span class=\"token number\">3</span><span class=\"token operator\">></span><span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">;</span>\n    <span class=\"token keyword\">auto</span> weights_tensor <span class=\"token operator\">=</span> weights<span class=\"token punctuation\">.</span>tensor<span class=\"token operator\">&lt;</span><span class=\"token keyword\">float</span><span class=\"token punctuation\">,</span> <span class=\"token number\">4</span><span class=\"token operator\">></span><span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">;</span>\n    <span class=\"token keyword\">auto</span> output_tensor  <span class=\"token operator\">=</span> output<span class=\"token operator\">-></span>tensor<span class=\"token operator\">&lt;</span><span class=\"token keyword\">float</span><span class=\"token punctuation\">,</span> <span class=\"token number\">4</span><span class=\"token operator\">></span><span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">;</span>\n    <span class=\"token function\">launchCapsulePrediction</span><span class=\"token punctuation\">(</span>ctx<span class=\"token operator\">-></span><span class=\"token function\">eigen_device</span><span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">,</span> input_tensor<span class=\"token punctuation\">,</span> weights_tensor<span class=\"token punctuation\">,</span>\n      output_tensor<span class=\"token punctuation\">)</span><span class=\"token punctuation\">;</span>\n  <span class=\"token punctuation\">}</span>\n<span class=\"token punctuation\">}</span><span class=\"token punctuation\">;</span></code></pre></div>\n<p>In the implementation above we haven’t done anything with CUDA yet, but we’ll get there so don’t worry. The code merely initializes the output shape from the input shapes and allocates the memory. The <code class=\"language-text\">OpKernelContext</code> object that is provided as a parameter makes sure to allocate the memory on the currently used device. In our case, this will be the GPU. Then, we obtain the <code class=\"language-text\">Eigen</code> tensors through the <code class=\"language-text\">tensor</code> method and pass them on to our <code class=\"language-text\">launchCapsulePrediction</code> function, where the actual magic happens.</p>\n<h2>Launching the kernel</h2>\n<p>Our <code class=\"language-text\">launchCapsulePrediction</code> function literally (at least in CUDA terminology) launches code on the GPU. Perhaps a little confusing, but CUDA refers to functions that run code on the ‘device’ as <em>kernels</em>. In TensorFlow terminology, a kernel is not necessarily a GPU implementation, while in CUDA terminology it is. Let’s not get too wrapped up in terminology and just get to the code:</p>\n<div class=\"gatsby-highlight\" data-language=\"cpp\"><pre class=\"language-cpp\"><code class=\"language-cpp\"><span class=\"token keyword\">void</span> <span class=\"token function\">launchCapsulePrediction</span><span class=\"token punctuation\">(</span>\n  <span class=\"token keyword\">const</span> GPUDevice<span class=\"token operator\">&amp;</span> d<span class=\"token punctuation\">,</span>\n  <span class=\"token keyword\">typename</span> TTypes<span class=\"token operator\">&lt;</span><span class=\"token keyword\">float</span><span class=\"token punctuation\">,</span> <span class=\"token number\">3</span><span class=\"token operator\">></span><span class=\"token operator\">::</span>ConstTensor x<span class=\"token punctuation\">,</span>\n  <span class=\"token keyword\">typename</span> TTypes<span class=\"token operator\">&lt;</span><span class=\"token keyword\">float</span><span class=\"token punctuation\">,</span> <span class=\"token number\">4</span><span class=\"token operator\">></span><span class=\"token operator\">::</span>ConstTensor weights<span class=\"token punctuation\">,</span>\n  <span class=\"token keyword\">typename</span> TTypes<span class=\"token operator\">&lt;</span><span class=\"token keyword\">float</span><span class=\"token punctuation\">,</span> <span class=\"token number\">4</span><span class=\"token operator\">></span><span class=\"token operator\">::</span>Tensor out<span class=\"token punctuation\">)</span>\n<span class=\"token punctuation\">{</span>\n  <span class=\"token comment\">// Get the dimensions</span>\n  <span class=\"token keyword\">const</span> int64 batch_size  <span class=\"token operator\">=</span> x<span class=\"token punctuation\">.</span><span class=\"token function\">dimension</span><span class=\"token punctuation\">(</span><span class=\"token number\">0</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">;</span>\n  <span class=\"token keyword\">const</span> int64 in_caps     <span class=\"token operator\">=</span> x<span class=\"token punctuation\">.</span><span class=\"token function\">dimension</span><span class=\"token punctuation\">(</span><span class=\"token number\">1</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">;</span>\n  <span class=\"token keyword\">const</span> int64 in_dim      <span class=\"token operator\">=</span> x<span class=\"token punctuation\">.</span><span class=\"token function\">dimension</span><span class=\"token punctuation\">(</span><span class=\"token number\">2</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">;</span>\n  <span class=\"token keyword\">const</span> int64 out_dim     <span class=\"token operator\">=</span> weights<span class=\"token punctuation\">.</span><span class=\"token function\">dimension</span><span class=\"token punctuation\">(</span><span class=\"token number\">2</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">;</span>\n  <span class=\"token keyword\">const</span> int64 out_caps    <span class=\"token operator\">=</span> weights<span class=\"token punctuation\">.</span><span class=\"token function\">dimension</span><span class=\"token punctuation\">(</span><span class=\"token number\">1</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">;</span>\n\n  <span class=\"token comment\">// Size first dim</span>\n  <span class=\"token keyword\">const</span> int64 w_d0 <span class=\"token operator\">=</span> out_caps <span class=\"token operator\">*</span> out_dim <span class=\"token operator\">*</span> in_dim<span class=\"token punctuation\">;</span>\n  <span class=\"token keyword\">const</span> int64 x_d0 <span class=\"token operator\">=</span> in_caps <span class=\"token operator\">*</span> in_dim<span class=\"token punctuation\">;</span>\n  <span class=\"token keyword\">const</span> int64 o_d0 <span class=\"token operator\">=</span> in_caps <span class=\"token operator\">*</span> out_caps <span class=\"token operator\">*</span> out_dim<span class=\"token punctuation\">;</span>\n\n  <span class=\"token comment\">// Second dim</span>\n  <span class=\"token keyword\">const</span> int64 w_d1 <span class=\"token operator\">=</span> out_dim <span class=\"token operator\">*</span> in_dim<span class=\"token punctuation\">;</span>\n  <span class=\"token keyword\">const</span> int64 x_d1 <span class=\"token operator\">=</span> in_dim<span class=\"token punctuation\">;</span>\n  <span class=\"token keyword\">const</span> int64 o_d1 <span class=\"token operator\">=</span> out_caps <span class=\"token operator\">*</span> out_dim<span class=\"token punctuation\">;</span>\n\n  <span class=\"token comment\">// Third dim</span>\n  <span class=\"token keyword\">const</span> int64 w_d2 <span class=\"token operator\">=</span> in_dim<span class=\"token punctuation\">;</span>\n  <span class=\"token keyword\">const</span> int64 o_d2 <span class=\"token operator\">=</span> out_dim<span class=\"token punctuation\">;</span>\n\n  <span class=\"token comment\">// Launch CUDA kernel for forward operation</span>\n  CudaLaunchConfig config <span class=\"token operator\">=</span> <span class=\"token function\">GetCudaLaunchConfig</span><span class=\"token punctuation\">(</span>out<span class=\"token punctuation\">.</span><span class=\"token function\">size</span><span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">,</span> d<span class=\"token punctuation\">)</span><span class=\"token punctuation\">;</span>\n  capsulePredictionKernel\n    <span class=\"token operator\">&lt;&lt;</span><span class=\"token operator\">&lt;</span>config<span class=\"token punctuation\">.</span>block_count<span class=\"token punctuation\">,</span> config<span class=\"token punctuation\">.</span>thread_per_block<span class=\"token punctuation\">,</span> <span class=\"token number\">0</span><span class=\"token punctuation\">,</span> d<span class=\"token punctuation\">.</span><span class=\"token function\">stream</span><span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span><span class=\"token operator\">>></span><span class=\"token operator\">></span><span class=\"token punctuation\">(</span>\n      x<span class=\"token punctuation\">.</span><span class=\"token function\">data</span><span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">,</span> weights<span class=\"token punctuation\">.</span><span class=\"token function\">data</span><span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">,</span> out<span class=\"token punctuation\">.</span><span class=\"token function\">data</span><span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">,</span>\n      o_d0<span class=\"token punctuation\">,</span> o_d1<span class=\"token punctuation\">,</span> o_d2<span class=\"token punctuation\">,</span> x_d0<span class=\"token punctuation\">,</span> x_d1<span class=\"token punctuation\">,</span> w_d0<span class=\"token punctuation\">,</span> w_d1<span class=\"token punctuation\">,</span> w_d2<span class=\"token punctuation\">,</span>\n      in_dim<span class=\"token punctuation\">,</span> out<span class=\"token punctuation\">.</span><span class=\"token function\">size</span><span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">;</span>\n<span class=\"token punctuation\">}</span></code></pre></div>\n<p>The <code class=\"language-text\">TTypes</code> templates that you can see in the function arguments and the <code class=\"language-text\">int64</code> types are defined in the <code class=\"language-text\">tensorflow</code> namespace. The next part about the dimensions should be pretty self-explanatory. Because we are passing our tensor data as one-dimensional arrays to the actual CUDA kernel, we need to figure out what the memory sizes are for each dimension and each kernel. Note that when I say ‘memory sizes’, I just refer to the number of floats for each axis and not the byte size. Let’s consider the memory sizes of the first axis of each tensor:</p>\n<div class=\"gatsby-highlight\" data-language=\"cpp\"><pre class=\"language-cpp\"><code class=\"language-cpp\"><span class=\"token comment\">// Size first dim</span>\n<span class=\"token keyword\">const</span> int64 w_d0 <span class=\"token operator\">=</span> out_caps <span class=\"token operator\">*</span> out_dim <span class=\"token operator\">*</span> in_dim<span class=\"token punctuation\">;</span>\n<span class=\"token keyword\">const</span> int64 x_d0 <span class=\"token operator\">=</span> in_caps <span class=\"token operator\">*</span> in_dim<span class=\"token punctuation\">;</span>\n<span class=\"token keyword\">const</span> int64 o_d0 <span class=\"token operator\">=</span> in_caps <span class=\"token operator\">*</span> out_caps <span class=\"token operator\">*</span> out_dim<span class=\"token punctuation\">;</span></code></pre></div>\n<p>Awesome, so we can simply get these using the dimensions we determined already. The code tells us that <code class=\"language-text\">w_d0</code> is just the product of <code class=\"language-text\">out_caps</code>, <code class=\"language-text\">out_dim</code> and <code class=\"language-text\">in_dim</code>. So if we want to jump from one index <span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><msub><mi>W</mi><mrow><mi>i</mi><mo separator=\"true\">,</mo><mi>j</mi><mo separator=\"true\">,</mo><mi>k</mi><mo separator=\"true\">,</mo><mi>l</mi></mrow></msub></mrow><annotation encoding=\"application/x-tex\">W_{i,j,k,l}</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.969438em;vertical-align:-0.286108em;\"></span><span class=\"mord\"><span class=\"mord mathdefault\" style=\"margin-right:0.13889em;\">W</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.3361079999999999em;\"><span style=\"top:-2.5500000000000003em;margin-left:-0.13889em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\"><span class=\"mord mathdefault mtight\">i</span><span class=\"mpunct mtight\">,</span><span class=\"mord mathdefault mtight\" style=\"margin-right:0.05724em;\">j</span><span class=\"mpunct mtight\">,</span><span class=\"mord mathdefault mtight\" style=\"margin-right:0.03148em;\">k</span><span class=\"mpunct mtight\">,</span><span class=\"mord mathdefault mtight\" style=\"margin-right:0.01968em;\">l</span></span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.286108em;\"><span></span></span></span></span></span></span></span></span></span> to <span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><msub><mi>W</mi><mrow><mi>i</mi><mo>+</mo><mn>1</mn><mo separator=\"true\">,</mo><mi>j</mi><mo separator=\"true\">,</mo><mi>k</mi><mo separator=\"true\">,</mo><mi>l</mi></mrow></msub></mrow><annotation encoding=\"application/x-tex\">W_{i+1,j,k,l}</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.969438em;vertical-align:-0.286108em;\"></span><span class=\"mord\"><span class=\"mord mathdefault\" style=\"margin-right:0.13889em;\">W</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.3361079999999999em;\"><span style=\"top:-2.5500000000000003em;margin-left:-0.13889em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\"><span class=\"mord mathdefault mtight\">i</span><span class=\"mbin mtight\">+</span><span class=\"mord mtight\">1</span><span class=\"mpunct mtight\">,</span><span class=\"mord mathdefault mtight\" style=\"margin-right:0.05724em;\">j</span><span class=\"mpunct mtight\">,</span><span class=\"mord mathdefault mtight\" style=\"margin-right:0.03148em;\">k</span><span class=\"mpunct mtight\">,</span><span class=\"mord mathdefault mtight\" style=\"margin-right:0.01968em;\">l</span></span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.286108em;\"><span></span></span></span></span></span></span></span></span></span> we should add <code class=\"language-text\">w_d0</code> to the one-dimensional index. The same goes for index  <span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mi>j</mi></mrow><annotation encoding=\"application/x-tex\">j</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.85396em;vertical-align:-0.19444em;\"></span><span class=\"mord mathdefault\" style=\"margin-right:0.05724em;\">j</span></span></span></span>\nand <code class=\"language-text\">w_d1</code> as you might already expect.</p>\n<p>The actual CUDA kernel launch is given at the bottom of the function and repeated here:</p>","frontmatter":{"title":"Cuda, TensorFlow and capsule networks","date":"February 11, 2018","description":null}}},"pageContext":{"isCreatedByStatefulCreatePages":false,"slug":"/capsnet/","previous":null,"next":{"fields":{"slug":"/spn01/"},"frontmatter":{"title":"Tensorizing Sum-Product Networks"}}}}