{"data":{"site":{"siteMetadata":{"title":"Machine Learning Blog - JvdW","author":"Jos van de Wolfshaar"}},"markdownRemark":{"id":"dcc32016-c32d-5ee9-8c3f-a748e74e073d","excerpt":"In this post I will cover a partial re-implementation of a recent paper on manifold regularization (Lecouat et al., 2018) for semi-supervised learning with…","html":"<p>In this post I will cover a partial re-implementation of a recent paper on manifold regularization (Lecouat et al., 2018) for semi-supervised learning with Generative Adversarial Networks (Goodfellow et al., 2014). I will attempt to re-implement their main contribution, rather than getting all the hyperparameter details just right. Also, for the sake of demonstration, time constraints and simplicity, I will consider the MNIST dataset rather than the CIFAR10 or SVHN datasets as done in the paper. Ultimately, this post aims at bridging the gap between the theory and implementation for GANs in the semi-supervised learning setting. The code that comes with this post can be found here.</p>\n<h2>Generative Adversarial Networks</h2>\n<p>Let’s quickly go over Generative Adversarial Networks (GAN). In terms of the current pace within the AI/ML community, they have been around for a while (just about 4 years), so you might already be familiar with them. The ‘vanilla’ GAN procedure is to train a <em>generator</em> to generate images that are realistic and capable of fooling a <em>discriminator</em>. The generator generates the images by means of a deep neural network that takes in a noise vector <span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mi mathvariant=\"bold-italic\">z</mi></mrow><annotation encoding=\"application/x-tex\">\\boldsymbol z</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.44444em;vertical-align:0em;\"></span><span class=\"mord\"><span class=\"mord boldsymbol\" style=\"margin-right:0.04213em;\">z</span></span></span></span></span>.</p>\n<p>The discriminator (which is a deep neural network as well) is fed with the generated images, but also with some real data. Its job is to say whether each image is either real (coming from the dataset) or fake (coming from the generator), which in terms of implementation comes down to <em>binary classification</em>. The image below summarizes the vanilla GAN setup.</p>\n<p><figure class=\"md-figure\"><img src=\"https://cdn-images-1.medium.com/max/2000/1*k5ry4wcrWGaxtNTqUZFAQQ.png\"><figcaption>&#x3C;i>&#x3C;center>Figure 1: A Vanilla GAN Setup.&#x3C;/center>&#x3C;/i></figcaption></figure></p>\n<h2>Semi-supervised learning</h2>\n<p>Semi-supervised learning problems concern a mix of <em>labeled</em> and <em>unlabeled</em> data. Leveraging the information in both the labeled and unlabeled data to eventually improve the performance on unseen labeled data is an interesting and more challenging problem than merely doing supervised learning on a large labeled dataset. In this case we might be limited to having only about 200 samples per class. So what should we do when only a small portion of the data is labeled?</p>\n<p>Note that adversarial training of <em>vanilla</em> GANs doesn’t require labeled data. At the same time, the deep neural network of the discriminator is able to learn powerful and robust abstractions of images by gradually becoming better at discriminating fake from real. Whatever it’s learning about unlabeled images will presumably also yield useful feature descriptors of labeled images. So how do we use the discriminator for both labeled and unlabeled data? Well, the discriminator is not necessarily limited to just telling fake from real. We could decide to train it to also <em>classify</em> the real data.</p>\n<p>A GAN with a classifying discriminator would be able to exploit both the unlabeled as well as the labeled data. The unlabeled data will be used to merely tell fake from real. The labeled data would be used to optimize the classification performance. In practice, this just means that the discriminator has a <em>softmax</em> output distribution for which we minimize the cross-entropy. Indeed, part of the training procedure is just doing supervised learning. The other part is about adversarial training. The image below summarizes the semi-supervised learning setup with a GAN.</p>\n<p><figure class=\"md-figure\"><img src=\"https://cdn-images-1.medium.com/max/2000/1*Grve_j-Mv4Jgmtq3u7yKyQ.png\"><figcaption>Semi-supervised learning setup with a GAN.</figcaption></figure></p>\n<h2>The Implementation</h2>\n<p>Let’s just head over to the implementation, since that might be the best way of understanding what’s happening. The snippet below prepares the data. It doesn’t really contain anything sophisticated. Basically, we take 400 samples per class and concatenate the resulting arrays as being our actual supervised subset. The unlabeled dataset consists of <em>all</em> train data (it also includes the labeled data, since we might as well use it anyway). As is customary for training GANs now, the output of the generator uses a hyperbolic tangent function, meaning its output is between <span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mo>−</mo><mn>1</mn></mrow><annotation encoding=\"application/x-tex\">-1</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.72777em;vertical-align:-0.08333em;\"></span><span class=\"mord\">−</span><span class=\"mord\">1</span></span></span></span> and <span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mo>+</mo><mn>1</mn></mrow><annotation encoding=\"application/x-tex\">+1</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.72777em;vertical-align:-0.08333em;\"></span><span class=\"mord\">+</span><span class=\"mord\">1</span></span></span></span>. Therefore, we rescale the data to be in that range as well. Then, we create TensorFlow iterators so that we can efficiently go through the data later without having to struggle with feed dicts later on.</p>\n<div class=\"gatsby-highlight\" data-language=\"python\"><pre class=\"language-python\"><code class=\"language-python\"><span class=\"token keyword\">def</span> <span class=\"token function\">prepare_input_pipeline</span><span class=\"token punctuation\">(</span>flags_obj<span class=\"token punctuation\">)</span><span class=\"token punctuation\">:</span>\n    <span class=\"token punctuation\">(</span>train_x<span class=\"token punctuation\">,</span> train_y<span class=\"token punctuation\">)</span><span class=\"token punctuation\">,</span> <span class=\"token punctuation\">(</span>test_x<span class=\"token punctuation\">,</span> test_y<span class=\"token punctuation\">)</span> <span class=\"token operator\">=</span> tf<span class=\"token punctuation\">.</span>keras<span class=\"token punctuation\">.</span>datasets<span class=\"token punctuation\">.</span>mnist<span class=\"token punctuation\">.</span>load_data<span class=\"token punctuation\">(</span>\n        <span class=\"token string\">\"/home/jos/datasets/mnist/mnist.npz\"</span><span class=\"token punctuation\">)</span>\n\n    <span class=\"token keyword\">def</span> <span class=\"token function\">reshape_and_scale</span><span class=\"token punctuation\">(</span>x<span class=\"token punctuation\">,</span> img_shape<span class=\"token operator\">=</span><span class=\"token punctuation\">(</span><span class=\"token operator\">-</span><span class=\"token number\">1</span><span class=\"token punctuation\">,</span> <span class=\"token number\">28</span><span class=\"token punctuation\">,</span> <span class=\"token number\">28</span><span class=\"token punctuation\">,</span> <span class=\"token number\">1</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">:</span>\n        <span class=\"token keyword\">return</span> x<span class=\"token punctuation\">.</span>reshape<span class=\"token punctuation\">(</span>img_shape<span class=\"token punctuation\">)</span><span class=\"token punctuation\">.</span>astype<span class=\"token punctuation\">(</span>np<span class=\"token punctuation\">.</span>float32<span class=\"token punctuation\">)</span> <span class=\"token operator\">/</span> <span class=\"token number\">255</span><span class=\"token punctuation\">.</span> <span class=\"token operator\">*</span> <span class=\"token number\">2.0</span> <span class=\"token operator\">-</span> <span class=\"token number\">1.0</span>\n\n    <span class=\"token comment\"># Reshape data and rescale to [-1, 1]</span>\n    train_x <span class=\"token operator\">=</span> reshape_and_scale<span class=\"token punctuation\">(</span>train_x<span class=\"token punctuation\">)</span>\n    test_x <span class=\"token operator\">=</span> reshape_and_scale<span class=\"token punctuation\">(</span>test_x<span class=\"token punctuation\">)</span>\n\n    <span class=\"token comment\"># Shuffle train data</span>\n    train_x_unlabeled<span class=\"token punctuation\">,</span> train_y_unlabeled <span class=\"token operator\">=</span> shuffle<span class=\"token punctuation\">(</span>train_x<span class=\"token punctuation\">,</span> train_y<span class=\"token punctuation\">)</span>\n\n    <span class=\"token comment\"># Select subset as supervised</span>\n    train_x_labeled<span class=\"token punctuation\">,</span> train_y_labeled <span class=\"token operator\">=</span> <span class=\"token punctuation\">[</span><span class=\"token punctuation\">]</span><span class=\"token punctuation\">,</span> <span class=\"token punctuation\">[</span><span class=\"token punctuation\">]</span>\n    <span class=\"token keyword\">for</span> i <span class=\"token keyword\">in</span> <span class=\"token builtin\">range</span><span class=\"token punctuation\">(</span>flags_obj<span class=\"token punctuation\">.</span>num_classes<span class=\"token punctuation\">)</span><span class=\"token punctuation\">:</span>\n        train_x_labeled<span class=\"token punctuation\">.</span>append<span class=\"token punctuation\">(</span>\n            train_x_unlabeled<span class=\"token punctuation\">[</span>train_y_unlabeled <span class=\"token operator\">==</span> i<span class=\"token punctuation\">]</span><span class=\"token punctuation\">[</span><span class=\"token punctuation\">:</span>flags_obj<span class=\"token punctuation\">.</span>num_labeled_examples<span class=\"token punctuation\">]</span><span class=\"token punctuation\">)</span>\n        train_y_labeled<span class=\"token punctuation\">.</span>append<span class=\"token punctuation\">(</span>\n            train_y_unlabeled<span class=\"token punctuation\">[</span>train_y_unlabeled <span class=\"token operator\">==</span> i<span class=\"token punctuation\">]</span><span class=\"token punctuation\">[</span><span class=\"token punctuation\">:</span>flags_obj<span class=\"token punctuation\">.</span>num_labeled_examples<span class=\"token punctuation\">]</span><span class=\"token punctuation\">)</span>\n    train_x_labeled <span class=\"token operator\">=</span> np<span class=\"token punctuation\">.</span>concatenate<span class=\"token punctuation\">(</span>train_x_labeled<span class=\"token punctuation\">)</span>\n    train_y_labeled <span class=\"token operator\">=</span> np<span class=\"token punctuation\">.</span>concatenate<span class=\"token punctuation\">(</span>train_y_labeled<span class=\"token punctuation\">)</span>\n\n    <span class=\"token keyword\">with</span> tf<span class=\"token punctuation\">.</span>name_scope<span class=\"token punctuation\">(</span><span class=\"token string\">\"InputPipeline\"</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">:</span>\n\n        <span class=\"token keyword\">def</span> <span class=\"token function\">train_pipeline</span><span class=\"token punctuation\">(</span>data<span class=\"token punctuation\">,</span> shuffle_buffer_size<span class=\"token punctuation\">)</span><span class=\"token punctuation\">:</span>\n            <span class=\"token keyword\">return</span> tf<span class=\"token punctuation\">.</span>data<span class=\"token punctuation\">.</span>Dataset<span class=\"token punctuation\">.</span>from_tensor_slices<span class=\"token punctuation\">(</span>data<span class=\"token punctuation\">)</span>\\\n                <span class=\"token punctuation\">.</span>cache<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span>\\\n                <span class=\"token punctuation\">.</span>shuffle<span class=\"token punctuation\">(</span>buffer_size<span class=\"token operator\">=</span>shuffle_buffer_size<span class=\"token punctuation\">)</span>\\\n                <span class=\"token punctuation\">.</span>batch<span class=\"token punctuation\">(</span>flags_obj<span class=\"token punctuation\">.</span>batch_size<span class=\"token punctuation\">)</span>\\\n                <span class=\"token punctuation\">.</span>repeat<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span>\\\n                <span class=\"token punctuation\">.</span>make_one_shot_iterator<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span>\n\n        <span class=\"token comment\"># Setup pipeline for labeled data</span>\n        train_ds_lab <span class=\"token operator\">=</span> train_pipeline<span class=\"token punctuation\">(</span>\n            <span class=\"token punctuation\">(</span>train_x_labeled<span class=\"token punctuation\">,</span> train_y_labeled<span class=\"token punctuation\">.</span>astype<span class=\"token punctuation\">(</span>np<span class=\"token punctuation\">.</span>int64<span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">,</span>\n            flags_obj<span class=\"token punctuation\">.</span>num_labeled_examples <span class=\"token operator\">*</span> flags_obj<span class=\"token punctuation\">.</span>num_classes<span class=\"token punctuation\">)</span>\n        images_lab<span class=\"token punctuation\">,</span> labels_lab <span class=\"token operator\">=</span> train_ds_lab<span class=\"token punctuation\">.</span>get_next<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span>\n\n        <span class=\"token comment\"># Setup pipeline for unlabeled data</span>\n        train_ds_unl <span class=\"token operator\">=</span> train_pipeline<span class=\"token punctuation\">(</span>\n            <span class=\"token punctuation\">(</span>train_x_unlabeled<span class=\"token punctuation\">,</span> train_y_unlabeled<span class=\"token punctuation\">.</span>astype<span class=\"token punctuation\">(</span>np<span class=\"token punctuation\">.</span>int64<span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">,</span> <span class=\"token builtin\">len</span><span class=\"token punctuation\">(</span>train_x_labeled<span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span>\n        images_unl<span class=\"token punctuation\">,</span> labels_unl <span class=\"token operator\">=</span> train_ds_unl<span class=\"token punctuation\">.</span>get_next<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span>\n\n        <span class=\"token comment\"># Setup another pipeline that also uses the unlabeled data, so that we use a different</span>\n        <span class=\"token comment\"># batch for computing the discriminator loss and the generator loss</span>\n        train_x_unlabeled<span class=\"token punctuation\">,</span> train_y_unlabeled <span class=\"token operator\">=</span> shuffle<span class=\"token punctuation\">(</span>train_x_unlabeled<span class=\"token punctuation\">,</span> train_y_unlabeled<span class=\"token punctuation\">)</span>\n        train_ds_unl2 <span class=\"token operator\">=</span> train_pipeline<span class=\"token punctuation\">(</span>\n            <span class=\"token punctuation\">(</span>train_x_unlabeled<span class=\"token punctuation\">,</span> train_y_unlabeled<span class=\"token punctuation\">.</span>astype<span class=\"token punctuation\">(</span>np<span class=\"token punctuation\">.</span>int64<span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">,</span> <span class=\"token builtin\">len</span><span class=\"token punctuation\">(</span>train_x_labeled<span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span>\n        images_unl2<span class=\"token punctuation\">,</span> labels_unl2 <span class=\"token operator\">=</span> train_ds_unl2<span class=\"token punctuation\">.</span>get_next<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span>\n\n        <span class=\"token comment\"># Setup pipeline for test data</span>\n        test_ds <span class=\"token operator\">=</span> tf<span class=\"token punctuation\">.</span>data<span class=\"token punctuation\">.</span>Dataset<span class=\"token punctuation\">.</span>from_tensor_slices<span class=\"token punctuation\">(</span><span class=\"token punctuation\">(</span>test_x<span class=\"token punctuation\">,</span> test_y<span class=\"token punctuation\">.</span>astype<span class=\"token punctuation\">(</span>np<span class=\"token punctuation\">.</span>int64<span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span>\\\n            <span class=\"token punctuation\">.</span>cache<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span>\\\n            <span class=\"token punctuation\">.</span>batch<span class=\"token punctuation\">(</span>flags_obj<span class=\"token punctuation\">.</span>batch_size<span class=\"token punctuation\">)</span>\\\n            <span class=\"token punctuation\">.</span>repeat<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span>\\\n            <span class=\"token punctuation\">.</span>make_one_shot_iterator<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span>\n        images_test<span class=\"token punctuation\">,</span> labels_test <span class=\"token operator\">=</span> test_ds<span class=\"token punctuation\">.</span>get_next<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span>\n\n    <span class=\"token keyword\">return</span> <span class=\"token punctuation\">(</span>images_lab<span class=\"token punctuation\">,</span> labels_lab<span class=\"token punctuation\">)</span><span class=\"token punctuation\">,</span> <span class=\"token punctuation\">(</span>images_unl<span class=\"token punctuation\">,</span> labels_unl<span class=\"token punctuation\">)</span><span class=\"token punctuation\">,</span> <span class=\"token punctuation\">(</span>images_unl2<span class=\"token punctuation\">,</span> labels_unl2<span class=\"token punctuation\">)</span><span class=\"token punctuation\">,</span> \\\n           <span class=\"token punctuation\">(</span>images_test<span class=\"token punctuation\">,</span> labels_test<span class=\"token punctuation\">)</span></code></pre></div>\n<p>Next up is to define the discriminator network. I have deviated quite a bit from the architecture in the <a href=\"https://arxiv.org/abs/1805.08957\">paper</a>. I’m going to play safe here and just use Keras layers to construct the model. Actually, this enables us to very conveniently reuse all weights for different input tensors, which will prove to be useful later on. In short, the discriminator’s architecture uses 3 convolutions with <span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mn>5</mn><mo>×</mo><mn>5</mn></mrow><annotation encoding=\"application/x-tex\">5\\times 5</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.72777em;vertical-align:-0.08333em;\"></span><span class=\"mord\">5</span><span class=\"mspace\" style=\"margin-right:0.2222222222222222em;\"></span><span class=\"mbin\">×</span><span class=\"mspace\" style=\"margin-right:0.2222222222222222em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:0.64444em;vertical-align:0em;\"></span><span class=\"mord\">5</span></span></span></span> kernels and strides of <span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mn>2</mn><mo>×</mo><mn>2</mn></mrow><annotation encoding=\"application/x-tex\">2\\times 2</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.72777em;vertical-align:-0.08333em;\"></span><span class=\"mord\">2</span><span class=\"mspace\" style=\"margin-right:0.2222222222222222em;\"></span><span class=\"mbin\">×</span><span class=\"mspace\" style=\"margin-right:0.2222222222222222em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:0.64444em;vertical-align:0em;\"></span><span class=\"mord\">2</span></span></span></span>, <span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mn>2</mn><mo>×</mo><mn>2</mn></mrow><annotation encoding=\"application/x-tex\">2 \\times 2</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.72777em;vertical-align:-0.08333em;\"></span><span class=\"mord\">2</span><span class=\"mspace\" style=\"margin-right:0.2222222222222222em;\"></span><span class=\"mbin\">×</span><span class=\"mspace\" style=\"margin-right:0.2222222222222222em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:0.64444em;vertical-align:0em;\"></span><span class=\"mord\">2</span></span></span></span> and <span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mn>1</mn><mo>×</mo><mn>1</mn></mrow><annotation encoding=\"application/x-tex\">1 \\times 1</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.72777em;vertical-align:-0.08333em;\"></span><span class=\"mord\">1</span><span class=\"mspace\" style=\"margin-right:0.2222222222222222em;\"></span><span class=\"mbin\">×</span><span class=\"mspace\" style=\"margin-right:0.2222222222222222em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:0.64444em;vertical-align:0em;\"></span><span class=\"mord\">1</span></span></span></span> respectively. Each convolution is followed by a leaky ReLU activation and a dropout layer with a dropout rate of 0.3. The flattened output of this stack of convolutions will be used as the <em>feature</em> layer.</p>\n<p>The feature layer can be used for a <a href=\"https://arxiv.org/abs/1606.03498\">feature matching loss</a> (rather than a sigmoid cross-entropy loss as in vanilla GANs), which has proven to yield a more reliable training process. The part of the network up to this feature layer is defined in <code class=\"language-text\">_define_tail</code> in the snippet below. The <code class=\"language-text\">_define_head</code> method defines the rest of the network. The ‘head’ of the network introduces only one additional fully connected layer with 10 outputs, that correspond to the logits of the class labels. Other than that, there are some methods to make the interface of a <code class=\"language-text\">Discriminator</code> instance behave similar to that of a <code class=\"language-text\">tf.keras.models.Sequential</code> instance.</p>\n<div class=\"gatsby-highlight\" data-language=\"python\"><pre class=\"language-python\"><code class=\"language-python\"><span class=\"token keyword\">class</span> <span class=\"token class-name\">Discriminator</span><span class=\"token punctuation\">:</span>\n\n    <span class=\"token keyword\">def</span> <span class=\"token function\">__init__</span><span class=\"token punctuation\">(</span>self<span class=\"token punctuation\">)</span><span class=\"token punctuation\">:</span>\n        <span class=\"token triple-quoted-string string\">\"\"\"The discriminator network. Split up in a 'tail' and 'head' network, so that we can\n        easily get the \"\"\"</span>\n        self<span class=\"token punctuation\">.</span>tail <span class=\"token operator\">=</span> self<span class=\"token punctuation\">.</span>_define_tail<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span>\n        self<span class=\"token punctuation\">.</span>head <span class=\"token operator\">=</span> self<span class=\"token punctuation\">.</span>_define_head<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span>\n\n    <span class=\"token keyword\">def</span> <span class=\"token function\">_define_tail</span><span class=\"token punctuation\">(</span>self<span class=\"token punctuation\">,</span> name<span class=\"token operator\">=</span><span class=\"token string\">\"Discriminator\"</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">:</span>\n        <span class=\"token triple-quoted-string string\">\"\"\"Defines the network until the intermediate layer that can be used for feature-matching\n        loss.\"\"\"</span>\n        feature_model <span class=\"token operator\">=</span> models<span class=\"token punctuation\">.</span>Sequential<span class=\"token punctuation\">(</span>name<span class=\"token operator\">=</span>name<span class=\"token punctuation\">)</span>\n\n        <span class=\"token keyword\">def</span> <span class=\"token function\">conv2d_dropout</span><span class=\"token punctuation\">(</span>filters<span class=\"token punctuation\">,</span> strides<span class=\"token punctuation\">,</span> index<span class=\"token operator\">=</span><span class=\"token number\">0</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">:</span>\n            <span class=\"token comment\"># Adds a convolution followed by a Dropout layer</span>\n            suffix <span class=\"token operator\">=</span> <span class=\"token builtin\">str</span><span class=\"token punctuation\">(</span>index<span class=\"token punctuation\">)</span>\n            feature_model<span class=\"token punctuation\">.</span>add<span class=\"token punctuation\">(</span>layers<span class=\"token punctuation\">.</span>Conv2D<span class=\"token punctuation\">(</span>\n                filters<span class=\"token operator\">=</span>filters<span class=\"token punctuation\">,</span> strides<span class=\"token operator\">=</span>strides<span class=\"token punctuation\">,</span> name<span class=\"token operator\">=</span><span class=\"token string\">\"Conv{}\"</span><span class=\"token punctuation\">.</span><span class=\"token builtin\">format</span><span class=\"token punctuation\">(</span>suffix<span class=\"token punctuation\">)</span><span class=\"token punctuation\">,</span> padding<span class=\"token operator\">=</span><span class=\"token string\">'same'</span><span class=\"token punctuation\">,</span>\n                kernel_size<span class=\"token operator\">=</span><span class=\"token number\">5</span><span class=\"token punctuation\">,</span> activation<span class=\"token operator\">=</span>tf<span class=\"token punctuation\">.</span>nn<span class=\"token punctuation\">.</span>leaky_relu<span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span>\n            feature_model<span class=\"token punctuation\">.</span>add<span class=\"token punctuation\">(</span>layers<span class=\"token punctuation\">.</span>Dropout<span class=\"token punctuation\">(</span>name<span class=\"token operator\">=</span><span class=\"token string\">\"Dropout{}\"</span><span class=\"token punctuation\">.</span><span class=\"token builtin\">format</span><span class=\"token punctuation\">(</span>suffix<span class=\"token punctuation\">)</span><span class=\"token punctuation\">,</span> rate<span class=\"token operator\">=</span><span class=\"token number\">0.3</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span>\n\n        <span class=\"token comment\"># Three blocks of convs and dropouts. They all have 5x5 kernels, leaky ReLU and 0.3</span>\n        <span class=\"token comment\"># dropout rate.</span>\n        conv2d_dropout<span class=\"token punctuation\">(</span>filters<span class=\"token operator\">=</span><span class=\"token number\">32</span><span class=\"token punctuation\">,</span> strides<span class=\"token operator\">=</span><span class=\"token number\">2</span><span class=\"token punctuation\">,</span> index<span class=\"token operator\">=</span><span class=\"token number\">0</span><span class=\"token punctuation\">)</span>\n        conv2d_dropout<span class=\"token punctuation\">(</span>filters<span class=\"token operator\">=</span><span class=\"token number\">64</span><span class=\"token punctuation\">,</span> strides<span class=\"token operator\">=</span><span class=\"token number\">2</span><span class=\"token punctuation\">,</span> index<span class=\"token operator\">=</span><span class=\"token number\">1</span><span class=\"token punctuation\">)</span>\n        conv2d_dropout<span class=\"token punctuation\">(</span>filters<span class=\"token operator\">=</span><span class=\"token number\">64</span><span class=\"token punctuation\">,</span> strides<span class=\"token operator\">=</span><span class=\"token number\">1</span><span class=\"token punctuation\">,</span> index<span class=\"token operator\">=</span><span class=\"token number\">2</span><span class=\"token punctuation\">)</span>\n\n        <span class=\"token comment\"># Flatten it and build logits layer</span>\n        feature_model<span class=\"token punctuation\">.</span>add<span class=\"token punctuation\">(</span>layers<span class=\"token punctuation\">.</span>Flatten<span class=\"token punctuation\">(</span>name<span class=\"token operator\">=</span><span class=\"token string\">\"Flatten\"</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span>\n        <span class=\"token keyword\">return</span> feature_model\n\n    <span class=\"token keyword\">def</span> <span class=\"token function\">_define_head</span><span class=\"token punctuation\">(</span>self<span class=\"token punctuation\">)</span><span class=\"token punctuation\">:</span>\n        <span class=\"token comment\"># Defines the remaining layers after the 'tail'</span>\n        head_model <span class=\"token operator\">=</span> models<span class=\"token punctuation\">.</span>Sequential<span class=\"token punctuation\">(</span>name<span class=\"token operator\">=</span><span class=\"token string\">\"DiscriminatorHead\"</span><span class=\"token punctuation\">)</span>\n        head_model<span class=\"token punctuation\">.</span>add<span class=\"token punctuation\">(</span>layers<span class=\"token punctuation\">.</span>Dense<span class=\"token punctuation\">(</span>units<span class=\"token operator\">=</span><span class=\"token number\">10</span><span class=\"token punctuation\">,</span> activation<span class=\"token operator\">=</span><span class=\"token boolean\">None</span><span class=\"token punctuation\">,</span> name<span class=\"token operator\">=</span><span class=\"token string\">\"Logits\"</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span>\n        <span class=\"token keyword\">return</span> head_model\n\n    @<span class=\"token builtin\">property</span>\n    <span class=\"token keyword\">def</span> <span class=\"token function\">trainable_variables</span><span class=\"token punctuation\">(</span>self<span class=\"token punctuation\">)</span><span class=\"token punctuation\">:</span>\n        <span class=\"token comment\"># Return both tail's parameters a well as those of the head</span>\n        <span class=\"token keyword\">return</span> self<span class=\"token punctuation\">.</span>tail<span class=\"token punctuation\">.</span>trainable_variables <span class=\"token operator\">+</span> self<span class=\"token punctuation\">.</span>head<span class=\"token punctuation\">.</span>trainable_variables\n\n    <span class=\"token keyword\">def</span> <span class=\"token function\">__call__</span><span class=\"token punctuation\">(</span>self<span class=\"token punctuation\">,</span> x<span class=\"token punctuation\">,</span> <span class=\"token operator\">*</span>args<span class=\"token punctuation\">,</span> <span class=\"token operator\">**</span>kwargs<span class=\"token punctuation\">)</span><span class=\"token punctuation\">:</span>\n        <span class=\"token comment\"># By adding this, the code below can treat a Discriminator instance as a</span>\n        <span class=\"token comment\"># tf.keras.models.Sequential instance</span>\n        features <span class=\"token operator\">=</span> self<span class=\"token punctuation\">.</span>tail<span class=\"token punctuation\">(</span>x<span class=\"token punctuation\">,</span> <span class=\"token operator\">*</span>args<span class=\"token punctuation\">,</span> <span class=\"token operator\">**</span>kwargs<span class=\"token punctuation\">)</span>\n        <span class=\"token keyword\">return</span> self<span class=\"token punctuation\">.</span>head<span class=\"token punctuation\">(</span>features<span class=\"token punctuation\">,</span> <span class=\"token operator\">*</span>args<span class=\"token punctuation\">,</span> <span class=\"token operator\">**</span>kwargs<span class=\"token punctuation\">)</span><span class=\"token punctuation\">,</span> features</code></pre></div>\n<p>The generator’s architecture also uses <span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mn>5</mn><mo>×</mo><mn>5</mn></mrow><annotation encoding=\"application/x-tex\">5 \\times 5</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.72777em;vertical-align:-0.08333em;\"></span><span class=\"mord\">5</span><span class=\"mspace\" style=\"margin-right:0.2222222222222222em;\"></span><span class=\"mbin\">×</span><span class=\"mspace\" style=\"margin-right:0.2222222222222222em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:0.64444em;vertical-align:0em;\"></span><span class=\"mord\">5</span></span></span></span> kernels. Many implementations of DCGAN-like architectures use transposed convolutions (sometimes wrongfully referred to as ‘deconvolutions’). I have decided to give the <em>upsampling-convolution</em> alternative a try. This should alleviate the issue of the <a href=\"https://distill.pub/2016/deconv-checkerboard/\">checkerboard pattern</a> that sometimes appears in generated images. Other than that, there are ReLU nonlinearities, and a first layer to go from the 100-dimensional noise to a (rather awkwardly shaped) <span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mn>7</mn><mo>×</mo><mn>7</mn><mo>×</mo><mn>64</mn></mrow><annotation encoding=\"application/x-tex\">7 \\times 7 \\times 64</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.72777em;vertical-align:-0.08333em;\"></span><span class=\"mord\">7</span><span class=\"mspace\" style=\"margin-right:0.2222222222222222em;\"></span><span class=\"mbin\">×</span><span class=\"mspace\" style=\"margin-right:0.2222222222222222em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:0.72777em;vertical-align:-0.08333em;\"></span><span class=\"mord\">7</span><span class=\"mspace\" style=\"margin-right:0.2222222222222222em;\"></span><span class=\"mbin\">×</span><span class=\"mspace\" style=\"margin-right:0.2222222222222222em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:0.64444em;vertical-align:0em;\"></span><span class=\"mord\">6</span><span class=\"mord\">4</span></span></span></span> spatial representation.</p>\n<div class=\"gatsby-highlight\" data-language=\"python\"><pre class=\"language-python\"><code class=\"language-python\"><span class=\"token keyword\">def</span> <span class=\"token function\">define_generator</span><span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">:</span>\n    model <span class=\"token operator\">=</span> models<span class=\"token punctuation\">.</span>Sequential<span class=\"token punctuation\">(</span>name<span class=\"token operator\">=</span><span class=\"token string\">\"Generator\"</span><span class=\"token punctuation\">)</span>\n\n    <span class=\"token keyword\">def</span> <span class=\"token function\">conv2d_block</span><span class=\"token punctuation\">(</span>filters<span class=\"token punctuation\">,</span> upsample<span class=\"token operator\">=</span><span class=\"token boolean\">True</span><span class=\"token punctuation\">,</span> activation<span class=\"token operator\">=</span>tf<span class=\"token punctuation\">.</span>nn<span class=\"token punctuation\">.</span>relu<span class=\"token punctuation\">,</span> index<span class=\"token operator\">=</span><span class=\"token number\">0</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">:</span>\n        <span class=\"token keyword\">if</span> upsample<span class=\"token punctuation\">:</span>\n            model<span class=\"token punctuation\">.</span>add<span class=\"token punctuation\">(</span>layers<span class=\"token punctuation\">.</span>UpSampling2D<span class=\"token punctuation\">(</span>name<span class=\"token operator\">=</span><span class=\"token string\">\"UpSampling\"</span> <span class=\"token operator\">+</span> <span class=\"token builtin\">str</span><span class=\"token punctuation\">(</span>index<span class=\"token punctuation\">)</span><span class=\"token punctuation\">,</span> size<span class=\"token operator\">=</span><span class=\"token punctuation\">(</span><span class=\"token number\">2</span><span class=\"token punctuation\">,</span> <span class=\"token number\">2</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span>\n        model<span class=\"token punctuation\">.</span>add<span class=\"token punctuation\">(</span>layers<span class=\"token punctuation\">.</span>Conv2D<span class=\"token punctuation\">(</span>\n            filters<span class=\"token operator\">=</span>filters<span class=\"token punctuation\">,</span> kernel_size<span class=\"token operator\">=</span><span class=\"token number\">5</span><span class=\"token punctuation\">,</span> padding<span class=\"token operator\">=</span><span class=\"token string\">'same'</span><span class=\"token punctuation\">,</span> name<span class=\"token operator\">=</span><span class=\"token string\">\"Conv2D\"</span> <span class=\"token operator\">+</span> <span class=\"token builtin\">str</span><span class=\"token punctuation\">(</span>index<span class=\"token punctuation\">)</span><span class=\"token punctuation\">,</span>\n            activation<span class=\"token operator\">=</span>activation<span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span>\n\n    <span class=\"token comment\"># From flat noise to spatial</span>\n    model<span class=\"token punctuation\">.</span>add<span class=\"token punctuation\">(</span>layers<span class=\"token punctuation\">.</span>Dense<span class=\"token punctuation\">(</span><span class=\"token number\">7</span> <span class=\"token operator\">*</span> <span class=\"token number\">7</span> <span class=\"token operator\">*</span> <span class=\"token number\">64</span><span class=\"token punctuation\">,</span> activation<span class=\"token operator\">=</span>tf<span class=\"token punctuation\">.</span>nn<span class=\"token punctuation\">.</span>relu<span class=\"token punctuation\">,</span> name<span class=\"token operator\">=</span><span class=\"token string\">\"NoiseToSpatial\"</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span>\n    model<span class=\"token punctuation\">.</span>add<span class=\"token punctuation\">(</span>layers<span class=\"token punctuation\">.</span>Reshape<span class=\"token punctuation\">(</span><span class=\"token punctuation\">(</span><span class=\"token number\">7</span><span class=\"token punctuation\">,</span> <span class=\"token number\">7</span><span class=\"token punctuation\">,</span> <span class=\"token number\">64</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span>\n\n    <span class=\"token comment\"># Four blocks of convolutions, 2 that upsample and convolve, and 2 more that</span>\n    <span class=\"token comment\"># just convolve</span>\n    conv2d_block<span class=\"token punctuation\">(</span>filters<span class=\"token operator\">=</span><span class=\"token number\">128</span><span class=\"token punctuation\">,</span> upsample<span class=\"token operator\">=</span><span class=\"token boolean\">True</span><span class=\"token punctuation\">,</span> index<span class=\"token operator\">=</span><span class=\"token number\">0</span><span class=\"token punctuation\">)</span>\n    conv2d_block<span class=\"token punctuation\">(</span>filters<span class=\"token operator\">=</span><span class=\"token number\">64</span><span class=\"token punctuation\">,</span> upsample<span class=\"token operator\">=</span><span class=\"token boolean\">True</span><span class=\"token punctuation\">,</span> index<span class=\"token operator\">=</span><span class=\"token number\">1</span><span class=\"token punctuation\">)</span>\n    conv2d_block<span class=\"token punctuation\">(</span>filters<span class=\"token operator\">=</span><span class=\"token number\">64</span><span class=\"token punctuation\">,</span> upsample<span class=\"token operator\">=</span><span class=\"token boolean\">False</span><span class=\"token punctuation\">,</span> index<span class=\"token operator\">=</span><span class=\"token number\">2</span><span class=\"token punctuation\">)</span>\n    conv2d_block<span class=\"token punctuation\">(</span>filters<span class=\"token operator\">=</span><span class=\"token number\">1</span><span class=\"token punctuation\">,</span> upsample<span class=\"token operator\">=</span><span class=\"token boolean\">False</span><span class=\"token punctuation\">,</span> activation<span class=\"token operator\">=</span>tf<span class=\"token punctuation\">.</span>nn<span class=\"token punctuation\">.</span>tanh<span class=\"token punctuation\">,</span> index<span class=\"token operator\">=</span><span class=\"token number\">3</span><span class=\"token punctuation\">)</span>\n    <span class=\"token keyword\">return</span> model</code></pre></div>\n<p>I have tried to make this model work with what TensorFlow’s Keras layers have to offer so that the code would be easy to digest (and to implement of course). This also means that I have deviated from the architectures in <a href=\"https://arxiv.org/abs/1805.08957\">the paper</a> (e.g. I’m not using weight normalization). Because of this experimental approach, I have also experienced just how sensitive the training setup is to small variations in network architectures and parameters. There are plenty of neat GAN ‘hacks’ listed <a href=\"https://github.com/soumith/ganhacks\">in this repo</a> which I definitely found insightful.</p>\n<h3>Putting It Together</h3>\n<p>Let’s do the forward computations now so that we see how all of the above comes together. This consists of setting up the input pipeline, noise vector, generator and discriminator. The snippet below does all of this. Note that when <code class=\"language-text\">define_generator</code> returns the <code class=\"language-text\">Sequential</code> instance, we can just use it as a functor to obtain the output of it for the noise tensor given by <span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mi mathvariant=\"bold-italic\">z</mi></mrow><annotation encoding=\"application/x-tex\">\\boldsymbol z</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.44444em;vertical-align:0em;\"></span><span class=\"mord\"><span class=\"mord boldsymbol\" style=\"margin-right:0.04213em;\">z</span></span></span></span></span>.</p>\n<p>The discriminator will do a lot more. It will take (i) the ‘fake’ images coming from the generator, (ii) a batch of unlabeled images and finally (iii) a batch of labeled images (both with and <em>without</em> dropout to also report the train accuracy). We can just repetitively call the <code class=\"language-text\">Discriminator</code> instance to build the graph for each of those outputs. Keras will make sure that the variables are reused in all cases. To turn off dropout for the labeled training data, we have to pass <code class=\"language-text\">training=False</code> explicitly.</p>\n<div class=\"gatsby-highlight\" data-language=\"python\"><pre class=\"language-python\"><code class=\"language-python\"><span class=\"token punctuation\">(</span>images_lab<span class=\"token punctuation\">,</span> labels_lab<span class=\"token punctuation\">)</span><span class=\"token punctuation\">,</span> <span class=\"token punctuation\">(</span>images_unl<span class=\"token punctuation\">,</span> labels_unl<span class=\"token punctuation\">)</span><span class=\"token punctuation\">,</span> <span class=\"token punctuation\">(</span>images_unl2<span class=\"token punctuation\">,</span> labels_unl2<span class=\"token punctuation\">)</span><span class=\"token punctuation\">,</span> \\\n            <span class=\"token punctuation\">(</span>images_test<span class=\"token punctuation\">,</span> labels_test<span class=\"token punctuation\">)</span> <span class=\"token operator\">=</span> prepare_input_pipeline<span class=\"token punctuation\">(</span>flags_obj<span class=\"token punctuation\">)</span>\n\n<span class=\"token keyword\">with</span> tf<span class=\"token punctuation\">.</span>name_scope<span class=\"token punctuation\">(</span><span class=\"token string\">\"BatchSize\"</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">:</span>\n    batch_size_tensor <span class=\"token operator\">=</span> tf<span class=\"token punctuation\">.</span>shape<span class=\"token punctuation\">(</span>images_lab<span class=\"token punctuation\">)</span><span class=\"token punctuation\">[</span><span class=\"token number\">0</span><span class=\"token punctuation\">]</span>\n\n<span class=\"token comment\"># Get the noise vectors</span>\nz<span class=\"token punctuation\">,</span> z_perturbed <span class=\"token operator\">=</span> define_noise<span class=\"token punctuation\">(</span>batch_size_tensor<span class=\"token punctuation\">,</span> flags_obj<span class=\"token punctuation\">)</span>\n\n<span class=\"token comment\"># Generate images from noise vector</span>\n<span class=\"token keyword\">with</span> tf<span class=\"token punctuation\">.</span>name_scope<span class=\"token punctuation\">(</span><span class=\"token string\">\"Generator\"</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">:</span>\n    g_model <span class=\"token operator\">=</span> define_generator<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span>\n    images_fake <span class=\"token operator\">=</span> g_model<span class=\"token punctuation\">(</span>z<span class=\"token punctuation\">)</span>\n    images_fake_perturbed <span class=\"token operator\">=</span> g_model<span class=\"token punctuation\">(</span>z_perturbed<span class=\"token punctuation\">)</span>\n\n<span class=\"token comment\"># Discriminate between real and fake, and try to classify the labeled data</span>\n<span class=\"token keyword\">with</span> tf<span class=\"token punctuation\">.</span>name_scope<span class=\"token punctuation\">(</span><span class=\"token string\">\"Discriminator\"</span><span class=\"token punctuation\">)</span> <span class=\"token keyword\">as</span> discriminator_scope<span class=\"token punctuation\">:</span>\n    d_model <span class=\"token operator\">=</span> Discriminator<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span>\n    logits_fake<span class=\"token punctuation\">,</span> features_fake          <span class=\"token operator\">=</span> d_model<span class=\"token punctuation\">(</span>images_fake<span class=\"token punctuation\">,</span> training<span class=\"token operator\">=</span><span class=\"token boolean\">True</span><span class=\"token punctuation\">)</span>\n    logits_fake_perturbed<span class=\"token punctuation\">,</span> _            <span class=\"token operator\">=</span> d_model<span class=\"token punctuation\">(</span>images_fake_perturbed<span class=\"token punctuation\">,</span> training<span class=\"token operator\">=</span><span class=\"token boolean\">True</span><span class=\"token punctuation\">)</span>\n    logits_real_unl<span class=\"token punctuation\">,</span> features_real_unl  <span class=\"token operator\">=</span> d_model<span class=\"token punctuation\">(</span>images_unl<span class=\"token punctuation\">,</span> training<span class=\"token operator\">=</span><span class=\"token boolean\">True</span><span class=\"token punctuation\">)</span>\n    logits_real_lab<span class=\"token punctuation\">,</span> features_real_lab  <span class=\"token operator\">=</span> d_model<span class=\"token punctuation\">(</span>images_lab<span class=\"token punctuation\">,</span> training<span class=\"token operator\">=</span><span class=\"token boolean\">True</span><span class=\"token punctuation\">)</span>\n    logits_train<span class=\"token punctuation\">,</span> _                     <span class=\"token operator\">=</span> d_model<span class=\"token punctuation\">(</span>images_lab<span class=\"token punctuation\">,</span> training<span class=\"token operator\">=</span><span class=\"token boolean\">False</span><span class=\"token punctuation\">)</span></code></pre></div>\n<h3>The Discriminator’s Loss</h3>\n<p>Recall that the discriminator will be doing more than just separating fake from real. It also classifies the labeled data. For this, we define a supervised loss which takes the softmax output. In terms of implementation, this means that we feed the unnormalized logits to <code class=\"language-text\">tf.nn.sparse_cross_entropy_with_logits</code>.</p>\n<p>Defining the loss for the unsupervised part is where things get a little bit more involved. Because the softmax distribution is overparameterized, we can fix the <em>unnormalized logit</em> at 0 for an image to be fake (i.e. coming from the generator). If we do so, the probability of it being real just turns into:</p>\n<span class=\"katex-display\"><span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mi>p</mi><mo stretchy=\"false\">(</mo><mi>x</mi><mo stretchy=\"false\">)</mo><mo>=</mo><mfrac><mrow><mi>Z</mi><mo stretchy=\"false\">(</mo><mi>x</mi><mo stretchy=\"false\">)</mo></mrow><mrow><mi>Z</mi><mo stretchy=\"false\">(</mo><mi>x</mi><mo stretchy=\"false\">)</mo><mo>+</mo><mi>exp</mi><mo>⁡</mo><mo stretchy=\"false\">(</mo><msub><mi>l</mi><mtext>fake</mtext></msub><mo stretchy=\"false\">)</mo></mrow></mfrac><mo>=</mo><mfrac><mrow><mi>Z</mi><mo stretchy=\"false\">(</mo><mi>x</mi><mo stretchy=\"false\">)</mo></mrow><mrow><mi>Z</mi><mo stretchy=\"false\">(</mo><mi>x</mi><mo stretchy=\"false\">)</mo><mo>+</mo><mn>1</mn></mrow></mfrac></mrow><annotation encoding=\"application/x-tex\">p(x) = \\frac{Z(x)}{Z(x) + \\exp(l_{\\text{fake}})} = \\frac{Z(x)}{Z(x) + 1}</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:1em;vertical-align:-0.25em;\"></span><span class=\"mord mathdefault\">p</span><span class=\"mopen\">(</span><span class=\"mord mathdefault\">x</span><span class=\"mclose\">)</span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span><span class=\"mrel\">=</span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:2.363em;vertical-align:-0.936em;\"></span><span class=\"mord\"><span class=\"mopen nulldelimiter\"></span><span class=\"mfrac\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:1.427em;\"><span style=\"top:-2.314em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span class=\"mord\"><span class=\"mord mathdefault\" style=\"margin-right:0.07153em;\">Z</span><span class=\"mopen\">(</span><span class=\"mord mathdefault\">x</span><span class=\"mclose\">)</span><span class=\"mspace\" style=\"margin-right:0.2222222222222222em;\"></span><span class=\"mbin\">+</span><span class=\"mspace\" style=\"margin-right:0.2222222222222222em;\"></span><span class=\"mop\">exp</span><span class=\"mopen\">(</span><span class=\"mord\"><span class=\"mord mathdefault\" style=\"margin-right:0.01968em;\">l</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.33610799999999996em;\"><span style=\"top:-2.5500000000000003em;margin-left:-0.01968em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\"><span class=\"mord text mtight\"><span class=\"mord mtight\">fake</span></span></span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span><span class=\"mclose\">)</span></span></span><span style=\"top:-3.23em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span class=\"frac-line\" style=\"border-bottom-width:0.04em;\"></span></span><span style=\"top:-3.677em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span class=\"mord\"><span class=\"mord mathdefault\" style=\"margin-right:0.07153em;\">Z</span><span class=\"mopen\">(</span><span class=\"mord mathdefault\">x</span><span class=\"mclose\">)</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.936em;\"><span></span></span></span></span></span><span class=\"mclose nulldelimiter\"></span></span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span><span class=\"mrel\">=</span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:2.363em;vertical-align:-0.936em;\"></span><span class=\"mord\"><span class=\"mopen nulldelimiter\"></span><span class=\"mfrac\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:1.427em;\"><span style=\"top:-2.314em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span class=\"mord\"><span class=\"mord mathdefault\" style=\"margin-right:0.07153em;\">Z</span><span class=\"mopen\">(</span><span class=\"mord mathdefault\">x</span><span class=\"mclose\">)</span><span class=\"mspace\" style=\"margin-right:0.2222222222222222em;\"></span><span class=\"mbin\">+</span><span class=\"mspace\" style=\"margin-right:0.2222222222222222em;\"></span><span class=\"mord\">1</span></span></span><span style=\"top:-3.23em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span class=\"frac-line\" style=\"border-bottom-width:0.04em;\"></span></span><span style=\"top:-3.677em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span class=\"mord\"><span class=\"mord mathdefault\" style=\"margin-right:0.07153em;\">Z</span><span class=\"mopen\">(</span><span class=\"mord mathdefault\">x</span><span class=\"mclose\">)</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.936em;\"><span></span></span></span></span></span><span class=\"mclose nulldelimiter\"></span></span></span></span></span></span>\n<p>where <span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mi>Z</mi><mo stretchy=\"false\">(</mo><mi>x</mi><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">Z(x)</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:1em;vertical-align:-0.25em;\"></span><span class=\"mord mathdefault\" style=\"margin-right:0.07153em;\">Z</span><span class=\"mopen\">(</span><span class=\"mord mathdefault\">x</span><span class=\"mclose\">)</span></span></span></span> is the sum of the <em>unnormalized probabilities</em>. Note that we currently only have the logits. Ultimately, we want to use the log-probability of the fake class to define our loss function. This can now be achieved by computing the whole expression in log-space:</p>\n<span class=\"katex-display\"><span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mi>log</mi><mo>⁡</mo><mo stretchy=\"false\">(</mo><mi>p</mi><mo stretchy=\"false\">(</mo><mi>x</mi><mo stretchy=\"false\">)</mo><mo stretchy=\"false\">)</mo><mo>=</mo><mi>log</mi><mo>⁡</mo><mo stretchy=\"false\">(</mo><mi>Z</mi><mo stretchy=\"false\">(</mo><mi>x</mi><mo stretchy=\"false\">)</mo><mo stretchy=\"false\">)</mo><mo>−</mo><mi>log</mi><mo>⁡</mo><mo stretchy=\"false\">(</mo><mn>1</mn><mo>+</mo><mi>Z</mi><mo stretchy=\"false\">(</mo><mi>x</mi><mo stretchy=\"false\">)</mo><mo stretchy=\"false\">)</mo><mo>=</mo><mtext mathvariant=\"monospace\">logsumexp</mtext><mo stretchy=\"false\">(</mo><msub><mi>l</mi><mn>1</mn></msub><mo separator=\"true\">,</mo><mo>…</mo><mo separator=\"true\">,</mo><msub><mi>l</mi><mi>K</mi></msub><mo stretchy=\"false\">)</mo><mo>−</mo><mtext mathvariant=\"monospace\">softplus</mtext><mo stretchy=\"false\">(</mo><mtext mathvariant=\"monospace\">logsumexp</mtext><mo stretchy=\"false\">(</mo><msub><mi>l</mi><mn>1</mn></msub><mo separator=\"true\">,</mo><mo>…</mo><mo separator=\"true\">,</mo><msub><mi>l</mi><mi>K</mi></msub><mo stretchy=\"false\">)</mo><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">\\log(p(x)) = \\log(Z(x)) - \\log(1 + Z(x)) = \\texttt{logsumexp}(l_1, \\ldots, l_K) - \\texttt{softplus}(\\texttt{logsumexp}(l_1, \\ldots, l_K))</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:1em;vertical-align:-0.25em;\"></span><span class=\"mop\">lo<span style=\"margin-right:0.01389em;\">g</span></span><span class=\"mopen\">(</span><span class=\"mord mathdefault\">p</span><span class=\"mopen\">(</span><span class=\"mord mathdefault\">x</span><span class=\"mclose\">)</span><span class=\"mclose\">)</span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span><span class=\"mrel\">=</span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:1em;vertical-align:-0.25em;\"></span><span class=\"mop\">lo<span style=\"margin-right:0.01389em;\">g</span></span><span class=\"mopen\">(</span><span class=\"mord mathdefault\" style=\"margin-right:0.07153em;\">Z</span><span class=\"mopen\">(</span><span class=\"mord mathdefault\">x</span><span class=\"mclose\">)</span><span class=\"mclose\">)</span><span class=\"mspace\" style=\"margin-right:0.2222222222222222em;\"></span><span class=\"mbin\">−</span><span class=\"mspace\" style=\"margin-right:0.2222222222222222em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:1em;vertical-align:-0.25em;\"></span><span class=\"mop\">lo<span style=\"margin-right:0.01389em;\">g</span></span><span class=\"mopen\">(</span><span class=\"mord\">1</span><span class=\"mspace\" style=\"margin-right:0.2222222222222222em;\"></span><span class=\"mbin\">+</span><span class=\"mspace\" style=\"margin-right:0.2222222222222222em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:1em;vertical-align:-0.25em;\"></span><span class=\"mord mathdefault\" style=\"margin-right:0.07153em;\">Z</span><span class=\"mopen\">(</span><span class=\"mord mathdefault\">x</span><span class=\"mclose\">)</span><span class=\"mclose\">)</span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span><span class=\"mrel\">=</span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:1em;vertical-align:-0.25em;\"></span><span class=\"mord text\"><span class=\"mord texttt\">logsumexp</span></span><span class=\"mopen\">(</span><span class=\"mord\"><span class=\"mord mathdefault\" style=\"margin-right:0.01968em;\">l</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.30110799999999993em;\"><span style=\"top:-2.5500000000000003em;margin-left:-0.01968em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\">1</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span><span class=\"mpunct\">,</span><span class=\"mspace\" style=\"margin-right:0.16666666666666666em;\"></span><span class=\"minner\">…</span><span class=\"mspace\" style=\"margin-right:0.16666666666666666em;\"></span><span class=\"mpunct\">,</span><span class=\"mspace\" style=\"margin-right:0.16666666666666666em;\"></span><span class=\"mord\"><span class=\"mord mathdefault\" style=\"margin-right:0.01968em;\">l</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.32833099999999993em;\"><span style=\"top:-2.5500000000000003em;margin-left:-0.01968em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mathdefault mtight\" style=\"margin-right:0.07153em;\">K</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span><span class=\"mclose\">)</span><span class=\"mspace\" style=\"margin-right:0.2222222222222222em;\"></span><span class=\"mbin\">−</span><span class=\"mspace\" style=\"margin-right:0.2222222222222222em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:1em;vertical-align:-0.25em;\"></span><span class=\"mord text\"><span class=\"mord texttt\">softplus</span></span><span class=\"mopen\">(</span><span class=\"mord text\"><span class=\"mord texttt\">logsumexp</span></span><span class=\"mopen\">(</span><span class=\"mord\"><span class=\"mord mathdefault\" style=\"margin-right:0.01968em;\">l</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.30110799999999993em;\"><span style=\"top:-2.5500000000000003em;margin-left:-0.01968em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\">1</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span><span class=\"mpunct\">,</span><span class=\"mspace\" style=\"margin-right:0.16666666666666666em;\"></span><span class=\"minner\">…</span><span class=\"mspace\" style=\"margin-right:0.16666666666666666em;\"></span><span class=\"mpunct\">,</span><span class=\"mspace\" style=\"margin-right:0.16666666666666666em;\"></span><span class=\"mord\"><span class=\"mord mathdefault\" style=\"margin-right:0.01968em;\">l</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.32833099999999993em;\"><span style=\"top:-2.5500000000000003em;margin-left:-0.01968em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mathdefault mtight\" style=\"margin-right:0.07153em;\">K</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span><span class=\"mclose\">)</span><span class=\"mclose\">)</span></span></span></span></span>\n<p>Where the lower case <span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mi>l</mi></mrow><annotation encoding=\"application/x-tex\">l</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.69444em;vertical-align:0em;\"></span><span class=\"mord mathdefault\" style=\"margin-right:0.01968em;\">l</span></span></span></span> with subscripts denote the individual logits. Divisions become subtractions and sums can be computed by the logsumexp function. Finally, we have used the definition of the <em>softplus</em> function:</p>\n<span class=\"katex-display\"><span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mtext mathvariant=\"monospace\">softplus</mtext><mo stretchy=\"false\">(</mo><mi>x</mi><mo stretchy=\"false\">)</mo><mo>=</mo><mi>log</mi><mo>⁡</mo><mo stretchy=\"false\">(</mo><mn>1</mn><mo>+</mo><mi>x</mi><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">\\texttt{softplus}(x) = \\log(1 + x)</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:1em;vertical-align:-0.25em;\"></span><span class=\"mord text\"><span class=\"mord texttt\">softplus</span></span><span class=\"mopen\">(</span><span class=\"mord mathdefault\">x</span><span class=\"mclose\">)</span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span><span class=\"mrel\">=</span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:1em;vertical-align:-0.25em;\"></span><span class=\"mop\">lo<span style=\"margin-right:0.01389em;\">g</span></span><span class=\"mopen\">(</span><span class=\"mord\">1</span><span class=\"mspace\" style=\"margin-right:0.2222222222222222em;\"></span><span class=\"mbin\">+</span><span class=\"mspace\" style=\"margin-right:0.2222222222222222em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:1em;vertical-align:-0.25em;\"></span><span class=\"mord mathdefault\">x</span><span class=\"mclose\">)</span></span></span></span></span>\n<p>In general, if you have the log-representation of a probability, it is numerically safer to keep things in log-space for as long as you can, since we are able to represent much smaller numbers in that case.</p>\n<p>We’re not there yet. Generative adversarial training asks us to ascend the gradient of:</p>\n<span class=\"katex-display\"><span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mi>log</mi><mo>⁡</mo><mo stretchy=\"false\">(</mo><mi>D</mi><mo stretchy=\"false\">(</mo><mi>x</mi><mo stretchy=\"false\">)</mo><mo stretchy=\"false\">)</mo><mo>+</mo><mi>log</mi><mo>⁡</mo><mo stretchy=\"false\">(</mo><mn>1</mn><mo>−</mo><mi>D</mi><mo stretchy=\"false\">(</mo><mi>G</mi><mo stretchy=\"false\">(</mo><mi mathvariant=\"bold-italic\">z</mi><mo stretchy=\"false\">)</mo><mo stretchy=\"false\">)</mo><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">\\log(D(x)) + \\log(1 - D(G(\\boldsymbol z)))</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:1em;vertical-align:-0.25em;\"></span><span class=\"mop\">lo<span style=\"margin-right:0.01389em;\">g</span></span><span class=\"mopen\">(</span><span class=\"mord mathdefault\" style=\"margin-right:0.02778em;\">D</span><span class=\"mopen\">(</span><span class=\"mord mathdefault\">x</span><span class=\"mclose\">)</span><span class=\"mclose\">)</span><span class=\"mspace\" style=\"margin-right:0.2222222222222222em;\"></span><span class=\"mbin\">+</span><span class=\"mspace\" style=\"margin-right:0.2222222222222222em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:1em;vertical-align:-0.25em;\"></span><span class=\"mop\">lo<span style=\"margin-right:0.01389em;\">g</span></span><span class=\"mopen\">(</span><span class=\"mord\">1</span><span class=\"mspace\" style=\"margin-right:0.2222222222222222em;\"></span><span class=\"mbin\">−</span><span class=\"mspace\" style=\"margin-right:0.2222222222222222em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:1em;vertical-align:-0.25em;\"></span><span class=\"mord mathdefault\" style=\"margin-right:0.02778em;\">D</span><span class=\"mopen\">(</span><span class=\"mord mathdefault\">G</span><span class=\"mopen\">(</span><span class=\"mord\"><span class=\"mord boldsymbol\" style=\"margin-right:0.04213em;\">z</span></span><span class=\"mclose\">)</span><span class=\"mclose\">)</span><span class=\"mclose\">)</span></span></span></span></span>\n<p>So whenever we call <code class=\"language-text\">tf.train.AdamOptimizer.minimize</code> we should <em>descent</em>:</p>\n<span class=\"katex-display\"><span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mo>−</mo><mi>log</mi><mo>⁡</mo><mo stretchy=\"false\">(</mo><mi>D</mi><mo stretchy=\"false\">(</mo><mi>x</mi><mo stretchy=\"false\">)</mo><mo stretchy=\"false\">)</mo><mo>−</mo><mi>log</mi><mo>⁡</mo><mo stretchy=\"false\">(</mo><mn>1</mn><mo>−</mo><mi>D</mi><mo stretchy=\"false\">(</mo><mi>G</mi><mo stretchy=\"false\">(</mo><mi mathvariant=\"bold-italic\">z</mi><mo stretchy=\"false\">)</mo><mo stretchy=\"false\">)</mo><mo stretchy=\"false\">)</mo><mo>=</mo><mo>−</mo><mi>log</mi><mo>⁡</mo><mrow><mo fence=\"true\">(</mo><mfrac><mrow><mi>Z</mi><mo stretchy=\"false\">(</mo><mi>x</mi><mo stretchy=\"false\">)</mo></mrow><mrow><mn>1</mn><mo>+</mo><mi>Z</mi><mo stretchy=\"false\">(</mo><mi>x</mi><mo stretchy=\"false\">)</mo></mrow></mfrac><mo fence=\"true\">)</mo></mrow><mo>−</mo><mi>log</mi><mo>⁡</mo><mrow><mo fence=\"true\">(</mo><mn>1</mn><mo>−</mo><mfrac><mrow><mi>Z</mi><mo stretchy=\"false\">(</mo><mi>G</mi><mo stretchy=\"false\">(</mo><mi mathvariant=\"bold-italic\">z</mi><mo stretchy=\"false\">)</mo><mo stretchy=\"false\">)</mo></mrow><mrow><mn>1</mn><mo>+</mo><mi>Z</mi><mo stretchy=\"false\">(</mo><mi>G</mi><mo stretchy=\"false\">(</mo><mi mathvariant=\"bold-italic\">z</mi><mo stretchy=\"false\">)</mo><mo stretchy=\"false\">)</mo></mrow></mfrac><mo fence=\"true\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">-\\log(D(x)) - \\log(1 - D(G(\\boldsymbol z))) = -\\log\\left(\\frac{Z(x)}{1 + Z(x)}\\right) - \\log\\left(1 - \\frac{Z(G(\\boldsymbol z))}{1 + Z(G(\\boldsymbol z))}\\right)</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:1em;vertical-align:-0.25em;\"></span><span class=\"mord\">−</span><span class=\"mspace\" style=\"margin-right:0.16666666666666666em;\"></span><span class=\"mop\">lo<span style=\"margin-right:0.01389em;\">g</span></span><span class=\"mopen\">(</span><span class=\"mord mathdefault\" style=\"margin-right:0.02778em;\">D</span><span class=\"mopen\">(</span><span class=\"mord mathdefault\">x</span><span class=\"mclose\">)</span><span class=\"mclose\">)</span><span class=\"mspace\" style=\"margin-right:0.2222222222222222em;\"></span><span class=\"mbin\">−</span><span class=\"mspace\" style=\"margin-right:0.2222222222222222em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:1em;vertical-align:-0.25em;\"></span><span class=\"mop\">lo<span style=\"margin-right:0.01389em;\">g</span></span><span class=\"mopen\">(</span><span class=\"mord\">1</span><span class=\"mspace\" style=\"margin-right:0.2222222222222222em;\"></span><span class=\"mbin\">−</span><span class=\"mspace\" style=\"margin-right:0.2222222222222222em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:1em;vertical-align:-0.25em;\"></span><span class=\"mord mathdefault\" style=\"margin-right:0.02778em;\">D</span><span class=\"mopen\">(</span><span class=\"mord mathdefault\">G</span><span class=\"mopen\">(</span><span class=\"mord\"><span class=\"mord boldsymbol\" style=\"margin-right:0.04213em;\">z</span></span><span class=\"mclose\">)</span><span class=\"mclose\">)</span><span class=\"mclose\">)</span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span><span class=\"mrel\">=</span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:2.40003em;vertical-align:-0.95003em;\"></span><span class=\"mord\">−</span><span class=\"mspace\" style=\"margin-right:0.16666666666666666em;\"></span><span class=\"mop\">lo<span style=\"margin-right:0.01389em;\">g</span></span><span class=\"mspace\" style=\"margin-right:0.16666666666666666em;\"></span><span class=\"minner\"><span class=\"mopen delimcenter\" style=\"top:0em;\"><span class=\"delimsizing size3\">(</span></span><span class=\"mord\"><span class=\"mopen nulldelimiter\"></span><span class=\"mfrac\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:1.427em;\"><span style=\"top:-2.314em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span class=\"mord\"><span class=\"mord\">1</span><span class=\"mspace\" style=\"margin-right:0.2222222222222222em;\"></span><span class=\"mbin\">+</span><span class=\"mspace\" style=\"margin-right:0.2222222222222222em;\"></span><span class=\"mord mathdefault\" style=\"margin-right:0.07153em;\">Z</span><span class=\"mopen\">(</span><span class=\"mord mathdefault\">x</span><span class=\"mclose\">)</span></span></span><span style=\"top:-3.23em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span class=\"frac-line\" style=\"border-bottom-width:0.04em;\"></span></span><span style=\"top:-3.677em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span class=\"mord\"><span class=\"mord mathdefault\" style=\"margin-right:0.07153em;\">Z</span><span class=\"mopen\">(</span><span class=\"mord mathdefault\">x</span><span class=\"mclose\">)</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.936em;\"><span></span></span></span></span></span><span class=\"mclose nulldelimiter\"></span></span><span class=\"mclose delimcenter\" style=\"top:0em;\"><span class=\"delimsizing size3\">)</span></span></span><span class=\"mspace\" style=\"margin-right:0.2222222222222222em;\"></span><span class=\"mbin\">−</span><span class=\"mspace\" style=\"margin-right:0.2222222222222222em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:2.40003em;vertical-align:-0.95003em;\"></span><span class=\"mop\">lo<span style=\"margin-right:0.01389em;\">g</span></span><span class=\"mspace\" style=\"margin-right:0.16666666666666666em;\"></span><span class=\"minner\"><span class=\"mopen delimcenter\" style=\"top:0em;\"><span class=\"delimsizing size3\">(</span></span><span class=\"mord\">1</span><span class=\"mspace\" style=\"margin-right:0.2222222222222222em;\"></span><span class=\"mbin\">−</span><span class=\"mspace\" style=\"margin-right:0.2222222222222222em;\"></span><span class=\"mord\"><span class=\"mopen nulldelimiter\"></span><span class=\"mfrac\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:1.427em;\"><span style=\"top:-2.314em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span class=\"mord\"><span class=\"mord\">1</span><span class=\"mspace\" style=\"margin-right:0.2222222222222222em;\"></span><span class=\"mbin\">+</span><span class=\"mspace\" style=\"margin-right:0.2222222222222222em;\"></span><span class=\"mord mathdefault\" style=\"margin-right:0.07153em;\">Z</span><span class=\"mopen\">(</span><span class=\"mord mathdefault\">G</span><span class=\"mopen\">(</span><span class=\"mord\"><span class=\"mord boldsymbol\" style=\"margin-right:0.04213em;\">z</span></span><span class=\"mclose\">)</span><span class=\"mclose\">)</span></span></span><span style=\"top:-3.23em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span class=\"frac-line\" style=\"border-bottom-width:0.04em;\"></span></span><span style=\"top:-3.677em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span class=\"mord\"><span class=\"mord mathdefault\" style=\"margin-right:0.07153em;\">Z</span><span class=\"mopen\">(</span><span class=\"mord mathdefault\">G</span><span class=\"mopen\">(</span><span class=\"mord\"><span class=\"mord boldsymbol\" style=\"margin-right:0.04213em;\">z</span></span><span class=\"mclose\">)</span><span class=\"mclose\">)</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.936em;\"><span></span></span></span></span></span><span class=\"mclose nulldelimiter\"></span></span><span class=\"mclose delimcenter\" style=\"top:0em;\"><span class=\"delimsizing size3\">)</span></span></span></span></span></span></span>\n<p>Time to rewrite this a bit! The first term on the right-hand side of the equation can be written:</p>\n<span class=\"katex-display\"><span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mtext mathvariant=\"monospace\">softplus</mtext><mo stretchy=\"false\">(</mo><mtext mathvariant=\"monospace\">logsumexp</mtext><mo stretchy=\"false\">(</mo><msubsup><mi>l</mi><mn>1</mn><mrow><mo stretchy=\"false\">(</mo><mi>x</mi><mo stretchy=\"false\">)</mo></mrow></msubsup><mo separator=\"true\">,</mo><mo>…</mo><mo separator=\"true\">,</mo><msubsup><mi>l</mi><mi>K</mi><mrow><mo stretchy=\"false\">(</mo><mi>x</mi><mo stretchy=\"false\">)</mo></mrow></msubsup><mo stretchy=\"false\">)</mo><mo stretchy=\"false\">)</mo><mo>−</mo><mtext mathvariant=\"monospace\">logsumexp</mtext><mo fence=\"false\">(</mo><msubsup><mi>l</mi><mn>1</mn><mrow><mo stretchy=\"false\">(</mo><mi>x</mi><mo stretchy=\"false\">)</mo></mrow></msubsup><mo separator=\"true\">,</mo><mo>…</mo><msubsup><mi>l</mi><mi>K</mi><mrow><mo stretchy=\"false\">(</mo><mi>x</mi><mo stretchy=\"false\">)</mo></mrow></msubsup><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">\\texttt{softplus}(\\texttt{logsumexp}(l_1^{(x)},\\ldots,l_K^{(x)})) - \\texttt{logsumexp}\\Big(l_1^{(x)},\\ldots l_K^{(x)})</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:1.338331em;vertical-align:-0.293531em;\"></span><span class=\"mord text\"><span class=\"mord texttt\">softplus</span></span><span class=\"mopen\">(</span><span class=\"mord text\"><span class=\"mord texttt\">logsumexp</span></span><span class=\"mopen\">(</span><span class=\"mord\"><span class=\"mord mathdefault\" style=\"margin-right:0.01968em;\">l</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:1.0448em;\"><span style=\"top:-2.433692em;margin-left:-0.01968em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\">1</span></span></span><span style=\"top:-3.2198em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\"><span class=\"mopen mtight\">(</span><span class=\"mord mathdefault mtight\">x</span><span class=\"mclose mtight\">)</span></span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.266308em;\"><span></span></span></span></span></span></span><span class=\"mpunct\">,</span><span class=\"mspace\" style=\"margin-right:0.16666666666666666em;\"></span><span class=\"minner\">…</span><span class=\"mspace\" style=\"margin-right:0.16666666666666666em;\"></span><span class=\"mpunct\">,</span><span class=\"mspace\" style=\"margin-right:0.16666666666666666em;\"></span><span class=\"mord\"><span class=\"mord mathdefault\" style=\"margin-right:0.01968em;\">l</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:1.0448em;\"><span style=\"top:-2.4064690000000004em;margin-left:-0.01968em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mathdefault mtight\" style=\"margin-right:0.07153em;\">K</span></span></span><span style=\"top:-3.2198em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\"><span class=\"mopen mtight\">(</span><span class=\"mord mathdefault mtight\">x</span><span class=\"mclose mtight\">)</span></span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.293531em;\"><span></span></span></span></span></span></span><span class=\"mclose\">)</span><span class=\"mclose\">)</span><span class=\"mspace\" style=\"margin-right:0.2222222222222222em;\"></span><span class=\"mbin\">−</span><span class=\"mspace\" style=\"margin-right:0.2222222222222222em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:1.80002em;vertical-align:-0.65002em;\"></span><span class=\"mord text\"><span class=\"mord texttt\">logsumexp</span></span><span class=\"mord\"><span class=\"delimsizing size2\">(</span></span><span class=\"mord\"><span class=\"mord mathdefault\" style=\"margin-right:0.01968em;\">l</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:1.0448em;\"><span style=\"top:-2.433692em;margin-left:-0.01968em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\">1</span></span></span><span style=\"top:-3.2198em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\"><span class=\"mopen mtight\">(</span><span class=\"mord mathdefault mtight\">x</span><span class=\"mclose mtight\">)</span></span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.266308em;\"><span></span></span></span></span></span></span><span class=\"mpunct\">,</span><span class=\"mspace\" style=\"margin-right:0.16666666666666666em;\"></span><span class=\"minner\">…</span><span class=\"mspace\" style=\"margin-right:0.16666666666666666em;\"></span><span class=\"mord\"><span class=\"mord mathdefault\" style=\"margin-right:0.01968em;\">l</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:1.0448em;\"><span style=\"top:-2.4064690000000004em;margin-left:-0.01968em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mathdefault mtight\" style=\"margin-right:0.07153em;\">K</span></span></span><span style=\"top:-3.2198em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\"><span class=\"mopen mtight\">(</span><span class=\"mord mathdefault mtight\">x</span><span class=\"mclose mtight\">)</span></span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.293531em;\"><span></span></span></span></span></span></span><span class=\"mclose\">)</span></span></span></span></span>\n<p><img src=\"https://cdn-images-1.medium.com/max/4768/1*m7Yj5icm4kVaAWlvassnnA.png\"></p>\n<p>The second term of the right-hand side can be written as:</p>\n<p><img src=\"https://cdn-images-1.medium.com/max/7828/1*ji0rb_RrNFQvFSUVVDEIFA.png\"></p>\n<p>So that finally, we arrive at the following loss:</p>\n<p><img src=\"https://cdn-images-1.medium.com/max/7954/1*_3YqX5Ppo5keIk1anwx21g.png\"></p>\n<div class=\"gatsby-highlight\" data-language=\"python\"><pre class=\"language-python\"><code class=\"language-python\"><span class=\"token comment\"># Set the discriminator losses</span>\n<span class=\"token keyword\">with</span> tf<span class=\"token punctuation\">.</span>name_scope<span class=\"token punctuation\">(</span><span class=\"token string\">\"DiscriminatorLoss\"</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">:</span>\n    <span class=\"token comment\"># Supervised loss, just cross-entropy. This normalizes p(y|x) where 1 &lt;= y &lt;= K</span>\n    loss_supervised <span class=\"token operator\">=</span> tf<span class=\"token punctuation\">.</span>reduce_mean<span class=\"token punctuation\">(</span>tf<span class=\"token punctuation\">.</span>nn<span class=\"token punctuation\">.</span>sparse_softmax_cross_entropy_with_logits<span class=\"token punctuation\">(</span>\n        labels<span class=\"token operator\">=</span>labels_lab<span class=\"token punctuation\">,</span> logits<span class=\"token operator\">=</span>logits_real_lab<span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span>\n\n    <span class=\"token comment\"># Sum of unnormalized log probabilities</span>\n    logits_sum_real <span class=\"token operator\">=</span> tf<span class=\"token punctuation\">.</span>reduce_logsumexp<span class=\"token punctuation\">(</span>logits_real_unl<span class=\"token punctuation\">,</span> axis<span class=\"token operator\">=</span><span class=\"token number\">1</span><span class=\"token punctuation\">)</span>\n    logits_sum_fake <span class=\"token operator\">=</span> tf<span class=\"token punctuation\">.</span>reduce_logsumexp<span class=\"token punctuation\">(</span>logits_fake<span class=\"token punctuation\">,</span> axis<span class=\"token operator\">=</span><span class=\"token number\">1</span><span class=\"token punctuation\">)</span>\n    loss_unsupervised <span class=\"token operator\">=</span> <span class=\"token number\">0.5</span> <span class=\"token operator\">*</span> <span class=\"token punctuation\">(</span>\n        tf<span class=\"token punctuation\">.</span>negative<span class=\"token punctuation\">(</span>tf<span class=\"token punctuation\">.</span>reduce_mean<span class=\"token punctuation\">(</span>logits_sum_real<span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span> <span class=\"token operator\">+</span>\n        tf<span class=\"token punctuation\">.</span>reduce_mean<span class=\"token punctuation\">(</span>tf<span class=\"token punctuation\">.</span>nn<span class=\"token punctuation\">.</span>softplus<span class=\"token punctuation\">(</span>logits_sum_real<span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span> <span class=\"token operator\">+</span>\n        tf<span class=\"token punctuation\">.</span>reduce_mean<span class=\"token punctuation\">(</span>tf<span class=\"token punctuation\">.</span>nn<span class=\"token punctuation\">.</span>softplus<span class=\"token punctuation\">(</span>logits_sum_fake<span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span>\n    loss_d <span class=\"token operator\">=</span> loss_supervised <span class=\"token operator\">+</span> loss_unsupervised</code></pre></div>\n<h2>Optimizing The Discriminator</h2>\n<p>Let’s setup the operations for actually updating the parameters of the discriminator. We will just reside to the <code class=\"language-text\">Adam</code> optimizer. While tweaking the parameters before I wrote this post, I figured I might slow down the discriminator by setting its learning rate at 0.1 times that of the generator. After that my results got much better, so I decided to leave it there for now. Notice also that we can very easily select the subset of variables corresponding to the discriminator by exploiting the encapsulation offered by Keras.</p>\n<div class=\"gatsby-highlight\" data-language=\"python\"><pre class=\"language-python\"><code class=\"language-python\"><span class=\"token comment\"># Configure discriminator training ops</span>\n<span class=\"token keyword\">with</span> tf<span class=\"token punctuation\">.</span>name_scope<span class=\"token punctuation\">(</span><span class=\"token string\">\"Train\"</span><span class=\"token punctuation\">)</span> <span class=\"token keyword\">as</span> train_scope<span class=\"token punctuation\">:</span>\n    optimizer <span class=\"token operator\">=</span> tf<span class=\"token punctuation\">.</span>train<span class=\"token punctuation\">.</span>AdamOptimizer<span class=\"token punctuation\">(</span>flags_obj<span class=\"token punctuation\">.</span>lr <span class=\"token operator\">*</span> <span class=\"token number\">0.1</span><span class=\"token punctuation\">)</span>\n    optimize_d <span class=\"token operator\">=</span> optimizer<span class=\"token punctuation\">.</span>minimize<span class=\"token punctuation\">(</span>loss_d<span class=\"token punctuation\">,</span> var_list<span class=\"token operator\">=</span>d_model<span class=\"token punctuation\">.</span>trainable_variables<span class=\"token punctuation\">)</span>\n    train_accuracy_op <span class=\"token operator\">=</span> accuracy<span class=\"token punctuation\">(</span>logits_train<span class=\"token punctuation\">,</span> labels_lab<span class=\"token punctuation\">)</span></code></pre></div>\n<h2>Adding some control flow to the graph</h2>\n<p>After we have the new weights for the discriminator, we want the generator’s update to be aware of the updated weights. TensorFlow will not guarantee that the updated weights will actually be used even if we were to redeclare the forward computation after defining the minimization operations for the discriminator. We can still force this by using <code class=\"language-text\">tf.control_dependencies</code>. Any operation defined in the scope of this context manager will depend on the evaluation of the ones that are passed to context manager at <em>instantiation</em>. In other words, our generator’s update that we define later on will be guaranteed to compute the gradients using the <em>updated weights</em> of the discriminator.</p>\n<div class=\"gatsby-highlight\" data-language=\"python\"><pre class=\"language-python\"><code class=\"language-python\"><span class=\"token keyword\">with</span> tf<span class=\"token punctuation\">.</span>name_scope<span class=\"token punctuation\">(</span>discriminator_scope<span class=\"token punctuation\">)</span><span class=\"token punctuation\">:</span>\n    <span class=\"token keyword\">with</span> tf<span class=\"token punctuation\">.</span>control_dependencies<span class=\"token punctuation\">(</span><span class=\"token punctuation\">[</span>optimize_d<span class=\"token punctuation\">]</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">:</span>\n        <span class=\"token comment\"># Build a second time, so that new variables are used</span>\n        logits_fake<span class=\"token punctuation\">,</span> features_fake <span class=\"token operator\">=</span> d_model<span class=\"token punctuation\">(</span>images_fake<span class=\"token punctuation\">,</span> training<span class=\"token operator\">=</span><span class=\"token boolean\">True</span><span class=\"token punctuation\">)</span>\n        logits_real_unl<span class=\"token punctuation\">,</span> features_real_unl <span class=\"token operator\">=</span> d_model<span class=\"token punctuation\">(</span>images_unl2<span class=\"token punctuation\">,</span> training<span class=\"token operator\">=</span><span class=\"token boolean\">True</span><span class=\"token punctuation\">)</span></code></pre></div>\n<h3>The generator’s loss and updates</h3>\n<p>In this implementation, the generator tries to minimize the L2 distance of the <em>average features</em> of the generated images vs. the <em>average features</em> of the real images. This <a href=\"https://arxiv.org/abs/1606.03498\">feature-matching loss</a> (Salimans et al., 2016) has proven to be more stable for training GANs than directly trying to optimize the discriminator’s probability for observing real data. It is straightforward to implement. While we’re at it, let’s also define the update operations for the generator. Notice that the learning rate of this optimizer is 10 times that of the discriminator.</p>\n<div class=\"gatsby-highlight\" data-language=\"python\"><pre class=\"language-python\"><code class=\"language-python\"><span class=\"token comment\"># Set the generator loss and the actual train op</span>\n<span class=\"token keyword\">with</span> tf<span class=\"token punctuation\">.</span>name_scope<span class=\"token punctuation\">(</span><span class=\"token string\">\"GeneratorLoss\"</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">:</span>\n    feature_mean_real <span class=\"token operator\">=</span> tf<span class=\"token punctuation\">.</span>reduce_mean<span class=\"token punctuation\">(</span>features_real_unl<span class=\"token punctuation\">,</span> axis<span class=\"token operator\">=</span><span class=\"token number\">0</span><span class=\"token punctuation\">)</span>\n    feature_mean_fake <span class=\"token operator\">=</span> tf<span class=\"token punctuation\">.</span>reduce_mean<span class=\"token punctuation\">(</span>features_fake<span class=\"token punctuation\">,</span> axis<span class=\"token operator\">=</span><span class=\"token number\">0</span><span class=\"token punctuation\">)</span>\n    <span class=\"token comment\"># L1 distance of features is the loss for the generator</span>\n    loss_g <span class=\"token operator\">=</span> tf<span class=\"token punctuation\">.</span>reduce_mean<span class=\"token punctuation\">(</span>tf<span class=\"token punctuation\">.</span><span class=\"token builtin\">abs</span><span class=\"token punctuation\">(</span>feature_mean_real <span class=\"token operator\">-</span> feature_mean_fake<span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span>\n\n<span class=\"token keyword\">with</span> tf<span class=\"token punctuation\">.</span>name_scope<span class=\"token punctuation\">(</span>train_scope<span class=\"token punctuation\">)</span><span class=\"token punctuation\">:</span>\n    optimizer <span class=\"token operator\">=</span> tf<span class=\"token punctuation\">.</span>train<span class=\"token punctuation\">.</span>AdamOptimizer<span class=\"token punctuation\">(</span>flags_obj<span class=\"token punctuation\">.</span>lr<span class=\"token punctuation\">,</span> beta1<span class=\"token operator\">=</span><span class=\"token number\">0.5</span><span class=\"token punctuation\">)</span>\n    train_op <span class=\"token operator\">=</span> optimizer<span class=\"token punctuation\">.</span>minimize<span class=\"token punctuation\">(</span>loss_g<span class=\"token punctuation\">,</span> var_list<span class=\"token operator\">=</span>g_model<span class=\"token punctuation\">.</span>trainable_variables<span class=\"token punctuation\">)</span></code></pre></div>\n<h3>Adding manifold regularization</h3>\n<p><a href=\"https://arxiv.org/abs/1805.08957\">Lecouat et. al</a> (2018) propose to add <em>manifold regularization</em> to the feature-matching GAN training procedure of <a href=\"https://arxiv.org/abs/1606.03498\">Salimans et al. (2016)</a>. The regularization forces the discriminator to yield similar logits (unnormalized log probabilities) for nearby points in the latent space in which <span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mi mathvariant=\"bold-italic\">z</mi></mrow><annotation encoding=\"application/x-tex\">\\boldsymbol z</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.44444em;vertical-align:0em;\"></span><span class=\"mord\"><span class=\"mord boldsymbol\" style=\"margin-right:0.04213em;\">z</span></span></span></span></span> resides. It can be implemented by generating a second perturbed version of <span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mi mathvariant=\"bold-italic\">z</mi></mrow><annotation encoding=\"application/x-tex\">\\boldsymbol z</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.44444em;vertical-align:0em;\"></span><span class=\"mord\"><span class=\"mord boldsymbol\" style=\"margin-right:0.04213em;\">z</span></span></span></span></span> and computing the generator’s and discriminator’s outputs once more with this slightly altered vector.</p>\n<p>This means that the noise generation code looks as follows:</p>\n<div class=\"gatsby-highlight\" data-language=\"python\"><pre class=\"language-python\"><code class=\"language-python\"><span class=\"token keyword\">def</span> <span class=\"token function\">define_noise</span><span class=\"token punctuation\">(</span>batch_size_tensor<span class=\"token punctuation\">,</span> flags_obj<span class=\"token punctuation\">)</span><span class=\"token punctuation\">:</span>\n    <span class=\"token comment\"># Setup noise vector</span>\n    <span class=\"token keyword\">with</span> tf<span class=\"token punctuation\">.</span>name_scope<span class=\"token punctuation\">(</span><span class=\"token string\">\"LatentNoiseVector\"</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">:</span>\n        z <span class=\"token operator\">=</span> tfd<span class=\"token punctuation\">.</span>Normal<span class=\"token punctuation\">(</span>loc<span class=\"token operator\">=</span><span class=\"token number\">0.0</span><span class=\"token punctuation\">,</span> scale<span class=\"token operator\">=</span>flags_obj<span class=\"token punctuation\">.</span>stddev<span class=\"token punctuation\">)</span><span class=\"token punctuation\">.</span>sample<span class=\"token punctuation\">(</span>\n            sample_shape<span class=\"token operator\">=</span><span class=\"token punctuation\">(</span>batch_size_tensor<span class=\"token punctuation\">,</span> flags_obj<span class=\"token punctuation\">.</span>z_dim_size<span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span>\n        z_perturbed <span class=\"token operator\">=</span> z <span class=\"token operator\">+</span> tfd<span class=\"token punctuation\">.</span>Normal<span class=\"token punctuation\">(</span>loc<span class=\"token operator\">=</span><span class=\"token number\">0.0</span><span class=\"token punctuation\">,</span> scale<span class=\"token operator\">=</span>flags_obj<span class=\"token punctuation\">.</span>stddev<span class=\"token punctuation\">)</span><span class=\"token punctuation\">.</span>sample<span class=\"token punctuation\">(</span>\n            sample_shape<span class=\"token operator\">=</span><span class=\"token punctuation\">(</span>batch_size_tensor<span class=\"token punctuation\">,</span> flags_obj<span class=\"token punctuation\">.</span>z_dim_size<span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span> <span class=\"token operator\">*</span> <span class=\"token number\">1e</span><span class=\"token operator\">-</span><span class=\"token number\">5</span>\n    <span class=\"token keyword\">return</span> z<span class=\"token punctuation\">,</span> z_perturbed</code></pre></div>\n<p>The discriminator’s loss will be updated as follows (note the 3 extra lines at the bottom):</p>\n<div class=\"gatsby-highlight has-highlighted-lines\" data-language=\"python\"><pre class=\"language-python\"><code class=\"language-python\"><span class=\"token comment\"># Set the discriminator losses</span>\n<span class=\"token keyword\">with</span> tf<span class=\"token punctuation\">.</span>name_scope<span class=\"token punctuation\">(</span><span class=\"token string\">\"DiscriminatorLoss\"</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">:</span>\n    <span class=\"token comment\"># Supervised loss, just cross-entropy. This normalizes p(y|x) where 1 &lt;= y &lt;= K</span>\n    loss_supervised <span class=\"token operator\">=</span> tf<span class=\"token punctuation\">.</span>reduce_mean<span class=\"token punctuation\">(</span>tf<span class=\"token punctuation\">.</span>nn<span class=\"token punctuation\">.</span>sparse_softmax_cross_entropy_with_logits<span class=\"token punctuation\">(</span>\n        labels<span class=\"token operator\">=</span>labels_lab<span class=\"token punctuation\">,</span> logits<span class=\"token operator\">=</span>logits_real_lab<span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span>\n\n    <span class=\"token comment\"># Sum of unnormalized log probabilities</span>\n    logits_sum_real <span class=\"token operator\">=</span> tf<span class=\"token punctuation\">.</span>reduce_logsumexp<span class=\"token punctuation\">(</span>logits_real_unl<span class=\"token punctuation\">,</span> axis<span class=\"token operator\">=</span><span class=\"token number\">1</span><span class=\"token punctuation\">)</span>\n    logits_sum_fake <span class=\"token operator\">=</span> tf<span class=\"token punctuation\">.</span>reduce_logsumexp<span class=\"token punctuation\">(</span>logits_fake<span class=\"token punctuation\">,</span> axis<span class=\"token operator\">=</span><span class=\"token number\">1</span><span class=\"token punctuation\">)</span>\n    loss_unsupervised <span class=\"token operator\">=</span> <span class=\"token number\">0.5</span> <span class=\"token operator\">*</span> <span class=\"token punctuation\">(</span>\n        tf<span class=\"token punctuation\">.</span>negative<span class=\"token punctuation\">(</span>tf<span class=\"token punctuation\">.</span>reduce_mean<span class=\"token punctuation\">(</span>logits_sum_real<span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span> <span class=\"token operator\">+</span>\n        tf<span class=\"token punctuation\">.</span>reduce_mean<span class=\"token punctuation\">(</span>tf<span class=\"token punctuation\">.</span>nn<span class=\"token punctuation\">.</span>softplus<span class=\"token punctuation\">(</span>logits_sum_real<span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span> <span class=\"token operator\">+</span>\n        tf<span class=\"token punctuation\">.</span>reduce_mean<span class=\"token punctuation\">(</span>tf<span class=\"token punctuation\">.</span>nn<span class=\"token punctuation\">.</span>softplus<span class=\"token punctuation\">(</span>logits_sum_fake<span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span>\n    loss_d <span class=\"token operator\">=</span> loss_supervised <span class=\"token operator\">+</span> loss_unsupervised\n<span class=\"gatsby-highlight-code-line\">    <span class=\"token keyword\">if</span> flags_obj<span class=\"token punctuation\">.</span>man_reg<span class=\"token punctuation\">:</span></span><span class=\"gatsby-highlight-code-line\">        loss_d <span class=\"token operator\">+=</span> <span class=\"token number\">1e</span><span class=\"token operator\">-</span><span class=\"token number\">3</span> <span class=\"token operator\">*</span> tf<span class=\"token punctuation\">.</span>nn<span class=\"token punctuation\">.</span>l2_loss<span class=\"token punctuation\">(</span>logits_fake <span class=\"token operator\">-</span> logits_fake_perturbed<span class=\"token punctuation\">)</span> \\</span><span class=\"gatsby-highlight-code-line\">            <span class=\"token operator\">/</span> tf<span class=\"token punctuation\">.</span>to_float<span class=\"token punctuation\">(</span>batch_size_tensor<span class=\"token punctuation\">)</span></span></code></pre></div>\n<h3>Classification performance</h3>\n<p>So how does it really perform? I have provided a few plots below. There are many things I might try to squeeze out additional performance (for instance, just training for longer, using a learning rate schedule, implementing weight normalization), but the main purpose of writing this post was to get to know a relatively simple yet powerful semi-supervised learning approach. After 100 epochs of training, the mean test accuracy approaches 98.9 percent.</p>\n<p>The full script can be found <a href=\"https://github.com/jostosh/gan\">in my repo</a>. Thanks for reading!</p>\n<p><img src=\"https://cdn-images-1.medium.com/max/2000/1*6Id2ap-9mQ-pMQKBDYqC1A.jpeg\"></p>","frontmatter":{"title":"Semi-Supervised Learning With GANs","date":"June 18, 2018","description":"Partial reimplementation of a paper on Semi-Supervised Learning with Generative Adversarial Networks."}}},"pageContext":{"isCreatedByStatefulCreatePages":false,"slug":"/ssl-gan/","previous":{"fields":{"slug":"/capsnet-cuda/"},"frontmatter":{"title":"Cuda, TensorFlow And Capsule Networks"}},"next":{"fields":{"slug":"/spn01/"},"frontmatter":{"title":"Sum-Product Networks"}}}}