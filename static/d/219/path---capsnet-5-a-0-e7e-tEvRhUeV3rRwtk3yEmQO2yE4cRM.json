{"data":{"site":{"siteMetadata":{"title":"Machine Learning Blog - JvdW","author":"Jos van de Wolfshaar"}},"markdownRemark":{"id":"82e1e556-6bc1-5bf8-9963-41cd9b94e491","excerpt":"","html":"<!-- ---\ntitle: Cuda, TensorFlow and capsule networks\ndate: \"2018-02-11T22:12:03.284Z\"\n---\n## Introduction\nRecently I've spent some time on CUDA programming and implementing custom Ops for TensorFlow. As an exercise, I decided to take a shot at implementing a custom Op for one of the operations in capsule networks that would normally require some reshape hacking or at least a couple of intermediate TensorFlow Ops. If you're not familiar with Capsule Networks, have a look at the article or my previous post. The open-sourced capsule network code by the original authors can be found here. The code that we discuss in this blog post can be found here.\n\nMany of the concepts that are covered in this post (CUDA, TensorFlow custom Ops, gradient testing) can be learned by going through their corresponding documentations, but I always think it is enlightening to see how such separate elements come together. This is the main motivation behind my blog post: showcasing the development of a custom TensorFlow Op with CUDA from start to end. So let's commence.\n\n**There also exists a Chinese translation of this post by Jakukyo Friel.**\n\n## Capsule prediction\nThe operation of interest in this blog post is the one that computes:\n\n and for all samples in a batch. Hence, the tensors W_ij and u_i are actually of shape [batch_size, in_caps, in_dim] and [in_caps, out_caps, out_dim, in_dim] respectively. This means that we will build an op that just takes in these two tensors and computes an output tensor u_hat_ji of shape [batch_size, in_caps, out_caps, out_dim]. In other words, for all batch indices [0,1,...,batch_size-1] and for all combinations of in capsules [0,1,...,in_caps-1] and out capsules [0,1,...,out_caps-1] we have to compute a matrix-vector product.\n\n## TensorFlow kernel implementation\nOur custom Op will be most valuable if we implement it for a GPU. In the TensorFlow documentation, you can find the necessary material to get you started on your own C++ kernels for TensorFlow. Then, you can read up on how to empower your algorithms with massively parallel GPU capabilities in this book or online. I will not repeat the details that you can find there, but I will provide a practical example that hopefully helps to understand how you can use CUDA for your own TensorFlow Ops. For me, getting to know some CUDA was easier than I thought, but squeezing out all performance can be tricky and I will leave further optimization of the kernel in this post for future work.\n\n#### Op registration\nLet's do the forward pass of the Op. First, we will register the Op:\n\n```python\nREGISTER_OP(\"CapsulePrediction\")\n.Input(\"input: T\")\n.Input(\"weights: T\")\n.Output(\"output: T\")\n.Attr(\"T: type\")\n.SetShapeFn([](InferenceContext* ctx) {\n    // Get shapes and ensure correct dimensionality\n    ShapeHandle in_shape;\n    ShapeHandle weights_shape;\n    TF_RETURN_IF_ERROR(ctx->WithRank(ctx->input(0), 3, &in_shape));\n    TF_RETURN_IF_ERROR(ctx->WithRank(ctx->input(1), 4, &weights_shape));\n\n    // Construct and set the output shape\n    DimensionHandle out_d0, out_d1, out_d2, out_d3;\n    std::vector<DimensionHandle> out_dims;\n    out_dims.push_back(ctx->MakeDim(ctx->Dim(ctx->input(0), 0)));\n    out_dims.push_back(ctx->MakeDim(ctx->Dim(ctx->input(1), 0)));\n    out_dims.push_back(ctx->MakeDim(ctx->Dim(ctx->input(1), 1)));\n    out_dims.push_back(ctx->MakeDim(ctx->Dim(ctx->input(1), 2)));\n    ShapeHandle out_shape = ctx->MakeShape(out_dims);\n    ctx->set_output(0, out_shape);\n\n    return Status::OK();\n});\n```\n\nFor now, I have defined this so that we could later specify different TensorFlow kernels for different dtypes by adding the \"..: T\" specification. Some of the classes that you see here such as ShapeHandle, DimensionHandle and InferenceContext are defined in the tensorflow namespace. The code shows a shape function that is implemented as a lambda function which first ensures ctx->input(0) (the input) and `ctx->input(1)` (the weights ) have the correct rank. Then, we determine the dimensions of the output tensor which we can obtain from the input tensors. The dimension of the Op's output is [batch_size, in_caps, out_caps, out_dim], so we take batch_size and in_caps from the $u_i$ tensor and out_caps and out_dim from the $W_{ij}$ tensor.\nForward capsule prediction\nNow, let's look at the Op's kernel. The word 'kernel' is TensorFlow terminology for the device-specific implementation of an Op. When defining a custom kernel, it should inherit from TensorFlow's OpKernel and it shall implement the Compute method:\n\n```python\nclass CapsulePredictionOp : public OpKernel\n{\n public:\n  explicit CapsulePredictionOp(OpKernelConstruction* ctx) : OpKernel(ctx) { }\n\n  void Compute(OpKernelContext* ctx) override\n  {\n    // Get inputs\n    const Tensor& input = ctx->input(0);\n    const Tensor& weights = ctx->input(1);\n\n    // Setup output shape\n    const TensorShape& input_shape(input.shape());\n    TensorShape output_shape(weights.shape());\n    output_shape.InsertDim(0, input_shape.dim_size(0));\n    output_shape.RemoveDim(4);\n\n    // Allocate output tensor\n    Tensor* output = nullptr;\n    OP_REQUIRES_OK(ctx, ctx->allocate_output(0, output_shape, &output));\n\n    // Get the Eigen tensors and pass them on the launcher\n    auto input_tensor   = input.tensor<float, 3>();\n    auto weights_tensor = weights.tensor<float, 4>();\n    auto output_tensor  = output->tensor<float, 4>();\n    launchCapsulePrediction(ctx->eigen_device(), input_tensor, weights_tensor,\n      output_tensor);\n  }\n};\n```\n            \nIn the implementation above we haven't done anything with CUDA yet, but we'll get there so don't worry. The code merely initializes the output shape from the input shapes and allocates the memory. The OpKernelContext object that is provided as a parameter makes sure to allocate the memory on the currently used device. In our case, this will be the GPU. Then, we obtain the Eigen tensors through the tensor method and pass them on to our launchCapsulePrediction function, where the actual magic happens.\nLaunching the kernel\nOur `launchCapsulePrediction` function literally (at least in CUDA terminology) launches code on the GPU. Perhaps a little confusing, but CUDA refers to functions that run code on the 'device' as kernels. In TensorFlow terminology, a kernel is not necessarily a GPU implementation, while in CUDA terminology it is. Let's not get too wrapped up in terminology and just get to the code:\n\n\nvoid launchCapsulePrediction(\n  const GPUDevice& d,\n  typename TTypes<float, 3>::ConstTensor x,\n  typename TTypes<float, 4>::ConstTensor weights,\n  typename TTypes<float, 4>::Tensor out)\n{\n  // Get the dimensions\n  const int64 batch_size  = x.dimension(0);\n  const int64 in_caps     = x.dimension(1);\n  const int64 in_dim      = x.dimension(2);\n  const int64 out_dim     = weights.dimension(2);\n  const int64 out_caps    = weights.dimension(1);\n\n  // Size first dim\n  const int64 w_d0 = out_caps * out_dim * in_dim;\n  const int64 x_d0 = in_caps * in_dim;\n  const int64 o_d0 = in_caps * out_caps * out_dim;\n\n  // Second dim\n  const int64 w_d1 = out_dim * in_dim;\n  const int64 x_d1 = in_dim;\n  const int64 o_d1 = out_caps * out_dim;\n\n  // Third dim\n  const int64 w_d2 = in_dim;\n  const int64 o_d2 = out_dim;\n\n  // Launch CUDA kernel for forward operation\n  CudaLaunchConfig config = GetCudaLaunchConfig(out.size(), d);\n  capsulePredictionKernel\n    <<<config.block_count, config.thread_per_block, 0, d.stream()>>>(\n      x.data(), weights.data(), out.data(),\n      o_d0, o_d1, o_d2, x_d0, x_d1, w_d0, w_d1, w_d2,\n      in_dim, out.size());\n}\n                \nThe TTypes templates that you can see in the function arguments and the int64 types are defined in the tensorflow namespace. The next part about the dimensions should be pretty self-explanatory. Because we are passing our tensor data as one-dimensional arrays to the actual CUDA kernel, we need to figure out what the memory sizes are for each dimension and each kernel. Note that when I say 'memory sizes', I just refer to the number of floats for each axis and not the byte size. Let's consider the memory sizes of the first axis of each tensor:\n\n\n// Size first dim\nconst int64 w_d0 = out_caps * out_dim * in_dim;\nconst int64 x_d0 = in_caps * in_dim;\nconst int64 o_d0 = in_caps * out_caps * out_dim;\n                \nAwesome, so we can simply get these using the dimensions we determined already. The code tells us that w_d0 is just the product of out_caps, out_dim and in_dim. So if we want to jump from one index \nW\ni\n,\nj\n,\nk\n,\nl\n to \nW\ni\n+\n1\n,\nj\n,\nk\n,\nl\n we should add w_d0 to the one-dimensional index. The same goes for index \nj\n and w_d1 as you might already expect.\n\nThe actual CUDA kernel launch is given at the bottom of the function and repeated here:\n\n\n// Launch CUDA kernel for forward operation\nCudaLaunchConfig config = GetCudaLaunchConfig(out.size(), d);\ncapsulePredictionKernel\n<<<config.block_count, config.thread_per_block, 0, d.stream()>>>(\n  x.data(), weights.data(), out.data(),\n  o_d0, o_d1, o_d2, x_d0, x_d1, w_d0, w_d1, w_d2,\n  in_dim, out.size());\n                \nBoth statements involve quite a few new concepts. The first statement uses a GetCudaLaunchConfig instance as a way to determine the number of blocks and the number of threads per block. It is provided in the TensorFlow header tensorflow/core/util/cuda_kernel_helper.h. You should definitely check out that file in case you are working on your own Op! The capsulePredictionKernel is the function that uses CUDA parallelism on the GPU. It is launched by using the triple-fold delimiters: <<<config.block_count, config.thread_per_block, 0, d.stream()>>>. When you launch a kernel, you must specify the number of blocks and threads per block, as is done here. The zero on the third position is not relevant for now and it should most likely be zero if you were to implement your own kernels. The CUDA stream d.stream() can be thought of as a pipeline of GPU instructions. Whenever you add your kernel to the stream, the stream will make sure the kernel ends before the next kernel on the stream is called. If you want to do two independent tasks in parallel, you could use two streams and launch one task on each.\n\nThreads and blocks\nAll blocks that are assigned to a call can be run in parallel. If you launch a kernel with N blocks, then you could think of it as running N separate instances of the kernel function. That's pretty convenient! The nvcc compiler will make sure that the kernel function has access to the exact block index so that the specific block-instance of the kernel knows which parts of the incoming arrays it should process.\n\nA block can contain multiple threads itself. Threads are just an additional layer of parallelism, so they run in parallel. Why another layer of parallelism, you ask? Well, threads can do things that blocks cannot. Threads can share their memory which is typically useful when you want to use the same value of some input array in the same block multiple times. The shared memory access is much faster and it is one of the many ways you can optimize your final CUDA implementation. Here is a schematic illustration of the two parallel layers:\n\nBlocks and threads\nThe CUDA kernel\nAfter a quick recap on threads and blocks in CUDA, we finally get to see the CUDA implementation of the forward capsule prediction:\n\n\n__global__ void capsulePredictionKernel(\n    const float* in, const float* weights, float* out,\n    const int64 o_d0, const int64 o_d1, const int64 o_d2,\n    const int64 x_d0, const int64 x_d1,\n    const int64 w_d0, const int64 w_d1, const int64 w_d2,\n    const int64 in_dim, const int64 output_size)\n{\n  CUDA_1D_KERNEL_LOOP(i, output_size)\n  {\n    // So here we have out[b,ci,cj,e]\n    const int64 b     = i / o_d0;\n    const int64 ci    = (i % o_d0) / o_d1;\n    const int64 cj    = (i % o_d1) / o_d2;\n    const int64 e_out  = i % o_d2;\n\n    // Then, we can have a look at computing the array indices for in and W\n    int64 in_idx = b * x_d0 + ci * x_d1;\n    int64 w_idx = ci * w_d0 + cj * w_d1 + e_out * w_d2;\n\n    // Initialize result\n    float result = 0.0;\n    for (int64 v = 0; v < in_dim; ++v)\n      // For both in and weights, the subsequent elements of the forward\n      // computation are also subsequent in memory\n      result += ldg(in + in_idx++) * ldg(weights + w_idx++);\n    // Write result\n    out[i] = result;\n  }\n}\n                \nThe first thing you might notice is the __global__ qualifier that precedes the function definition. This is what the nvcc compiler uses to make the function available globally, meaning that it can be launched from CPU, or 'host' code in CUDA terminology. The function's arguments inherit their names from the launchCapsulePrediction function so they should not cause too much confusion. The CUDA_1D_KERNEL_LOOP is a macro defined in tensorflow/core/util/cuda_kernel_helper.h. It replaces this line of code with:\n\n\nfor (int i = blockIdx.x * blockDim.x + threadIdx.x; i < output_size;\n     i += blockDim.x * gridDim.x)\n            \nThe CUDA kernel launch together with this TensorFlow macro forces us to think in an abstract yet convenient way: it gives us some index i that correspond to the i-th element of the output array out. Different block/thread instantiations of this kernel will get their own values for i. Now all we have to do is figure out what the indices are of our additional arrays in and weights. In order to do that, we determine the batch index b, the input capsule index ci, the output capsule index cj and the out capsule element index e_out:\n\n\n// So here we have out[b,ci,cj,e]\nconst int64 b     = i / o_d0;\nconst int64 ci    = (i % o_d0) / o_d1;\nconst int64 cj    = (i % o_d1) / o_d2;\nconst int64 e_out = i % o_d2;\n                \nDetermining these becomes easy once we know the number of elements contained in each axis. In fact, we have given the memory sizes as arguments to the function. For the other arrays, we can then convert b, ci, cj and e_out to their respective one-dimensional indices:\n\n\n// Then, we can have a look at computing the array indices for in and W\nint64 in_idx = b * x_d0 + ci * x_d1;\nint64 w_idx  = ci * w_d0 + cj * w_d1 + e_out * w_d2;\n            \nAgain, we use the already provided memory sizes for each of the axes to get our one-dimensional indices. These are the indices for the first input capsule element of (i) input capsule ci at batch index b and (ii) the weights of the input capsule ci, the output capsule cj and the output capsule element e_out. If you're familiar with Matlab, then perhaps it helps to remind you of the sub2ind function that concerns the very same thing.\n\nWe assume that the last axis of both in and W corresponds to the input capsule elements. This means that they are subsequent in memory and it is therefore straightforward to construct the loop that goes over the individual input capsule elements:\n\n\n// Initialize result\nfloat result = 0.0;\nfor (int64 v = 0; v < in_dim; ++v)\n  // For both in and weights, the subsequent elements of the forward\n  // computation are also subsequent in memory\n  result += ldg(in + in_idx++) * ldg(weights + w_idx++);\n// Write result\nout[i] = result;\n                \nThe ldg function is a Read-Only Data Cache Load Function. It just receives a pointer to the actual element to read. Remember that we are computing matrix-vector products, which are just sets of inner products. A potential improvement here is to use shared memory since a single input capsule value is used many times, but we will leave out further optimization for now.\n\nTesting\nI want this post to be an end-to-end showcase of the development of a TensorFlow custom Op for GPU. This includes testing the Op. Here's the forward computation with numpy:\n\n\nimport tensorflow as tf\nfrom ops.capsuleprediction import capsule_prediction\nimport numpy as np\nfrom parameterized import parameterized\nimport itertools\n\n\nclass CapsulePredictionOpTest(tf.test.TestCase):\n\n    @staticmethod\n    def _numpy_capsule_prediction(x, weights):\n        \"\"\" Generate the output for x and weights with numpy \"\"\"\n        batch_size, in_caps, in_dim = x.shape\n        _, out_caps, out_dim, _ = weights.shape\n\n        out_shape = (batch_size, in_caps, out_caps, out_dim)\n        out = np.zeros(out_shape)\n\n        for b in range(batch_size):\n            for i in range(in_caps):\n                for j in range(out_caps):\n                    for c in range(out_dim):\n                        out[b, i, j, c] = np.dot(x[b, i], weights[i, j, c])\n        return out\n                \nThe file ops/capsuleprediction.py contains the capsule_prediction that actually loads the Op from the shared library file after being compiled. The function above should be straightforward to interpret: we loop over batch, in capsules, out capsules and out capsule elements and compute a dot product for each combination in the output. We'll use this to verify the forward computation of the Op. Another thing to note is the tf.test.TestCase class which we inherit from. It provides some utility functions for testing with TensorFlow.\n\nNow let's look at the test for the forward capsule prediction:\n\n\n@parameterized.expand([\n    (batch_size, in_caps, out_caps, in_dim, out_dim) for\n    batch_size, in_caps, out_caps, in_dim, out_dim in\n    itertools.product([4, 8], [4, 8], [4, 8], [4, 8], [4, 8])\n])\ndef test_capsule_prediction_op(self, batch_size, in_caps, out_caps, in_dim,\n                               out_dim):\n    \"\"\" Tests the forward capsmatmul op \"\"\"\n    x = np.random.rand(batch_size, in_caps, in_dim)\n    weights = np.random.rand(in_caps, out_caps, out_dim, in_dim)\n\n    truth = self._numpy_capsule_prediction(x, weights)\n    with self.test_session() as sess:\n        x_ph = tf.placeholder(tf.float32, x.shape)\n        w_ph = tf.placeholder(tf.float32, weights.shape)\n\n        ret = capsule_prediction(x_ph, w_ph)\n        out = sess.run(ret, {x_ph: x, w_ph: weights})\n    self.assertAllClose(truth, out)\n                \nI've used quite a few tricks in here. First, the parameterized decorator offers a way to invoke the test with different parameters where each test should succeed on its own and will be considered as a separate test by pytest. If it fails, the provided input will also be displayed in the test logs so in my experience, using it really speeds up the debugging if needed. The parameterized.expand decorator expects a list of tuples. Each tuple will be unpacked as positional function parameters. We can easily generate many tuples to vary the dimension sizes by using itertools.product which takes the Cartesian product of all of its arguments.\n\nThe x and weights arrays are initialized randomly. The TensorFlow graph that we build is simple: it only holds two placeholders and the capsule_prediction Op. The returned value should be the same as that of the _numpy_capsule_prediction function. Let's run the tests:\n\nTest output capsule prediction\nA nice feature of pytest is that you can add the -k flag to select a specific set of tests. Hooray, all tests passed!\n\nThe backward capsule prediction\nNext up is the backward computation. You'll notice that we have visited most of the coming concepts already. We already looked at methods to compute the correct indices for one-dimensional arrays by dimension sizes, we wrote a CUDA kernel, we registered our Op and we set up our tests. Therefore, I will speed things up a bit from here on. The only thing that is in our way is the exact definition of the gradient. It helps to consider a normal dense layer first:\nz\n=\nW\nx\n.\nGiven the gradients of a loss function \nL\n with respect to \nz\n, we should be able to get the gradients of \nx\n and \nW\n:\nd\nL\nd\nx\ni\n=\nd\nL\nd\nz\nd\nz\nd\nx\ni\n=\n→\nw\ni\nz\n′\n,\nwhere \n→\nw\ni\n is the i-th row vector and \nz\n′\n is the vector holding the local gradients of the output. If this is the result for \ni\n, then the whole gradient for \nx\n can be computed by:\nd\nL\nd\nx\n=\nW\n⊤\nz\n′\n.\nTo do the same for our capsule prediction operation we just have to replace \nz\n′\n with \n^\nu\n′\nj\n|\ni\n and \nW\nwith \nW\ni\nj\n.\n\nWhat about the gradient of a specific weight? Well, this is perhaps even easier:\nd\nL\nd\nw\nr\nc\n=\nx\nr\nz\n′\nc\n.\nSo the matrix holding the gradients becomes an outer product:\nd\nL\nd\nW\n=\nx\n(\nz\n′\n)\n⊤\n.\nIntuitively, this tells us to just pick the two neurons that are connected between these two layers and multiply the local gradient of the output neuron with the input neuron. This means that we can do the same for the capsule prediction layer. The only difference is the dimensionality of the tensors involved.\n\nThe gradient OpKernel\nI won't bother you with the details anymore. Take it or leave it:\n\n\nclass CapsulePredictionGradOp : public OpKernel\n{\n public:\n  explicit CapsulePredictionGradOp(OpKernelConstruction* ctx) : OpKernel(ctx) { }\n\n  void Compute(OpKernelContext* ctx) override\n  {\n    // Get the input tensors\n    const Tensor& grad = ctx->input(0);\n    const Tensor& input = ctx->input(1);\n    const Tensor& weights = ctx->input(2);\n\n    // Get the shapes so that we can allocate outputs\n    const TensorShape& input_shape(input.shape());\n    const TensorShape& weights_shape(weights.shape());\n\n    // Allocate outputs\n    Tensor* grad_input = nullptr;\n    Tensor* grad_weights = nullptr;\n    OP_REQUIRES_OK(ctx, ctx->allocate_output(0, input_shape, &grad_input));\n    OP_REQUIRES_OK(ctx, ctx->allocate_output(1, weights_shape, &grad_weights));\n\n    // Get the Eigen tensors and pass them on to the launch function\n    auto input_tensor         = input.tensor<float, 3>();\n    auto weights_tensor       = weights.tensor<float, 4>();\n    auto grad_tensor          = grad.tensor<float, 4>();\n    auto grad_input_tensor    = grad_input->tensor<float, 3>();\n    auto grad_weights_tensor  = grad_weights->tensor<float, 4>();\n    launchCapsulePredictionGrad(\n      ctx->eigen_device<GPUDevice>(), input_tensor, weights_tensor, grad_tensor,\n      grad_input_tensor, grad_weights_tensor\n    );\n  }\n};\n                \nNothing truly new here. An important difference is that we now have to allocate two output tensors: one for the weight gradient and one for the input gradient. The shape of a tensor's gradient is identical to the shape of the tensor itself. Thus finding the shape for the allocated tensors is a piece of cake. Let's check out the launchCapsulePredictionGrad function:\n\n\nvoid launchCapsulePredictionGrad(\n  const GPUDevice& d,\n  typename TTypes<float, 3>::ConstTensor input,\n  typename TTypes<float, 4>::ConstTensor weights,\n  typename TTypes<float, 4>::ConstTensor grad,\n  typename TTypes<float, 3>::Tensor grad_input,\n  typename TTypes<float, 4>::Tensor grad_weights)\n{\n  const int64 batch_size  = input.dimension(0);\n  const int64 in_caps     = input.dimension(1);\n  const int64 in_dim      = input.dimension(2);\n  const int64 out_dim     = weights.dimension(2);\n  const int64 out_caps    = weights.dimension(1);\n\n  // Size first dim\n  const int64 w_d0 = out_caps * out_dim * in_dim;\n  const int64 x_d0 = in_caps * in_dim;\n  const int64 o_d0 = in_caps * out_caps * out_dim;\n\n  // Second dim\n  const int64 w_d1 = out_dim * in_dim;\n  const int64 x_d1 = in_dim;\n  const int64 o_d1 = out_caps * out_dim;\n\n  // Third dim\n  const int64 w_d2 = in_dim;\n  const int64 o_d2 = out_dim;\n\n  // Launch input gradient kernel\n  CudaLaunchConfig config = GetCudaLaunchConfig(grad_input.size(), d);\n  capsulePredictionInputGradKernel\n    <<<config.block_count, config.thread_per_block, 0, d.stream()>>>(\n      grad.data(), weights.data(), grad_input.data(),\n      w_d0, x_d0, x_d1, o_d0, o_d1, out_caps, out_dim, in_dim,\n      grad_input.size());\n\n  // Launch weight gradient kernel\n  config = GetCudaLaunchConfig(grad_weights.size(), d);\n  capsulePredictionWeightsGradKernel\n    <<<config.block_count, config.thread_per_block, 0, d.stream()>>>(\n      grad.data(), input.data(), grad_weights.data(), batch_size,\n      grad_weights.size(), w_d0, w_d1, w_d2, x_d0, x_d1, o_d0, o_d1, o_d2);\n}\n                \nAgain, we see a similar code structure. We obtain the dimensions, we determine the memory sizes and finally, we launch not one, but two kernels.\nGlad you're still there! Now it gets a bit more complicated. Behold the input gradient CUDA kernel:\n\n\n__global__ void capsulePredictionInputGradKernel(\n  const float* grad, const float* weights, float* grad_input,\n  const int64 w_d0,\n  const int64 x_d0, const int64 x_d1,\n  const int64 o_d0, const int64 o_d1,\n  const int64 out_caps,\n  const int64 out_dim,\n  const int64 in_dim,\n  const int64 output_size)\n{\n  CUDA_1D_KERNEL_LOOP(i, output_size)\n  {\n    // So here we have in_grad[b,ci,e]\n    const int64 b     = i / x_d0;\n    const int64 ci    = (i % x_d0) / x_d1;\n    const int64 e_in  = i % x_d1;\n\n    // Then, we can have a look at computing the array indices for in and W\n    int64 w_idx       = ci * w_d0 + e_in;\n    int64 grad_idx    = b * o_d0 + ci * o_d1;\n\n    // Initialize result\n    float result      = 0.0;\n    // Iterate over cj and e_out, we already have the other indices\n    for (int cj = 0; cj < out_caps; ++cj)\n    {\n      for (int e_out = 0; e_out < out_dim; ++e_out)\n      {\n        // Next element of grad can be found by incrementing grad_idx\n        result  += ldg(grad + grad_idx++) * ldg(weights + w_idx);\n        // Next element of weights can be found by going to the next output\n        // capsule element, meaning that we add in_dim to w_idx\n        w_idx   += in_dim;\n      }\n    }\n    // Write the result\n    grad_input[i] = result;\n  }\n}\n                \nI have added quite some comments to the code to make it more readable. Similar to our previous CUDA kernel, this one determines the axis indices that matter: b, ci and e_in. These are then used to compute the one-dimensional indices for w and grad (the output gradient). A single input neuron is not just used for one matrix-vector product, but it is involved for all prediction matrices \nW\ni\nj\n,\nj\n∈\n{\n1\n,\n…\n,\n#\noutput capsules\n}\n. Therefore, we require two loops, one dealing with the output capsules and another one that deals with the individual capsule elements. Rather than just incrementing the index for our weights, we must now skip to the next output capsule in the inner loop. This means that we should add in_dim to the index on each iteration.\n\nThe weight gradient CUDA kernel implementation is:\n\n\n__global__ void capsulePredictionWeightsGradKernel(\n  const float* grad, const float* input, float* grad_weights,\n  const int64 batch_size, const int64 output_size,\n  const int64 w_d0, const int64 w_d1, const int64 w_d2,\n  const int64 x_d0, const int64 x_d1,\n  const int64 o_d0, const int64 o_d1, const int64 o_d2\n)\n{\n  CUDA_1D_KERNEL_LOOP(i, output_size)\n  {\n    // So here we have w[ci,cj,e_out,e_in]\n    const int64 ci    = i / w_d0;\n    const int64 cj    = (i % w_d0) / w_d1;\n    const int64 e_out = (i % w_d1) / w_d2;\n    const int64 e_in  = i % w_d2;\n\n    // Then, we can have a look at computing the array indices for\n    // in and grad\n    int64 input_idx   = ci * x_d1 + e_in;               // (b == 0)\n    int64 grad_idx    = ci * o_d1 + cj * o_d2 + e_out;  // (b == 0)\n\n    // Initilize result\n    float result      = 0.0;\n    // We only iterate over b, since we have the other indices already\n    for (int64 b = 0; b < batch_size; b++)\n    {\n      result += ldg(grad + grad_idx) * ldg(input + input_idx);\n      // Next elements can be found by jumping to the next batch\n      input_idx += x_d0;\n      grad_idx  += o_d0;\n    }\n    grad_weights[i] = result;\n  }\n}\n                \nThe same tricks apply here again: we compute the tensor axis indices, we compute the one-dimensional indices and we loop over the required elements to obtain our output. What I didn't mention before is that we have to sum the gradients for each weight over all samples in the batch. Given the memory size for the input and output tensors along the 0-th axis, this is a straightforward thing to do.\n\nBefore our gradient implementation nicely integrates with TensorFlow, we have to register it as being the gradient of our CapsulePrediction Op:\n\n\n@ops.RegisterGradient(\"CapsulePrediction\")\ndef _capsule_prediction_grad(op, grad):\n    \"\"\" Computes gradient for capsule prediction operation \"\"\"\n    return op_module.capsule_prediction_grad(grad, op.inputs[0], op.inputs[1])\n                \nNow we can just use tf.gradients after which the computation graph for the gradient should include our gradient op. Awesome!\n\nTesting the gradients\nWe have arrived at one of the last stages: testing the gradient. This really sounds harder than it is. TensorFlow already has a gradient testing utility, and we'll use it here. We add the following methods to our CapsulePredictionOpTest class:\n\n\n@parameterized.expand([\n    (batch_size, in_caps, out_caps, in_dim, out_dim) for\n    batch_size, in_caps, out_caps, in_dim, out_dim in\n    itertools.product([4, 8], [4, 8], [4, 8], [4, 8], [4, 8])\n])\ndef test_capsule_prediction_weights_grad(self, batch_size, in_caps, out_caps,\n                                         in_dim, out_dim):\n    \"\"\" Tests gradient of output w.r.t. weights \"\"\"\n    x = np.random.rand(batch_size, in_caps, in_dim)\n    weights = np.random.rand(in_caps, out_caps, out_dim, in_dim)\n    out_shape = (batch_size, in_caps, out_caps, out_dim)\n\n    with self.test_session():\n        x_ph = tf.placeholder(tf.float32, x.shape)\n        w_ph = tf.placeholder(tf.float32, weights.shape)\n        fd = {x_ph: x, w_ph: weights}\n\n        caps_out = capsule_prediction(x_ph, w_ph)\n        grad_w = tf.test.compute_gradient(\n            w_ph, weights.shape, caps_out, out_shape, extra_feed_dict=fd\n        )\n\n    self.assertAllClose(grad_w[0], grad_w[1], atol=1e-3, rtol=1e-3)\n\n@parameterized.expand([\n    (batch_size, in_caps, out_caps, in_dim, out_dim) for\n    batch_size, in_caps, out_caps, in_dim, out_dim in\n    itertools.product([4, 8], [4, 8], [4, 8], [4, 8], [4, 8])\n])\ndef test_capsule_prediction_input_grad(self, batch_size, in_caps, out_caps,\n                                       in_dim, out_dim):\n    \"\"\" Tests gradient of output w.r.t. x \"\"\"\n    x = np.random.rand(batch_size, in_caps, in_dim)\n    weights = np.random.rand(in_caps, out_caps, out_dim, in_dim)\n    out_shape = (batch_size, in_caps, out_caps, out_dim)\n\n    with self.test_session():\n        x_ph = tf.placeholder(tf.float32, x.shape)\n        w_ph = tf.placeholder(tf.float32, weights.shape)\n        fd = {x_ph: x, w_ph: weights}\n        caps_out = capsule_prediction(x_ph, w_ph)\n        grad_x = tf.test.compute_gradient(\n            x_ph, x.shape, caps_out, out_shape, extra_feed_dict=fd\n        )\n\n    self.assertAllClose(grad_x[0], grad_x[1], atol=1e-3, rtol=1e-3)\n                \nThe tf.test.compute_gradient function determines the 'theoretical' and numerical gradient respectively. The numerical gradient is computed by finite differences whereas the theoretical gradient is computed by our Op's registered gradient. They should be nearly equal, so we assert they are close by using the assertAllClose method that is inherited from tf.test.TestCase. Here is the resulting output:\n\nTest output capsule prediction\nWhoa! It's alive!\n\nRunning it on MNIST classification\nIn my previous blog post, I have already discussed a capsule network for MNIST classification. We can now insert our capsule_prediction function in the code:\n\n\ndef digit_caps(incoming, n_digit_caps, dim_digit_caps, name=\"DigitCaps\",\n               neuron_axis=-1, capsule_axis=-2, routing_iters=3):\n    \"\"\" Digit capsule layer \"\"\"\n    with tf.variable_scope(name):\n        # Get number of capsules and dimensionality of previous layer\n        in_shape = incoming.shape.as_list()\n        n_primary_caps = in_shape[capsule_axis]\n        dim_primary_caps = in_shape[neuron_axis]\n        # Initialize all weight matrices\n        w_shape = [n_primary_caps, n_digit_caps, dim_digit_caps, dim_primary_caps]\\\n            if args.custom_op \\\n            else [n_primary_caps, n_digit_caps * dim_digit_caps, dim_primary_caps]\n\n        W_ij = tf.get_variable(\n            \"weights\", shape=w_shape,\n            initializer=tf.keras.initializers.glorot_uniform()\n        )\n        # Initialize routing logits, the leading axis with size 1 is added for\n        # convenience.\n        b_ij = tf.get_variable(\n            \"logits\", shape=[1, n_primary_caps, n_digit_caps],\n            initializer=tf.zeros_initializer(), trainable=args.logits_trainable\n        )\n        if args.custom_op:\n            # Custom op\n            u_hat = capsule_prediction(incoming, W_ij)\n        else:\n            # Reshape and transpose hacking\n            u_i = tf.transpose(incoming, (1, 2, 0))\n            u_hat = tf.matmul(W_ij, u_i)\n            u_hat = tf.reshape(\n                tf.transpose(u_hat, (2, 0, 1)),\n                (-1, n_primary_caps, n_digit_caps, dim_digit_caps)\n            )\n\n        def capsule_out(b_ij):\n            \"\"\" Given the logits b_ij, computes the output of this layer. \"\"\"\n            c_ij = tf.nn.softmax(b_ij, axis=2)\n            s_j = tf.reduce_sum(\n                tf.reshape(c_ij, (-1, n_primary_caps, n_digit_caps, 1)) * u_hat,\n                axis=1\n            )\n            v_j = squash(s_j)\n            return v_j\n\n        def routing_iteration(iter, logits):\n            \"\"\"\n            Given a set of logits, computes the new logits using the routing\n            definition from the paper.\n            \"\"\"\n            v_j = capsule_out(logits)\n            a_ij = tf.reduce_sum(tf.expand_dims(v_j, axis=1) * u_hat, axis=3)\n            logits = tf.reshape(logits + a_ij, (-1, n_primary_caps, n_digit_caps))\n            return [iter + 1, logits]\n\n        # Compute routing\n        i = tf.constant(0)\n        routing_result = tf.while_loop(\n            lambda i, logits: tf.less(i, routing_iters),\n            routing_iteration,\n            [i, tf.tile(b_ij, tf.stack([tf.shape(incoming)[0], 1, 1]))]\n        )\n        # Second element of the result contains our final logits\n        v_j = capsule_out(routing_result[1])\n\n    return v_j\n                \nSo what about performance? Well, it turns out that training with the custom op runs somewhat slower than training with the transpose and reshape hacking. As I've stated before, the code could be optimized even more. Perhaps this will be on a next blog post. Anyway, thanks for staying around till the end and I would love to hear suggestions and feedback! -->","frontmatter":{"title":"","date":null,"description":null}}},"pageContext":{"isCreatedByStatefulCreatePages":false,"slug":"/capsnet/","previous":{"fields":{"slug":"/spn01/"},"frontmatter":{"title":"Tensorizing Sum-Product Networks"}},"next":null}}