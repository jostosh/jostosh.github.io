{"data":{"site":{"siteMetadata":{"title":"Machine Learning Blog - JvdW","author":"Jos van de Wolfshaar"}},"markdownRemark":{"id":"8fb27e4d-adcd-5d3e-85c8-ce73b6468923","excerpt":"In Part I, we have looked at the fundamental principles for Sum-Product Networks. Recall the following:SPNs consist of leaf nodes, sum nodes and product nodes…","html":"<p>In <a href=\"../spn01\">Part I</a>, we have looked at the fundamental principles for Sum-Product Networks. Recall the following:</p>\n<ul>\n<li>SPNs consist of leaf nodes, sum nodes and product nodes.</li>\n<li>Leaf nodes correspond to <em>indicators</em> or <em>components</em> of single variables.</li>\n<li>A <em>scope</em> is a set of variables. A parent node obtains the union of the scopes of its children. Leaf nodes have scopes of exactly one variable. The root of an SPN contains all variables in its scope.</li>\n<li>Children of sum nodes must have identical scopes to ensure <em>completeness</em></li>\n<li>Children of product nodes must have pairwise disjoint scopes to ensure <em>decomposability</em></li>\n<li>Decomposability and completeness are sufficient properties for <em>validity</em></li>\n<li>A <em>valid</em> SPN computes an unnormalized probability. A <em>normalized</em> valid SPN computes a normalized probability directly.</li>\n</ul>\n<h2>Layered Implementations</h2>\n<p>Thinking of neural or SPN architectures in terms of layers helps us to quickly summarize the structure of a model. By simply giving the description of these layers, we can get a grasp of the complexity of the model and how it tends to divide and conquer its inputs. In the realm of neural networks, layers serve as the de facto implementation strategy. In fact, most feedforward layers can be expressed with only a few operations. Take, for instance, a dense layer with ReLU activations:</p>\n<span class=\"katex-display\"><span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mi>f</mi><mo stretchy=\"false\">(</mo><mi>X</mi><mo stretchy=\"false\">)</mo><mo>=</mo><mi>max</mi><mo>⁡</mo><mo stretchy=\"false\">{</mo><mi>X</mi><mi>W</mi><mo separator=\"true\">,</mo><mn>0</mn><mo stretchy=\"false\">}</mo></mrow><annotation encoding=\"application/x-tex\">f(X) = \\max\\{XW, 0\\}</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:1em;vertical-align:-0.25em;\"></span><span class=\"mord mathdefault\" style=\"margin-right:0.10764em;\">f</span><span class=\"mopen\">(</span><span class=\"mord mathdefault\" style=\"margin-right:0.07847em;\">X</span><span class=\"mclose\">)</span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span><span class=\"mrel\">=</span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:1em;vertical-align:-0.25em;\"></span><span class=\"mop\">max</span><span class=\"mopen\">{</span><span class=\"mord mathdefault\" style=\"margin-right:0.07847em;\">X</span><span class=\"mord mathdefault\" style=\"margin-right:0.13889em;\">W</span><span class=\"mpunct\">,</span><span class=\"mspace\" style=\"margin-right:0.16666666666666666em;\"></span><span class=\"mord\">0</span><span class=\"mclose\">}</span></span></span></span></span>\n<p>Where <span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mi>X</mi></mrow><annotation encoding=\"application/x-tex\">X</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.68333em;vertical-align:0em;\"></span><span class=\"mord mathdefault\" style=\"margin-right:0.07847em;\">X</span></span></span></span> is a matrix with dimensions <code class=\"language-text\">[batch, num_in]</code> and <span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mi>W</mi></mrow><annotation encoding=\"application/x-tex\">W</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.68333em;vertical-align:0em;\"></span><span class=\"mord mathdefault\" style=\"margin-right:0.13889em;\">W</span></span></span></span> is a weight matrix with dimensions <code class=\"language-text\">[num_in, num_out]</code>. In TensorFlow, this is simply:</p>\n<div class=\"gatsby-highlight\" data-language=\"python\"><pre class=\"language-python\"><code class=\"language-python\">f_x <span class=\"token operator\">=</span> tf<span class=\"token punctuation\">.</span>nn<span class=\"token punctuation\">.</span>relu<span class=\"token punctuation\">(</span>tf<span class=\"token punctuation\">.</span>matmul<span class=\"token punctuation\">(</span>x<span class=\"token punctuation\">,</span> w<span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span></code></pre></div>\n<p>Such that the output <code class=\"language-text\">f_x</code> is also a matrix of shape <code class=\"language-text\">[batch, num_out]</code>.</p>\n<p>Other layer types like convolutional layers or recurrent layers will require slightly different views on the dimensions and internal operations, but usually, there is a leading <code class=\"language-text\">batch</code> dimension in each of the tensors including some sort of dot product operation with weights to compute activations of neurons.</p>\n<p>We will aim to bring our SPN implementations close to this approach. If we manage to describe SPN layers in terms of a few operations to compute all sum operations for all nodes in a single layer for all samples in the batch, we maximally exploit TensorFlow’s ability to launch these operations in parallel on GPUs! However, SPNs introduce some specific challenges which we’ll get to next.</p>\n<h3>Sum Layer</h3>\n<p>A sum layer in an SPN computes weighted sums, similar to a dense layer in an MLP. There are two noticable differences with weighted sums in dense layers and weighted sums in SPN layers:</p>\n<ol>\n<li>SPN layers compute probabilities and should be implemented to operate in the log-domain to avoid numerical underflow;</li>\n<li>A single sum in a sum layer of an SPN is connected to only a subset of the preceding layer, as it can only have children with identical scopes.</li>\n</ol>\n<p>So let’s take a step back and assume that we only want to compute the log-probability of a single sum node:</p>\n<div class=\"gatsby-highlight\" data-language=\"python\"><pre class=\"language-python\"><code class=\"language-python\"><span class=\"token comment\"># x is a vector of log-prob inputs, while w is a vector of weights </span>\nout <span class=\"token operator\">=</span> tf<span class=\"token punctuation\">.</span>reduce_logsumexp<span class=\"token punctuation\">(</span>x_prime <span class=\"token operator\">+</span> w_prime<span class=\"token punctuation\">)</span></code></pre></div>\n<p>As we’re operating in the log-domain, the pairwise multiplication of <span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mi mathvariant=\"bold-italic\">x</mi></mrow><annotation encoding=\"application/x-tex\">\\boldsymbol x</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.44444em;vertical-align:0em;\"></span><span class=\"mord\"><span class=\"mord boldsymbol\">x</span></span></span></span></span> and <span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mi mathvariant=\"bold-italic\">w</mi></mrow><annotation encoding=\"application/x-tex\">\\boldsymbol w</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.44444em;vertical-align:0em;\"></span><span class=\"mord\"><span class=\"mord boldsymbol\" style=\"margin-right:0.02778em;\">w</span></span></span></span></span> becomes a pairwise addition of the log-space correspondents <span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><msup><mi mathvariant=\"bold-italic\">x</mi><mo mathvariant=\"normal\">′</mo></msup></mrow><annotation encoding=\"application/x-tex\">\\boldsymbol x&#x27;</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.751892em;vertical-align:0em;\"></span><span class=\"mord\"><span class=\"mord\"><span class=\"mord boldsymbol\">x</span></span><span class=\"msupsub\"><span class=\"vlist-t\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.751892em;\"><span style=\"top:-3.063em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\"><span class=\"mord mtight\">′</span></span></span></span></span></span></span></span></span></span></span></span> and <span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><msup><mi mathvariant=\"bold-italic\">w</mi><mo mathvariant=\"normal\">′</mo></msup></mrow><annotation encoding=\"application/x-tex\">\\boldsymbol w&#x27;</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.751892em;vertical-align:0em;\"></span><span class=\"mord\"><span class=\"mord\"><span class=\"mord boldsymbol\" style=\"margin-right:0.02778em;\">w</span></span><span class=\"msupsub\"><span class=\"vlist-t\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.751892em;\"><span style=\"top:-3.063em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\"><span class=\"mord mtight\">′</span></span></span></span></span></span></span></span></span></span></span></span>. To sum up the pairwise multiplications, we use <code class=\"language-text\">tf.reduce_logsumexp</code>, which does the following:</p>\n<span class=\"katex-display\"><span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><msup><mi mathvariant=\"bold-italic\">y</mi><mo mathvariant=\"normal\">′</mo></msup><mo>=</mo><mi>log</mi><mo>⁡</mo><mrow><mo fence=\"true\">(</mo><munder><mo>∑</mo><mi>i</mi></munder><mi>exp</mi><mo>⁡</mo><mo stretchy=\"false\">(</mo><msubsup><mi>x</mi><mi>i</mi><mo mathvariant=\"normal\">′</mo></msubsup><mo>+</mo><msubsup><mi>w</mi><mi>i</mi><mo mathvariant=\"normal\">′</mo></msubsup><mo stretchy=\"false\">)</mo><mo fence=\"true\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">\\boldsymbol y&#x27; = \\log \\left( \\sum_i \\exp(x_i&#x27; + w_i&#x27;) \\right)</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.996332em;vertical-align:-0.19444em;\"></span><span class=\"mord\"><span class=\"mord\"><span class=\"mord boldsymbol\" style=\"margin-right:0.03704em;\">y</span></span><span class=\"msupsub\"><span class=\"vlist-t\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.801892em;\"><span style=\"top:-3.113em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\"><span class=\"mord mtight\">′</span></span></span></span></span></span></span></span></span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span><span class=\"mrel\">=</span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:3.027669em;vertical-align:-1.277669em;\"></span><span class=\"mop\">lo<span style=\"margin-right:0.01389em;\">g</span></span><span class=\"mspace\" style=\"margin-right:0.16666666666666666em;\"></span><span class=\"minner\"><span class=\"mopen delimcenter\" style=\"top:0em;\"><span class=\"delimsizing size4\">(</span></span><span class=\"mop op-limits\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:1.0500050000000003em;\"><span style=\"top:-1.872331em;margin-left:0em;\"><span class=\"pstrut\" style=\"height:3.05em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mathdefault mtight\">i</span></span></span><span style=\"top:-3.050005em;\"><span class=\"pstrut\" style=\"height:3.05em;\"></span><span><span class=\"mop op-symbol large-op\">∑</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:1.277669em;\"><span></span></span></span></span></span><span class=\"mspace\" style=\"margin-right:0.16666666666666666em;\"></span><span class=\"mop\">exp</span><span class=\"mopen\">(</span><span class=\"mord\"><span class=\"mord mathdefault\">x</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.8018919999999999em;\"><span style=\"top:-2.4530000000000003em;margin-left:0em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mathdefault mtight\">i</span></span></span><span style=\"top:-3.113em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\"><span class=\"mord mtight\">′</span></span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.247em;\"><span></span></span></span></span></span></span><span class=\"mspace\" style=\"margin-right:0.2222222222222222em;\"></span><span class=\"mbin\">+</span><span class=\"mspace\" style=\"margin-right:0.2222222222222222em;\"></span><span class=\"mord\"><span class=\"mord mathdefault\" style=\"margin-right:0.02691em;\">w</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.8018919999999999em;\"><span style=\"top:-2.4530000000000003em;margin-left:-0.02691em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mathdefault mtight\">i</span></span></span><span style=\"top:-3.113em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\"><span class=\"mord mtight\">′</span></span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.247em;\"><span></span></span></span></span></span></span><span class=\"mclose\">)</span><span class=\"mclose delimcenter\" style=\"top:0em;\"><span class=\"delimsizing size4\">)</span></span></span></span></span></span></span>\n<p>But here lies a danger: computing <span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mi>exp</mi><mo>⁡</mo><mo stretchy=\"false\">(</mo><msubsup><mi>x</mi><mi>i</mi><mo mathvariant=\"normal\">′</mo></msubsup><mo>+</mo><msubsup><mi>w</mi><mi>i</mi><mo mathvariant=\"normal\">′</mo></msubsup><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">\\exp(x_i&#x27; + w_i&#x27;)</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:1.010556em;vertical-align:-0.258664em;\"></span><span class=\"mop\">exp</span><span class=\"mopen\">(</span><span class=\"mord\"><span class=\"mord mathdefault\">x</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.751892em;\"><span style=\"top:-2.441336em;margin-left:0em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mathdefault mtight\">i</span></span></span><span style=\"top:-3.063em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\"><span class=\"mord mtight\">′</span></span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.258664em;\"><span></span></span></span></span></span></span><span class=\"mspace\" style=\"margin-right:0.2222222222222222em;\"></span><span class=\"mbin\">+</span><span class=\"mspace\" style=\"margin-right:0.2222222222222222em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:1.010556em;vertical-align:-0.258664em;\"></span><span class=\"mord\"><span class=\"mord mathdefault\" style=\"margin-right:0.02691em;\">w</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.751892em;\"><span style=\"top:-2.441336em;margin-left:-0.02691em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mathdefault mtight\">i</span></span></span><span style=\"top:-3.063em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\"><span class=\"mord mtight\">′</span></span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.258664em;\"><span></span></span></span></span></span></span><span class=\"mclose\">)</span></span></span></span> could already result in underflow. The remedy is to subtract a constant <span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mi>C</mi></mrow><annotation encoding=\"application/x-tex\">C</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.68333em;vertical-align:0em;\"></span><span class=\"mord mathdefault\" style=\"margin-right:0.07153em;\">C</span></span></span></span> from each element of the result of <span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><msubsup><mi>x</mi><mi>i</mi><mo mathvariant=\"normal\">′</mo></msubsup></mrow><annotation encoding=\"application/x-tex\">x_i&#x27;</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:1.010556em;vertical-align:-0.258664em;\"></span><span class=\"mord\"><span class=\"mord mathdefault\">x</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.751892em;\"><span style=\"top:-2.441336em;margin-left:0em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mathdefault mtight\">i</span></span></span><span style=\"top:-3.063em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\"><span class=\"mord mtight\">′</span></span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.258664em;\"><span></span></span></span></span></span></span></span></span></span> and <span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><msubsup><mi>w</mi><mi>i</mi><mo mathvariant=\"normal\">′</mo></msubsup></mrow><annotation encoding=\"application/x-tex\">w_i&#x27;</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:1.010556em;vertical-align:-0.258664em;\"></span><span class=\"mord\"><span class=\"mord mathdefault\" style=\"margin-right:0.02691em;\">w</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.751892em;\"><span style=\"top:-2.441336em;margin-left:-0.02691em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mathdefault mtight\">i</span></span></span><span style=\"top:-3.063em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\"><span class=\"mord mtight\">′</span></span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.258664em;\"><span></span></span></span></span></span></span></span></span></span>.</p>","frontmatter":{"title":"Tensor-Based Sum-Product Networks: Part II","date":"July 07, 2019","description":"Layer-based implementations of Sum-Product Networks with TensorFlow."}}},"pageContext":{"isCreatedByStatefulCreatePages":false,"slug":"/spn02/","previous":{"fields":{"slug":"/spn01/"},"frontmatter":{"title":"Tensor-Based Sum-Product Networks: Part I"}},"next":null}}