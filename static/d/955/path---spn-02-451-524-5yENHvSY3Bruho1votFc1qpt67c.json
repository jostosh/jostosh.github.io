{"data":{"site":{"siteMetadata":{"title":"Machine Learning Blog - JvdW","author":"Jos van de Wolfshaar"}},"markdownRemark":{"id":"8fb27e4d-adcd-5d3e-85c8-ce73b6468923","excerpt":"In Part I, we have looked at the fundamental principles for Sum-Product Networks. Recall the following:SPNs consist of leaf nodes, sum nodes and product nodes…","html":"<p>In <a href=\"../spn01\">Part I</a>, we have looked at the fundamental principles for Sum-Product Networks. Recall the following:</p>\n<ul>\n<li>SPNs consist of leaf nodes, sum nodes and product nodes.</li>\n<li>Leaf nodes correspond to <em>indicators</em> or <em>components</em> of single variables.</li>\n<li>A <em>scope</em> is a set of variables. A parent node obtains the union of the scopes of its children. Leaf nodes have scopes of exactly one variable. The root of an SPN contains all variables in its scope.</li>\n<li>Children of sum nodes must have identical scopes to ensure <em>completeness</em></li>\n<li>Children of product nodes must have pairwise disjoint scopes to ensure <em>decomposability</em></li>\n<li>Decomposability and completeness are sufficient properties for <em>validity</em></li>\n<li>A <em>valid</em> SPN computes an unnormalized probability. A <em>normalized</em> valid SPN computes a normalized probability directly.</li>\n</ul>\n<h2>Layered Implementations</h2>\n<p>Thinking of neural or SPN architectures in terms of layers helps us to quickly summarize the structure of a model. By simply giving the description of these layers, we can get a grasp of the complexity of the model and how it tends to divide and conquer its inputs. In the realm of neural networks, layers serve as the de facto implementation strategy. In fact, most feedforward layers can be expressed with only a few operations. Take, for instance, a dense layer with ReLU activations:</p>\n<span class=\"katex-display\"><span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mi>f</mi><mo stretchy=\"false\">(</mo><mi>X</mi><mo stretchy=\"false\">)</mo><mo>=</mo><mi>max</mi><mo>⁡</mo><mo stretchy=\"false\">{</mo><mi>X</mi><mi>W</mi><mo separator=\"true\">,</mo><mn>0</mn><mo stretchy=\"false\">}</mo></mrow><annotation encoding=\"application/x-tex\">f(X) = \\max\\{XW, 0\\}</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:1em;vertical-align:-0.25em;\"></span><span class=\"mord mathdefault\" style=\"margin-right:0.10764em;\">f</span><span class=\"mopen\">(</span><span class=\"mord mathdefault\" style=\"margin-right:0.07847em;\">X</span><span class=\"mclose\">)</span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span><span class=\"mrel\">=</span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:1em;vertical-align:-0.25em;\"></span><span class=\"mop\">max</span><span class=\"mopen\">{</span><span class=\"mord mathdefault\" style=\"margin-right:0.07847em;\">X</span><span class=\"mord mathdefault\" style=\"margin-right:0.13889em;\">W</span><span class=\"mpunct\">,</span><span class=\"mspace\" style=\"margin-right:0.16666666666666666em;\"></span><span class=\"mord\">0</span><span class=\"mclose\">}</span></span></span></span></span>\n<p>Where <span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mi>X</mi></mrow><annotation encoding=\"application/x-tex\">X</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.68333em;vertical-align:0em;\"></span><span class=\"mord mathdefault\" style=\"margin-right:0.07847em;\">X</span></span></span></span> is a matrix with dimensions <code class=\"language-text\">[batch, num_in]</code> and <span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mi>W</mi></mrow><annotation encoding=\"application/x-tex\">W</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.68333em;vertical-align:0em;\"></span><span class=\"mord mathdefault\" style=\"margin-right:0.13889em;\">W</span></span></span></span> is a weight matrix with dimensions <code class=\"language-text\">[num_in, num_out]</code>. In TensorFlow, this is simply:</p>\n<div class=\"gatsby-highlight\" data-language=\"python\"><pre class=\"language-python\"><code class=\"language-python\">f_x <span class=\"token operator\">=</span> tf<span class=\"token punctuation\">.</span>nn<span class=\"token punctuation\">.</span>relu<span class=\"token punctuation\">(</span>tf<span class=\"token punctuation\">.</span>matmul<span class=\"token punctuation\">(</span>x<span class=\"token punctuation\">,</span> w<span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span></code></pre></div>\n<p>Such that the output <code class=\"language-text\">f_x</code> is also a matrix of shape <code class=\"language-text\">[batch, num_out]</code></p>","frontmatter":{"title":"Tensor-Based Sum-Product Networks: Part II","date":"July 07, 2019","description":"Layer-based implementations of Sum-Product Networks with TensorFlow."}}},"pageContext":{"isCreatedByStatefulCreatePages":false,"slug":"/spn02/","previous":{"fields":{"slug":"/spn01/"},"frontmatter":{"title":"Tensor-Based Sum-Product Networks: Part I"}},"next":null}}