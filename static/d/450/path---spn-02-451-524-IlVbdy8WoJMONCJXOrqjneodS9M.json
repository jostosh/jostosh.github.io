{"data":{"site":{"siteMetadata":{"title":"Machine Learning Blog - JvdW","author":"Jos van de Wolfshaar"}},"markdownRemark":{"id":"8fb27e4d-adcd-5d3e-85c8-ce73b6468923","excerpt":"In Part I, we have looked at the fundamental principles for Sum-Product Networks. Recall the following:SPNs consist of leaf nodes, sum nodes and product nodes…","html":"<p>In <a href=\"../spn01\">Part I</a>, we have looked at the fundamental principles for Sum-Product Networks. Recall the following:</p>\n<ul>\n<li>SPNs consist of leaf nodes, sum nodes and product nodes.</li>\n<li>Leaf nodes correspond to <em>indicators</em> or <em>components</em> of single variables.</li>\n<li>A <em>scope</em> is a set of variables. A parent node obtains the union of the scopes of its children. Leaf nodes have scopes of exactly one variable. The root of an SPN contains all variables in its scope.</li>\n<li>Children of sum nodes must have identical scopes to ensure <em>completeness</em></li>\n<li>Children of product nodes must have pairwise disjoint scopes to ensure <em>decomposability</em></li>\n<li>Decomposability and completeness are sufficient properties for <em>validity</em></li>\n<li>A <em>valid</em> SPN computes an unnormalized probability. A <em>normalized</em> valid SPN computes a normalized probability directly.</li>\n</ul>\n<h2>Layered Implementations</h2>\n<p>Thinking of neural or SPN architectures in terms of layers helps us to quickly summarize the structure of a model. By simply giving the description of these layers, we can get a grasp of the complexity of the model and how it tends to divide and conquer its inputs. In the realm of neural networks, layers serve as the de facto implementation strategy. In fact, most feedforward layers can be expressed with only a few operations. Take, for instance, a dense layer with ReLU activations:</p>\n<span class=\"katex-display\"><span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mi>f</mi><mo stretchy=\"false\">(</mo><mi>X</mi><mo stretchy=\"false\">)</mo><mo>=</mo><mi>max</mi><mo>⁡</mo><mo stretchy=\"false\">{</mo><mi>X</mi><mi>W</mi><mo separator=\"true\">,</mo><mn>0</mn><mo stretchy=\"false\">}</mo></mrow><annotation encoding=\"application/x-tex\">f(X) = \\max\\{XW, 0\\}</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:1em;vertical-align:-0.25em;\"></span><span class=\"mord mathdefault\" style=\"margin-right:0.10764em;\">f</span><span class=\"mopen\">(</span><span class=\"mord mathdefault\" style=\"margin-right:0.07847em;\">X</span><span class=\"mclose\">)</span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span><span class=\"mrel\">=</span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:1em;vertical-align:-0.25em;\"></span><span class=\"mop\">max</span><span class=\"mopen\">{</span><span class=\"mord mathdefault\" style=\"margin-right:0.07847em;\">X</span><span class=\"mord mathdefault\" style=\"margin-right:0.13889em;\">W</span><span class=\"mpunct\">,</span><span class=\"mspace\" style=\"margin-right:0.16666666666666666em;\"></span><span class=\"mord\">0</span><span class=\"mclose\">}</span></span></span></span></span>\n<p>Where <span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mi>X</mi></mrow><annotation encoding=\"application/x-tex\">X</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.68333em;vertical-align:0em;\"></span><span class=\"mord mathdefault\" style=\"margin-right:0.07847em;\">X</span></span></span></span> is a matrix with dimensions <code class=\"language-text\">[batch, num_in]</code> and <span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mi>W</mi></mrow><annotation encoding=\"application/x-tex\">W</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.68333em;vertical-align:0em;\"></span><span class=\"mord mathdefault\" style=\"margin-right:0.13889em;\">W</span></span></span></span> is a weight matrix with dimensions <code class=\"language-text\">[num_in, num_out]</code>. In TensorFlow, this is simply:</p>\n<div class=\"gatsby-highlight\" data-language=\"python\"><pre class=\"language-python\"><code class=\"language-python\">f_x <span class=\"token operator\">=</span> tf<span class=\"token punctuation\">.</span>nn<span class=\"token punctuation\">.</span>relu<span class=\"token punctuation\">(</span>tf<span class=\"token punctuation\">.</span>matmul<span class=\"token punctuation\">(</span>x<span class=\"token punctuation\">,</span> w<span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span></code></pre></div>\n<p>Such that the output <code class=\"language-text\">f_x</code> is also a matrix of shape <code class=\"language-text\">[batch, num_out]</code>.</p>\n<p>Other layer types like convolutional layers or recurrent layers will require slightly different views on the dimensions and internal operations, but usually, there is a leading <code class=\"language-text\">batch</code> dimension in each of the tensors including some sort of dot product operation with weights to compute activations of neurons.</p>\n<p>We will aim to bring our SPN implementations close to this approach. If we manage to describe SPN layers in terms of a few operations to compute all sum operations for all nodes in a single layer for all samples in the batch, we maximally exploit TensorFlow’s ability to launch these operations in parallel on GPUs! However, SPNs introduce some specific challenges which we’ll get to next.</p>\n<h3>Sum Layer</h3>\n<p>A sum layer in an SPN computes weighted sums, similar to a dense layer in an MLP. There are two noticable differences with weighted sums in dense layers and weighted sums in SPN layers:</p>\n<ol>\n<li>SPN layers compute probabilities and should be implemented to operate in the log-domain to avoid numerical underflow;</li>\n<li>A single sum in a sum layer of an SPN is connected to only a subset of the preceding layer, as it can only have children with identical scopes.</li>\n</ol>\n<p>So let’s take a step back and assume that we only want to compute the log-probability of a single sum node:</p>\n<div class=\"gatsby-highlight\" data-language=\"python\"><pre class=\"language-python\"><code class=\"language-python\"><span class=\"token comment\"># x is a vector of log-prob inputs, while w is a vector of weights </span>\nout <span class=\"token operator\">=</span> tf<span class=\"token punctuation\">.</span>reduce_logsumexp<span class=\"token punctuation\">(</span>x <span class=\"token operator\">+</span> w<span class=\"token punctuation\">)</span></code></pre></div>\n<p>As we’re operating in the log-domain, the pairwise multiplication of <code class=\"language-text\">x</code> and <code class=\"language-text\">w</code> becomes a pairwise addition.</p>","frontmatter":{"title":"Tensor-Based Sum-Product Networks: Part II","date":"July 07, 2019","description":"Layer-based implementations of Sum-Product Networks with TensorFlow."}}},"pageContext":{"isCreatedByStatefulCreatePages":false,"slug":"/spn02/","previous":{"fields":{"slug":"/spn01/"},"frontmatter":{"title":"Tensor-Based Sum-Product Networks: Part I"}},"next":null}}