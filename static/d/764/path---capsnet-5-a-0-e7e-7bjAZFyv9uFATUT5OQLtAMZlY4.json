{"data":{"site":{"siteMetadata":{"title":"Machine Learning Blog - JvdW","author":"Jos van de Wolfshaar"}},"markdownRemark":{"id":"82e1e556-6bc1-5bf8-9963-41cd9b94e491","excerpt":"Recently I’ve spent some time on CUDA programming and implementing custom Ops for TensorFlow. As an exercise, I decided to take a shot at implementing a custom…","html":"<p>Recently I’ve spent some time on CUDA programming and implementing custom Ops for TensorFlow. As an exercise, I decided to take a shot at implementing a custom Op for one of the operations in capsule networks that would normally require some reshape hacking or at least a <a href=\"https://github.com/Sarasra/models/tree/master/research/capsules\">couple of intermediate TensorFlow Ops</a>. If you’re not familiar with Capsule Networks, have a look at the <a href=\"https://arxiv.org/abs/1710.09829\">original paper</a> or my previous post. The open-sourced capsule network code by the paper’s authors can be found <a href=\"https://github.com/Sarasra/models/tree/master/research/capsules\">here</a>. The code that we discuss in this blog post can be found <a href=\"https://github.com/jostosh/capsnet\">here</a>.</p>\n<p>Many of the concepts that are covered in this post (CUDA, TensorFlow custom Ops, gradient testing) can be learned by going through their corresponding documentations, but I always think it is enlightening to see how such separate elements come together. This is the main motivation behind my blog post: showcasing the development of a custom TensorFlow Op with CUDA from start to end. So let’s commence.</p>\n<p>There also exists a Chinese translation of this post by <a href=\"https://www.jqr.com/article/000055\">Jakukyo Friel</a>.</p>\n<h2>Capsule prediction</h2>\n<p>The operation of interest in this blog post is the one that computes:</p>\n<span class=\"katex-display\"><span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><msub><mover accent=\"true\"><mi mathvariant=\"bold-italic\">u</mi><mo>^</mo></mover><mrow><mi>j</mi><mi mathvariant=\"normal\">∣</mi><mi>i</mi></mrow></msub><mo>=</mo><msub><mi>W</mi><mrow><mi>i</mi><mi>j</mi></mrow></msub><msub><mi mathvariant=\"bold-italic\">u</mi><mrow><mi>i</mi><mi>j</mi></mrow></msub></mrow><annotation encoding=\"application/x-tex\">\\hat{\\boldsymbol u}_{j|i} = W_{ij} \\boldsymbol u_{ij}</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:1.0630799999999998em;vertical-align:-0.3551999999999999em;\"></span><span class=\"mord\"><span class=\"mord accent\"><span class=\"vlist-t\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.70788em;\"><span style=\"top:-3em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span class=\"mord\"><span class=\"mord\"><span class=\"mord boldsymbol\">u</span></span></span></span><span style=\"top:-3.01344em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span class=\"accent-body\" style=\"left:-0.25em;\">^</span></span></span></span></span></span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.34480000000000005em;\"><span style=\"top:-2.5198em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\"><span class=\"mord mathdefault mtight\" style=\"margin-right:0.05724em;\">j</span><span class=\"mord mtight\">∣</span><span class=\"mord mathdefault mtight\">i</span></span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.3551999999999999em;\"><span></span></span></span></span></span></span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span><span class=\"mrel\">=</span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:0.969438em;vertical-align:-0.286108em;\"></span><span class=\"mord\"><span class=\"mord mathdefault\" style=\"margin-right:0.13889em;\">W</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.311664em;\"><span style=\"top:-2.5500000000000003em;margin-left:-0.13889em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\"><span class=\"mord mathdefault mtight\">i</span><span class=\"mord mathdefault mtight\" style=\"margin-right:0.05724em;\">j</span></span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.286108em;\"><span></span></span></span></span></span></span><span class=\"mord\"><span class=\"mord\"><span class=\"mord boldsymbol\">u</span></span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.311664em;\"><span style=\"top:-2.5500000000000003em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\"><span class=\"mord mathdefault mtight\">i</span><span class=\"mord mathdefault mtight\" style=\"margin-right:0.05724em;\">j</span></span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.286108em;\"><span></span></span></span></span></span></span></span></span></span></span>\n<p>where <span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><msub><mi mathvariant=\"bold-italic\">u</mi><mrow><mi>i</mi><mi>j</mi></mrow></msub></mrow><annotation encoding=\"application/x-tex\">\\boldsymbol u_{ij}</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.730548em;vertical-align:-0.286108em;\"></span><span class=\"mord\"><span class=\"mord\"><span class=\"mord boldsymbol\">u</span></span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.311664em;\"><span style=\"top:-2.5500000000000003em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\"><span class=\"mord mathdefault mtight\">i</span><span class=\"mord mathdefault mtight\" style=\"margin-right:0.05724em;\">j</span></span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.286108em;\"><span></span></span></span></span></span></span></span></span></span> is the activation vector of capsule <span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mi>j</mi></mrow><annotation encoding=\"application/x-tex\">j</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.85396em;vertical-align:-0.19444em;\"></span><span class=\"mord mathdefault\" style=\"margin-right:0.05724em;\">j</span></span></span></span> as ‘predicted’ by capsule <span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mi>i</mi></mrow><annotation encoding=\"application/x-tex\">i</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.65952em;vertical-align:0em;\"></span><span class=\"mord mathdefault\">i</span></span></span></span> through the matrix-vector multiplication\n<span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><msub><mi>W</mi><mrow><mi>i</mi><mi>j</mi></mrow></msub><msub><mi mathvariant=\"bold-italic\">u</mi><mi>i</mi></msub></mrow><annotation encoding=\"application/x-tex\">W_{ij} \\boldsymbol u_i</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.969438em;vertical-align:-0.286108em;\"></span><span class=\"mord\"><span class=\"mord mathdefault\" style=\"margin-right:0.13889em;\">W</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.311664em;\"><span style=\"top:-2.5500000000000003em;margin-left:-0.13889em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\"><span class=\"mord mathdefault mtight\">i</span><span class=\"mord mathdefault mtight\" style=\"margin-right:0.05724em;\">j</span></span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.286108em;\"><span></span></span></span></span></span></span><span class=\"mord\"><span class=\"mord\"><span class=\"mord boldsymbol\">u</span></span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.31166399999999994em;\"><span style=\"top:-2.5500000000000003em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mathdefault mtight\">i</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span></span></span></span>. The matrix <span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><msub><mi>W</mi><mrow><mi>i</mi><mi>j</mi></mrow></msub></mrow><annotation encoding=\"application/x-tex\">W_{ij}</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.969438em;vertical-align:-0.286108em;\"></span><span class=\"mord\"><span class=\"mord mathdefault\" style=\"margin-right:0.13889em;\">W</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.311664em;\"><span style=\"top:-2.5500000000000003em;margin-left:-0.13889em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\"><span class=\"mord mathdefault mtight\">i</span><span class=\"mord mathdefault mtight\" style=\"margin-right:0.05724em;\">j</span></span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.286108em;\"><span></span></span></span></span></span></span></span></span></span> is of shape <code class=\"language-text\">[out_dim, in_dim]</code> and the vector <span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><msub><mi mathvariant=\"bold-italic\">u</mi><mi>i</mi></msub></mrow><annotation encoding=\"application/x-tex\">\\boldsymbol u_i</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.59444em;vertical-align:-0.15em;\"></span><span class=\"mord\"><span class=\"mord\"><span class=\"mord boldsymbol\">u</span></span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.31166399999999994em;\"><span style=\"top:-2.5500000000000003em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mathdefault mtight\">i</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span></span></span></span> denotes the output vector of capsule <span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mi>i</mi></mrow><annotation encoding=\"application/x-tex\">i</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.65952em;vertical-align:0em;\"></span><span class=\"mord mathdefault\">i</span></span></span></span>. A single capsule layer computes this for all pairs <span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mi>i</mi><mo separator=\"true\">,</mo><mi>j</mi></mrow><annotation encoding=\"application/x-tex\">i,j</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.85396em;vertical-align:-0.19444em;\"></span><span class=\"mord mathdefault\">i</span><span class=\"mpunct\">,</span><span class=\"mspace\" style=\"margin-right:0.16666666666666666em;\"></span><span class=\"mord mathdefault\" style=\"margin-right:0.05724em;\">j</span></span></span></span> and for all samples in a batch. Hence, the tensors <code class=\"language-text\">W_ij</code> and <code class=\"language-text\">u_i</code> are actually of shape <code class=\"language-text\">[batch_size, in_caps, in_dim]</code> and <code class=\"language-text\">[in_caps, out_caps, out_dim, in_dim]</code> respectively. This means that we will build an op that just takes in these two tensors and computes an output tensor <code class=\"language-text\">u_hat_ji</code> of shape <code class=\"language-text\">[batch_size, in_caps, out_caps, out_dim]</code>. In other words, for all batch indices <code class=\"language-text\">[0,1,...,batch_size-1]</code> and for all combinations of in capsules <code class=\"language-text\">[0,1,...,in_caps-1]</code> and out capsules <code class=\"language-text\">[0,1,...,out_caps-1]</code> we have to compute a matrix-vector product.</p>\n<h2>TensorFlow kernel implementation</h2>\n<p>Our custom Op will be most valuable if we implement it for a GPU. In the <a href=\"https://www.tensorflow.org/extend/adding_an_op\">TensorFlow documentation</a>, you can find the necessary material to get you started on your own C++ kernels for TensorFlow. Then, you can read up on how to empower your algorithms with massively parallel GPU capabilities in <a href=\"https://developer.nvidia.com/cuda-example\">this book</a> or <a href=\"https://devblogs.nvidia.com/even-easier-introduction-cuda/\">online</a>. I will not repeat the details that you can find there, but I will provide a practical example that hopefully helps to understand how you can use CUDA for your own TensorFlow Ops. For me, getting to know some CUDA was easier than I thought, but squeezing out all performance can be tricky and I will leave further optimization of the kernel in this post for future work.</p>\n<h2>Op registration</h2>\n<p>Let’s do the forward pass of the Op. First, we will register the Op:</p>\n<div class=\"gatsby-highlight\" data-language=\"cpp\"><pre class=\"language-cpp\"><code class=\"language-cpp\"><span class=\"token function\">REGISTER_OP</span><span class=\"token punctuation\">(</span><span class=\"token string\">\"CapsulePrediction\"</span><span class=\"token punctuation\">)</span>\n<span class=\"token punctuation\">.</span><span class=\"token function\">Input</span><span class=\"token punctuation\">(</span><span class=\"token string\">\"input: T\"</span><span class=\"token punctuation\">)</span>\n<span class=\"token punctuation\">.</span><span class=\"token function\">Input</span><span class=\"token punctuation\">(</span><span class=\"token string\">\"weights: T\"</span><span class=\"token punctuation\">)</span>\n<span class=\"token punctuation\">.</span><span class=\"token function\">Output</span><span class=\"token punctuation\">(</span><span class=\"token string\">\"output: T\"</span><span class=\"token punctuation\">)</span>\n<span class=\"token punctuation\">.</span><span class=\"token function\">Attr</span><span class=\"token punctuation\">(</span><span class=\"token string\">\"T: type\"</span><span class=\"token punctuation\">)</span>\n<span class=\"token punctuation\">.</span><span class=\"token function\">SetShapeFn</span><span class=\"token punctuation\">(</span><span class=\"token punctuation\">[</span><span class=\"token punctuation\">]</span><span class=\"token punctuation\">(</span>InferenceContext<span class=\"token operator\">*</span> ctx<span class=\"token punctuation\">)</span> <span class=\"token punctuation\">{</span>\n    <span class=\"token comment\">// Get shapes and ensure correct dimensionality</span>\n    ShapeHandle in_shape<span class=\"token punctuation\">;</span>\n    ShapeHandle weights_shape<span class=\"token punctuation\">;</span>\n    <span class=\"token function\">TF_RETURN_IF_ERROR</span><span class=\"token punctuation\">(</span>ctx<span class=\"token operator\">-></span><span class=\"token function\">WithRank</span><span class=\"token punctuation\">(</span>ctx<span class=\"token operator\">-></span><span class=\"token function\">input</span><span class=\"token punctuation\">(</span><span class=\"token number\">0</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">,</span> <span class=\"token number\">3</span><span class=\"token punctuation\">,</span> <span class=\"token operator\">&amp;</span>in_shape<span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">;</span>\n    <span class=\"token function\">TF_RETURN_IF_ERROR</span><span class=\"token punctuation\">(</span>ctx<span class=\"token operator\">-></span><span class=\"token function\">WithRank</span><span class=\"token punctuation\">(</span>ctx<span class=\"token operator\">-></span><span class=\"token function\">input</span><span class=\"token punctuation\">(</span><span class=\"token number\">1</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">,</span> <span class=\"token number\">4</span><span class=\"token punctuation\">,</span> <span class=\"token operator\">&amp;</span>weights_shape<span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">;</span>\n\n    <span class=\"token comment\">// Construct and set the output shape</span>\n    DimensionHandle out_d0<span class=\"token punctuation\">,</span> out_d1<span class=\"token punctuation\">,</span> out_d2<span class=\"token punctuation\">,</span> out_d3<span class=\"token punctuation\">;</span>\n    std<span class=\"token operator\">::</span>vector<span class=\"token operator\">&lt;</span>DimensionHandle<span class=\"token operator\">></span> out_dims<span class=\"token punctuation\">;</span>\n    out_dims<span class=\"token punctuation\">.</span><span class=\"token function\">push_back</span><span class=\"token punctuation\">(</span>ctx<span class=\"token operator\">-></span><span class=\"token function\">MakeDim</span><span class=\"token punctuation\">(</span>ctx<span class=\"token operator\">-></span><span class=\"token function\">Dim</span><span class=\"token punctuation\">(</span>ctx<span class=\"token operator\">-></span><span class=\"token function\">input</span><span class=\"token punctuation\">(</span><span class=\"token number\">0</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">,</span> <span class=\"token number\">0</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">;</span>\n    out_dims<span class=\"token punctuation\">.</span><span class=\"token function\">push_back</span><span class=\"token punctuation\">(</span>ctx<span class=\"token operator\">-></span><span class=\"token function\">MakeDim</span><span class=\"token punctuation\">(</span>ctx<span class=\"token operator\">-></span><span class=\"token function\">Dim</span><span class=\"token punctuation\">(</span>ctx<span class=\"token operator\">-></span><span class=\"token function\">input</span><span class=\"token punctuation\">(</span><span class=\"token number\">1</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">,</span> <span class=\"token number\">0</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">;</span>\n    out_dims<span class=\"token punctuation\">.</span><span class=\"token function\">push_back</span><span class=\"token punctuation\">(</span>ctx<span class=\"token operator\">-></span><span class=\"token function\">MakeDim</span><span class=\"token punctuation\">(</span>ctx<span class=\"token operator\">-></span><span class=\"token function\">Dim</span><span class=\"token punctuation\">(</span>ctx<span class=\"token operator\">-></span><span class=\"token function\">input</span><span class=\"token punctuation\">(</span><span class=\"token number\">1</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">,</span> <span class=\"token number\">1</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">;</span>\n    out_dims<span class=\"token punctuation\">.</span><span class=\"token function\">push_back</span><span class=\"token punctuation\">(</span>ctx<span class=\"token operator\">-></span><span class=\"token function\">MakeDim</span><span class=\"token punctuation\">(</span>ctx<span class=\"token operator\">-></span><span class=\"token function\">Dim</span><span class=\"token punctuation\">(</span>ctx<span class=\"token operator\">-></span><span class=\"token function\">input</span><span class=\"token punctuation\">(</span><span class=\"token number\">1</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">,</span> <span class=\"token number\">2</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">;</span>\n    ShapeHandle out_shape <span class=\"token operator\">=</span> ctx<span class=\"token operator\">-></span><span class=\"token function\">MakeShape</span><span class=\"token punctuation\">(</span>out_dims<span class=\"token punctuation\">)</span><span class=\"token punctuation\">;</span>\n    ctx<span class=\"token operator\">-></span><span class=\"token function\">set_output</span><span class=\"token punctuation\">(</span><span class=\"token number\">0</span><span class=\"token punctuation\">,</span> out_shape<span class=\"token punctuation\">)</span><span class=\"token punctuation\">;</span>\n\n    <span class=\"token keyword\">return</span> Status<span class=\"token operator\">::</span><span class=\"token function\">OK</span><span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">;</span>\n<span class=\"token punctuation\">}</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">;</span></code></pre></div>\n<p>For now, I have defined this so that we could later specify different TensorFlow kernels for different dtypes by adding the <code class=\"language-text\">&quot;..: T&quot;</code> specification. Some of the classes that you see here such as <code class=\"language-text\">ShapeHandle</code>, <code class=\"language-text\">DimensionHandle</code> and <code class=\"language-text\">InferenceContext</code> are defined in the <code class=\"language-text\">tensorflow</code> namespace. The code shows a <em>shape function</em> that is implemented as a lambda function which first ensures <code class=\"language-text\">ctx-&gt;input(0)</code> (the input <span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><msub><mi mathvariant=\"bold-italic\">u</mi><mi>i</mi></msub></mrow><annotation encoding=\"application/x-tex\">\\boldsymbol u_i</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.59444em;vertical-align:-0.15em;\"></span><span class=\"mord\"><span class=\"mord\"><span class=\"mord boldsymbol\">u</span></span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.31166399999999994em;\"><span style=\"top:-2.5500000000000003em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mathdefault mtight\">i</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span></span></span></span>) and <code class=\"language-text\">ctx-&gt;input(1)</code> (the weights <span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><msub><mi>W</mi><mrow><mi>i</mi><mi>j</mi></mrow></msub></mrow><annotation encoding=\"application/x-tex\">W_{ij}</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.969438em;vertical-align:-0.286108em;\"></span><span class=\"mord\"><span class=\"mord mathdefault\" style=\"margin-right:0.13889em;\">W</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.311664em;\"><span style=\"top:-2.5500000000000003em;margin-left:-0.13889em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\"><span class=\"mord mathdefault mtight\">i</span><span class=\"mord mathdefault mtight\" style=\"margin-right:0.05724em;\">j</span></span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.286108em;\"><span></span></span></span></span></span></span></span></span></span>) have the correct rank. Then, we determine the dimensions of the output tensor which we can obtain from the input tensors. The dimension of the Op’s output is <code class=\"language-text\">[batch_size, in_caps, out_caps, out_dim]</code>, so we take batch<em>size and in</em>caps from the <span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><msub><mi mathvariant=\"bold-italic\">u</mi><mi>i</mi></msub></mrow><annotation encoding=\"application/x-tex\">\\boldsymbol u_i</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.59444em;vertical-align:-0.15em;\"></span><span class=\"mord\"><span class=\"mord\"><span class=\"mord boldsymbol\">u</span></span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.31166399999999994em;\"><span style=\"top:-2.5500000000000003em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mathdefault mtight\">i</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span></span></span></span> tensor and out<em>caps and out</em>dim from the $W_{ij} tensor.</p>\n<h2>Forward Capsule Prediction</h2>\n<p>Now, let’s look at the Op’s kernel. The word ‘kernel’ is TensorFlow terminology for the device-specific implementation of an Op. When defining a custom kernel, it should inherit from TensorFlow’s <code class=\"language-text\">OpKernel</code> and it shall implement the <code class=\"language-text\">Compute</code> method:</p>\n<div class=\"gatsby-highlight\" data-language=\"cpp\"><pre class=\"language-cpp\"><code class=\"language-cpp\"><span class=\"token keyword\">class</span> <span class=\"token class-name\">CapsulePredictionOp</span> <span class=\"token operator\">:</span> <span class=\"token keyword\">public</span> OpKernel\n<span class=\"token punctuation\">{</span>\n <span class=\"token keyword\">public</span><span class=\"token operator\">:</span>\n  <span class=\"token keyword\">explicit</span> <span class=\"token function\">CapsulePredictionOp</span><span class=\"token punctuation\">(</span>OpKernelConstruction<span class=\"token operator\">*</span> ctx<span class=\"token punctuation\">)</span> <span class=\"token operator\">:</span> <span class=\"token function\">OpKernel</span><span class=\"token punctuation\">(</span>ctx<span class=\"token punctuation\">)</span> <span class=\"token punctuation\">{</span> <span class=\"token punctuation\">}</span>\n\n  <span class=\"token keyword\">void</span> <span class=\"token function\">Compute</span><span class=\"token punctuation\">(</span>OpKernelContext<span class=\"token operator\">*</span> ctx<span class=\"token punctuation\">)</span> override\n  <span class=\"token punctuation\">{</span>\n    <span class=\"token comment\">// Get inputs</span>\n    <span class=\"token keyword\">const</span> Tensor<span class=\"token operator\">&amp;</span> input <span class=\"token operator\">=</span> ctx<span class=\"token operator\">-></span><span class=\"token function\">input</span><span class=\"token punctuation\">(</span><span class=\"token number\">0</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">;</span>\n    <span class=\"token keyword\">const</span> Tensor<span class=\"token operator\">&amp;</span> weights <span class=\"token operator\">=</span> ctx<span class=\"token operator\">-></span><span class=\"token function\">input</span><span class=\"token punctuation\">(</span><span class=\"token number\">1</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">;</span>\n\n    <span class=\"token comment\">// Setup output shape</span>\n    <span class=\"token keyword\">const</span> TensorShape<span class=\"token operator\">&amp;</span> <span class=\"token function\">input_shape</span><span class=\"token punctuation\">(</span>input<span class=\"token punctuation\">.</span><span class=\"token function\">shape</span><span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">;</span>\n    TensorShape <span class=\"token function\">output_shape</span><span class=\"token punctuation\">(</span>weights<span class=\"token punctuation\">.</span><span class=\"token function\">shape</span><span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">;</span>\n    output_shape<span class=\"token punctuation\">.</span><span class=\"token function\">InsertDim</span><span class=\"token punctuation\">(</span><span class=\"token number\">0</span><span class=\"token punctuation\">,</span> input_shape<span class=\"token punctuation\">.</span><span class=\"token function\">dim_size</span><span class=\"token punctuation\">(</span><span class=\"token number\">0</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">;</span>\n    output_shape<span class=\"token punctuation\">.</span><span class=\"token function\">RemoveDim</span><span class=\"token punctuation\">(</span><span class=\"token number\">4</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">;</span>\n\n    <span class=\"token comment\">// Allocate output tensor</span>\n    Tensor<span class=\"token operator\">*</span> output <span class=\"token operator\">=</span> <span class=\"token keyword\">nullptr</span><span class=\"token punctuation\">;</span>\n    <span class=\"token function\">OP_REQUIRES_OK</span><span class=\"token punctuation\">(</span>ctx<span class=\"token punctuation\">,</span> ctx<span class=\"token operator\">-></span><span class=\"token function\">allocate_output</span><span class=\"token punctuation\">(</span><span class=\"token number\">0</span><span class=\"token punctuation\">,</span> output_shape<span class=\"token punctuation\">,</span> <span class=\"token operator\">&amp;</span>output<span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">;</span>\n\n    <span class=\"token comment\">// Get the Eigen tensors and pass them on the launcher</span>\n    <span class=\"token keyword\">auto</span> input_tensor   <span class=\"token operator\">=</span> input<span class=\"token punctuation\">.</span>tensor<span class=\"token operator\">&lt;</span><span class=\"token keyword\">float</span><span class=\"token punctuation\">,</span> <span class=\"token number\">3</span><span class=\"token operator\">></span><span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">;</span>\n    <span class=\"token keyword\">auto</span> weights_tensor <span class=\"token operator\">=</span> weights<span class=\"token punctuation\">.</span>tensor<span class=\"token operator\">&lt;</span><span class=\"token keyword\">float</span><span class=\"token punctuation\">,</span> <span class=\"token number\">4</span><span class=\"token operator\">></span><span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">;</span>\n    <span class=\"token keyword\">auto</span> output_tensor  <span class=\"token operator\">=</span> output<span class=\"token operator\">-></span>tensor<span class=\"token operator\">&lt;</span><span class=\"token keyword\">float</span><span class=\"token punctuation\">,</span> <span class=\"token number\">4</span><span class=\"token operator\">></span><span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">;</span>\n    <span class=\"token function\">launchCapsulePrediction</span><span class=\"token punctuation\">(</span>ctx<span class=\"token operator\">-></span><span class=\"token function\">eigen_device</span><span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">,</span> input_tensor<span class=\"token punctuation\">,</span> weights_tensor<span class=\"token punctuation\">,</span>\n      output_tensor<span class=\"token punctuation\">)</span><span class=\"token punctuation\">;</span>\n  <span class=\"token punctuation\">}</span>\n<span class=\"token punctuation\">}</span><span class=\"token punctuation\">;</span></code></pre></div>\n<p>In the implementation above we haven’t done anything with CUDA yet, but we’ll get there so don’t worry. The code merely initializes the output shape from the input shapes and allocates the memory. The <code class=\"language-text\">OpKernelContext</code> object that is provided as a parameter makes sure to allocate the memory on the currently used device. In our case, this will be the GPU. Then, we obtain the <code class=\"language-text\">Eigen</code> tensors through the <code class=\"language-text\">tensor</code> method and pass them on to our <code class=\"language-text\">launchCapsulePrediction</code> function, where the actual magic happens.</p>\n<h2>Launching the kernel</h2>\n<p>Our <code class=\"language-text\">launchCapsulePrediction</code> function literally (at least in CUDA terminology) launches code on the GPU. Perhaps a little confusing, but CUDA refers to functions that run code on the ‘device’ as <em>kernels</em>. In TensorFlow terminology, a kernel is not necessarily a GPU implementation, while in CUDA terminology it is. Let’s not get too wrapped up in terminology and just get to the code:</p>\n<div class=\"gatsby-highlight\" data-language=\"cpp\"><pre class=\"language-cpp\"><code class=\"language-cpp\"><span class=\"token keyword\">void</span> <span class=\"token function\">launchCapsulePrediction</span><span class=\"token punctuation\">(</span>\n  <span class=\"token keyword\">const</span> GPUDevice<span class=\"token operator\">&amp;</span> d<span class=\"token punctuation\">,</span>\n  <span class=\"token keyword\">typename</span> TTypes<span class=\"token operator\">&lt;</span><span class=\"token keyword\">float</span><span class=\"token punctuation\">,</span> <span class=\"token number\">3</span><span class=\"token operator\">></span><span class=\"token operator\">::</span>ConstTensor x<span class=\"token punctuation\">,</span>\n  <span class=\"token keyword\">typename</span> TTypes<span class=\"token operator\">&lt;</span><span class=\"token keyword\">float</span><span class=\"token punctuation\">,</span> <span class=\"token number\">4</span><span class=\"token operator\">></span><span class=\"token operator\">::</span>ConstTensor weights<span class=\"token punctuation\">,</span>\n  <span class=\"token keyword\">typename</span> TTypes<span class=\"token operator\">&lt;</span><span class=\"token keyword\">float</span><span class=\"token punctuation\">,</span> <span class=\"token number\">4</span><span class=\"token operator\">></span><span class=\"token operator\">::</span>Tensor out<span class=\"token punctuation\">)</span>\n<span class=\"token punctuation\">{</span>\n  <span class=\"token comment\">// Get the dimensions</span>\n  <span class=\"token keyword\">const</span> int64 batch_size  <span class=\"token operator\">=</span> x<span class=\"token punctuation\">.</span><span class=\"token function\">dimension</span><span class=\"token punctuation\">(</span><span class=\"token number\">0</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">;</span>\n  <span class=\"token keyword\">const</span> int64 in_caps     <span class=\"token operator\">=</span> x<span class=\"token punctuation\">.</span><span class=\"token function\">dimension</span><span class=\"token punctuation\">(</span><span class=\"token number\">1</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">;</span>\n  <span class=\"token keyword\">const</span> int64 in_dim      <span class=\"token operator\">=</span> x<span class=\"token punctuation\">.</span><span class=\"token function\">dimension</span><span class=\"token punctuation\">(</span><span class=\"token number\">2</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">;</span>\n  <span class=\"token keyword\">const</span> int64 out_dim     <span class=\"token operator\">=</span> weights<span class=\"token punctuation\">.</span><span class=\"token function\">dimension</span><span class=\"token punctuation\">(</span><span class=\"token number\">2</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">;</span>\n  <span class=\"token keyword\">const</span> int64 out_caps    <span class=\"token operator\">=</span> weights<span class=\"token punctuation\">.</span><span class=\"token function\">dimension</span><span class=\"token punctuation\">(</span><span class=\"token number\">1</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">;</span>\n\n  <span class=\"token comment\">// Size first dim</span>\n  <span class=\"token keyword\">const</span> int64 w_d0 <span class=\"token operator\">=</span> out_caps <span class=\"token operator\">*</span> out_dim <span class=\"token operator\">*</span> in_dim<span class=\"token punctuation\">;</span>\n  <span class=\"token keyword\">const</span> int64 x_d0 <span class=\"token operator\">=</span> in_caps <span class=\"token operator\">*</span> in_dim<span class=\"token punctuation\">;</span>\n  <span class=\"token keyword\">const</span> int64 o_d0 <span class=\"token operator\">=</span> in_caps <span class=\"token operator\">*</span> out_caps <span class=\"token operator\">*</span> out_dim<span class=\"token punctuation\">;</span>\n\n  <span class=\"token comment\">// Second dim</span>\n  <span class=\"token keyword\">const</span> int64 w_d1 <span class=\"token operator\">=</span> out_dim <span class=\"token operator\">*</span> in_dim<span class=\"token punctuation\">;</span>\n  <span class=\"token keyword\">const</span> int64 x_d1 <span class=\"token operator\">=</span> in_dim<span class=\"token punctuation\">;</span>\n  <span class=\"token keyword\">const</span> int64 o_d1 <span class=\"token operator\">=</span> out_caps <span class=\"token operator\">*</span> out_dim<span class=\"token punctuation\">;</span>\n\n  <span class=\"token comment\">// Third dim</span>\n  <span class=\"token keyword\">const</span> int64 w_d2 <span class=\"token operator\">=</span> in_dim<span class=\"token punctuation\">;</span>\n  <span class=\"token keyword\">const</span> int64 o_d2 <span class=\"token operator\">=</span> out_dim<span class=\"token punctuation\">;</span>\n\n  <span class=\"token comment\">// Launch CUDA kernel for forward operation</span>\n  CudaLaunchConfig config <span class=\"token operator\">=</span> <span class=\"token function\">GetCudaLaunchConfig</span><span class=\"token punctuation\">(</span>out<span class=\"token punctuation\">.</span><span class=\"token function\">size</span><span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">,</span> d<span class=\"token punctuation\">)</span><span class=\"token punctuation\">;</span>\n  capsulePredictionKernel\n    <span class=\"token operator\">&lt;&lt;</span><span class=\"token operator\">&lt;</span>config<span class=\"token punctuation\">.</span>block_count<span class=\"token punctuation\">,</span> config<span class=\"token punctuation\">.</span>thread_per_block<span class=\"token punctuation\">,</span> <span class=\"token number\">0</span><span class=\"token punctuation\">,</span> d<span class=\"token punctuation\">.</span><span class=\"token function\">stream</span><span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span><span class=\"token operator\">>></span><span class=\"token operator\">></span><span class=\"token punctuation\">(</span>\n      x<span class=\"token punctuation\">.</span><span class=\"token function\">data</span><span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">,</span> weights<span class=\"token punctuation\">.</span><span class=\"token function\">data</span><span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">,</span> out<span class=\"token punctuation\">.</span><span class=\"token function\">data</span><span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">,</span>\n      o_d0<span class=\"token punctuation\">,</span> o_d1<span class=\"token punctuation\">,</span> o_d2<span class=\"token punctuation\">,</span> x_d0<span class=\"token punctuation\">,</span> x_d1<span class=\"token punctuation\">,</span> w_d0<span class=\"token punctuation\">,</span> w_d1<span class=\"token punctuation\">,</span> w_d2<span class=\"token punctuation\">,</span>\n      in_dim<span class=\"token punctuation\">,</span> out<span class=\"token punctuation\">.</span><span class=\"token function\">size</span><span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">;</span>\n<span class=\"token punctuation\">}</span></code></pre></div>\n<p>The <code class=\"language-text\">TTypes</code> templates that you can see in the function arguments and the <code class=\"language-text\">int64</code> types are defined in the <code class=\"language-text\">tensorflow</code> namespace. The next part about the dimensions should be pretty self-explanatory. Because we are passing our tensor data as one-dimensional arrays to the actual CUDA kernel, we need to figure out what the memory sizes are for each dimension and each kernel. Note that when I say ‘memory sizes’, I just refer to the number of floats for each axis and not the byte size. Let’s consider the memory sizes of the first axis of each tensor:</p>\n<div class=\"gatsby-highlight\" data-language=\"cpp\"><pre class=\"language-cpp\"><code class=\"language-cpp\"><span class=\"token comment\">// Size first dim</span>\n<span class=\"token keyword\">const</span> int64 w_d0 <span class=\"token operator\">=</span> out_caps <span class=\"token operator\">*</span> out_dim <span class=\"token operator\">*</span> in_dim<span class=\"token punctuation\">;</span>\n<span class=\"token keyword\">const</span> int64 x_d0 <span class=\"token operator\">=</span> in_caps <span class=\"token operator\">*</span> in_dim<span class=\"token punctuation\">;</span>\n<span class=\"token keyword\">const</span> int64 o_d0 <span class=\"token operator\">=</span> in_caps <span class=\"token operator\">*</span> out_caps <span class=\"token operator\">*</span> out_dim<span class=\"token punctuation\">;</span></code></pre></div>\n<p>Awesome, so we can simply get these using the dimensions we determined already. The code tells us that <code class=\"language-text\">w_d0</code> is just the product of <code class=\"language-text\">out_caps</code>, <code class=\"language-text\">out_dim</code> and <code class=\"language-text\">in_dim</code>. So if we want to jump from one index <span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><msub><mi>W</mi><mrow><mi>i</mi><mo separator=\"true\">,</mo><mi>j</mi><mo separator=\"true\">,</mo><mi>k</mi><mo separator=\"true\">,</mo><mi>l</mi></mrow></msub></mrow><annotation encoding=\"application/x-tex\">W_{i,j,k,l}</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.969438em;vertical-align:-0.286108em;\"></span><span class=\"mord\"><span class=\"mord mathdefault\" style=\"margin-right:0.13889em;\">W</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.3361079999999999em;\"><span style=\"top:-2.5500000000000003em;margin-left:-0.13889em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\"><span class=\"mord mathdefault mtight\">i</span><span class=\"mpunct mtight\">,</span><span class=\"mord mathdefault mtight\" style=\"margin-right:0.05724em;\">j</span><span class=\"mpunct mtight\">,</span><span class=\"mord mathdefault mtight\" style=\"margin-right:0.03148em;\">k</span><span class=\"mpunct mtight\">,</span><span class=\"mord mathdefault mtight\" style=\"margin-right:0.01968em;\">l</span></span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.286108em;\"><span></span></span></span></span></span></span></span></span></span> to <span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><msub><mi>W</mi><mrow><mi>i</mi><mo>+</mo><mn>1</mn><mo separator=\"true\">,</mo><mi>j</mi><mo separator=\"true\">,</mo><mi>k</mi><mo separator=\"true\">,</mo><mi>l</mi></mrow></msub></mrow><annotation encoding=\"application/x-tex\">W_{i+1,j,k,l}</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.969438em;vertical-align:-0.286108em;\"></span><span class=\"mord\"><span class=\"mord mathdefault\" style=\"margin-right:0.13889em;\">W</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.3361079999999999em;\"><span style=\"top:-2.5500000000000003em;margin-left:-0.13889em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\"><span class=\"mord mathdefault mtight\">i</span><span class=\"mbin mtight\">+</span><span class=\"mord mtight\">1</span><span class=\"mpunct mtight\">,</span><span class=\"mord mathdefault mtight\" style=\"margin-right:0.05724em;\">j</span><span class=\"mpunct mtight\">,</span><span class=\"mord mathdefault mtight\" style=\"margin-right:0.03148em;\">k</span><span class=\"mpunct mtight\">,</span><span class=\"mord mathdefault mtight\" style=\"margin-right:0.01968em;\">l</span></span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.286108em;\"><span></span></span></span></span></span></span></span></span></span> we should add <code class=\"language-text\">w_d0</code> to the one-dimensional index. The same goes for index  <span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mi>j</mi></mrow><annotation encoding=\"application/x-tex\">j</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.85396em;vertical-align:-0.19444em;\"></span><span class=\"mord mathdefault\" style=\"margin-right:0.05724em;\">j</span></span></span></span>\nand <code class=\"language-text\">w_d1</code> as you might already expect.</p>\n<p>The actual CUDA kernel launch is given at the bottom of the function and repeated here:</p>\n<div class=\"gatsby-highlight\" data-language=\"cpp\"><pre class=\"language-cpp\"><code class=\"language-cpp\"><span class=\"token comment\">// Launch CUDA kernel for forward operation</span>\nCudaLaunchConfig config <span class=\"token operator\">=</span> <span class=\"token function\">GetCudaLaunchConfig</span><span class=\"token punctuation\">(</span>out<span class=\"token punctuation\">.</span><span class=\"token function\">size</span><span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">,</span> d<span class=\"token punctuation\">)</span><span class=\"token punctuation\">;</span>\ncapsulePredictionKernel\n<span class=\"token operator\">&lt;&lt;</span><span class=\"token operator\">&lt;</span>config<span class=\"token punctuation\">.</span>block_count<span class=\"token punctuation\">,</span> config<span class=\"token punctuation\">.</span>thread_per_block<span class=\"token punctuation\">,</span> <span class=\"token number\">0</span><span class=\"token punctuation\">,</span> d<span class=\"token punctuation\">.</span><span class=\"token function\">stream</span><span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span><span class=\"token operator\">>></span><span class=\"token operator\">></span><span class=\"token punctuation\">(</span>\n  x<span class=\"token punctuation\">.</span><span class=\"token function\">data</span><span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">,</span> weights<span class=\"token punctuation\">.</span><span class=\"token function\">data</span><span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">,</span> out<span class=\"token punctuation\">.</span><span class=\"token function\">data</span><span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">,</span>\n  o_d0<span class=\"token punctuation\">,</span> o_d1<span class=\"token punctuation\">,</span> o_d2<span class=\"token punctuation\">,</span> x_d0<span class=\"token punctuation\">,</span> x_d1<span class=\"token punctuation\">,</span> w_d0<span class=\"token punctuation\">,</span> w_d1<span class=\"token punctuation\">,</span> w_d2<span class=\"token punctuation\">,</span>\n  in_dim<span class=\"token punctuation\">,</span> out<span class=\"token punctuation\">.</span><span class=\"token function\">size</span><span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">;</span></code></pre></div>\n<p>Both statements involve quite a few new concepts. The first statement uses a <code class=\"language-text\">GetCudaLaunchConfig</code> instance as a way to determine the number of blocks and the number of threads per block. It is provided in the TensorFlow header <code class=\"language-text\">tensorflow/core/util/cuda_kernel_helper.h</code>. You should definitely check out that file in case you are working on your own Op! The capsulePredictionKernel is the function that uses CUDA parallelism on the GPU. It is launched by using the triple-fold delimiters: <code class=\"language-text\">&lt;&lt;&lt;config.block_count, config.thread_per_block, 0, d.stream()&gt;&gt;&gt;</code>. When you launch a kernel, you must specify the number of blocks and threads per block, as is done here. The zero on the third position is not relevant for now and it should most likely be zero if you were to implement your own kernels. The CUDA stream <code class=\"language-text\">d.stream()</code> can be thought of as a pipeline of GPU instructions. Whenever you add your kernel to the stream, the stream will make sure the kernel ends before the next kernel on the stream is called. If you want to do two independent tasks in parallel, you could use two streams and launch one task on each.</p>\n<h2>Threads And Blocks</h2>\n<p>All <em>blocks</em> that are assigned to a call can be run in parallel. If you launch a kernel with <code class=\"language-text\">N</code> blocks, then you could think of it as running <code class=\"language-text\">N</code> separate instances of the kernel function. That’s pretty convenient! The <code class=\"language-text\">nvcc</code> compiler will make sure that the kernel function has access to the exact block index so that the specific block-instance of the kernel knows which parts of the incoming arrays it should process.</p>\n<p>A block can contain multiple <em>threads</em> itself. Threads are just an additional layer of parallelism, so they run in parallel. Why another layer of parallelism, you ask? Well, threads can do things that blocks cannot. Threads can share their memory which is typically useful when you want to use the same value of some input array in the same block multiple times. The shared memory access is much faster and it is one of the many ways you can optimize your final CUDA implementation. Here is a schematic illustration of the two parallel layers:\n<figure class=\"gatsby-resp-image-figure\" style=\"\">\n    <a\n    class=\"gatsby-resp-image-link\"\n    href=\"/static/6eda768c2efe2c3c6645a6228af64a81/ee1bb/blocks_and_threads.png\"\n    style=\"display: block\"\n    target=\"_blank\"\n    rel=\"noopener\"\n  >\n    <span\n    class=\"gatsby-resp-image-wrapper\"\n    style=\"position: relative; display: block; margin-left: auto; margin-right: auto;  max-width: 511px;\"\n  >\n    <span\n      class=\"gatsby-resp-image-background-image\"\n      style=\"padding-bottom: 35.0293542074364%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAHCAYAAAAIy204AAAACXBIWXMAAAsSAAALEgHS3X78AAABYUlEQVQoz62OXUvCYBiGx/5ORHQy9cU66yAKpJ9T9EFBQf2BDjqLRE2tKOggv8jNfbzbTDf8bHNOUZybGgSd+b5N04OgDoJuuOHhfp7r5iHsYJhywlHq8fhkM7q7F3iL3Xq6l0FqeB2n7FDEE98/CNwfHm2MIjGvFQwvEDP1r8JLAzdLnJ6th7Z3tpzYjbd9fkERBp9BLYlBGpPEei6FuwUO6VwKmRKNWmIW1ekEbgrP40FZxjqfgfPCBp/WLAVOmPGE6ykQldMPiCj2CrhklzBTYzCncdNZsYpY7StYtVRMV2kMDYiMDwO/dPPyrI/Md+SG/q5POfaVxRWngmGHwwTfyAKxyYLqUAUluzCdBYN2zQCpxYHaSAVKT/a5ud/NF+cfutwybOb8lYHiKzsFIJs8yNafAPFX3cVTxOrayu8HQpMhpZbgmp9ZIKGZI0WT/TH/zn3tRNf5tkjSWpIk/luf8UEEtjWaYJ4AAAAASUVORK5CYII='); background-size: cover; display: block;\"\n    ></span>\n    <img\n        class=\"gatsby-resp-image-image\"\n        style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;box-shadow:inset 0px 0px 0px 400px white;\"\n        alt=\"<i><center>Blocks and threads for parallel computing on a CUDA device.</center></i>\"\n        title=\"\"\n        src=\"/static/6eda768c2efe2c3c6645a6228af64a81/ee1bb/blocks_and_threads.png\"\n        srcset=\"/static/6eda768c2efe2c3c6645a6228af64a81/26396/blocks_and_threads.png 200w,\n/static/6eda768c2efe2c3c6645a6228af64a81/b3879/blocks_and_threads.png 400w,\n/static/6eda768c2efe2c3c6645a6228af64a81/ee1bb/blocks_and_threads.png 511w\"\n        sizes=\"(max-width: 511px) 100vw, 511px\"\n      />\n  </span>\n  </a>\n    <figcaption class=\"gatsby-resp-image-figcaption\"><i><center>Blocks and threads for parallel computing on a CUDA device.</center></i></figcaption>\n  </figure></p>\n<h2>The CUDA kernel</h2>\n<p>After a quick recap on threads and blocks in CUDA, we finally get to see the CUDA implementation of the forward capsule prediction:</p>\n<div class=\"gatsby-highlight\" data-language=\"cpp\"><pre class=\"language-cpp\"><code class=\"language-cpp\">__global__ <span class=\"token keyword\">void</span> <span class=\"token function\">capsulePredictionKernel</span><span class=\"token punctuation\">(</span>\n    <span class=\"token keyword\">const</span> <span class=\"token keyword\">float</span><span class=\"token operator\">*</span> in<span class=\"token punctuation\">,</span> <span class=\"token keyword\">const</span> <span class=\"token keyword\">float</span><span class=\"token operator\">*</span> weights<span class=\"token punctuation\">,</span> <span class=\"token keyword\">float</span><span class=\"token operator\">*</span> out<span class=\"token punctuation\">,</span>\n    <span class=\"token keyword\">const</span> int64 o_d0<span class=\"token punctuation\">,</span> <span class=\"token keyword\">const</span> int64 o_d1<span class=\"token punctuation\">,</span> <span class=\"token keyword\">const</span> int64 o_d2<span class=\"token punctuation\">,</span>\n    <span class=\"token keyword\">const</span> int64 x_d0<span class=\"token punctuation\">,</span> <span class=\"token keyword\">const</span> int64 x_d1<span class=\"token punctuation\">,</span>\n    <span class=\"token keyword\">const</span> int64 w_d0<span class=\"token punctuation\">,</span> <span class=\"token keyword\">const</span> int64 w_d1<span class=\"token punctuation\">,</span> <span class=\"token keyword\">const</span> int64 w_d2<span class=\"token punctuation\">,</span>\n    <span class=\"token keyword\">const</span> int64 in_dim<span class=\"token punctuation\">,</span> <span class=\"token keyword\">const</span> int64 output_size<span class=\"token punctuation\">)</span>\n<span class=\"token punctuation\">{</span>\n  <span class=\"token function\">CUDA_1D_KERNEL_LOOP</span><span class=\"token punctuation\">(</span>i<span class=\"token punctuation\">,</span> output_size<span class=\"token punctuation\">)</span>\n  <span class=\"token punctuation\">{</span>\n    <span class=\"token comment\">// So here we have out[b,ci,cj,e]</span>\n    <span class=\"token keyword\">const</span> int64 b     <span class=\"token operator\">=</span> i <span class=\"token operator\">/</span> o_d0<span class=\"token punctuation\">;</span>\n    <span class=\"token keyword\">const</span> int64 ci    <span class=\"token operator\">=</span> <span class=\"token punctuation\">(</span>i <span class=\"token operator\">%</span> o_d0<span class=\"token punctuation\">)</span> <span class=\"token operator\">/</span> o_d1<span class=\"token punctuation\">;</span>\n    <span class=\"token keyword\">const</span> int64 cj    <span class=\"token operator\">=</span> <span class=\"token punctuation\">(</span>i <span class=\"token operator\">%</span> o_d1<span class=\"token punctuation\">)</span> <span class=\"token operator\">/</span> o_d2<span class=\"token punctuation\">;</span>\n    <span class=\"token keyword\">const</span> int64 e_out  <span class=\"token operator\">=</span> i <span class=\"token operator\">%</span> o_d2<span class=\"token punctuation\">;</span>\n\n    <span class=\"token comment\">// Then, we can have a look at computing the array indices for in and W</span>\n    int64 in_idx <span class=\"token operator\">=</span> b <span class=\"token operator\">*</span> x_d0 <span class=\"token operator\">+</span> ci <span class=\"token operator\">*</span> x_d1<span class=\"token punctuation\">;</span>\n    int64 w_idx <span class=\"token operator\">=</span> ci <span class=\"token operator\">*</span> w_d0 <span class=\"token operator\">+</span> cj <span class=\"token operator\">*</span> w_d1 <span class=\"token operator\">+</span> e_out <span class=\"token operator\">*</span> w_d2<span class=\"token punctuation\">;</span>\n\n    <span class=\"token comment\">// Initialize result</span>\n    <span class=\"token keyword\">float</span> result <span class=\"token operator\">=</span> <span class=\"token number\">0.0</span><span class=\"token punctuation\">;</span>\n    <span class=\"token keyword\">for</span> <span class=\"token punctuation\">(</span>int64 v <span class=\"token operator\">=</span> <span class=\"token number\">0</span><span class=\"token punctuation\">;</span> v <span class=\"token operator\">&lt;</span> in_dim<span class=\"token punctuation\">;</span> <span class=\"token operator\">++</span>v<span class=\"token punctuation\">)</span>\n      <span class=\"token comment\">// For both in and weights, the subsequent elements of the forward</span>\n      <span class=\"token comment\">// computation are also subsequent in memory</span>\n      result <span class=\"token operator\">+=</span> <span class=\"token function\">ldg</span><span class=\"token punctuation\">(</span>in <span class=\"token operator\">+</span> in_idx<span class=\"token operator\">++</span><span class=\"token punctuation\">)</span> <span class=\"token operator\">*</span> <span class=\"token function\">ldg</span><span class=\"token punctuation\">(</span>weights <span class=\"token operator\">+</span> w_idx<span class=\"token operator\">++</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">;</span>\n    <span class=\"token comment\">// Write result</span>\n    out<span class=\"token punctuation\">[</span>i<span class=\"token punctuation\">]</span> <span class=\"token operator\">=</span> result<span class=\"token punctuation\">;</span>\n  <span class=\"token punctuation\">}</span>\n<span class=\"token punctuation\">}</span></code></pre></div>\n<p>The first thing you might notice is the <code class=\"language-text\">__global__</code> qualifier that precedes the function definition. This is what the <code class=\"language-text\">nvcc</code> compiler uses to make the function available globally, meaning that it can be launched from CPU, or ‘host’ code in CUDA terminology. The function’s arguments inherit their names from the launchCapsulePrediction function so they should not cause too much confusion. The <code class=\"language-text\">CUDA_1D_KERNEL_LOOP</code> is a macro defined in <code class=\"language-text\">tensorflow/core/util/cuda_kernel_helper.h</code>. It replaces this line of code with:</p>\n<div class=\"gatsby-highlight\" data-language=\"cpp\"><pre class=\"language-cpp\"><code class=\"language-cpp\"><span class=\"token keyword\">for</span> <span class=\"token punctuation\">(</span><span class=\"token keyword\">int</span> i <span class=\"token operator\">=</span> blockIdx<span class=\"token punctuation\">.</span>x <span class=\"token operator\">*</span> blockDim<span class=\"token punctuation\">.</span>x <span class=\"token operator\">+</span> threadIdx<span class=\"token punctuation\">.</span>x<span class=\"token punctuation\">;</span> i <span class=\"token operator\">&lt;</span> output_size<span class=\"token punctuation\">;</span>\n     i <span class=\"token operator\">+=</span> blockDim<span class=\"token punctuation\">.</span>x <span class=\"token operator\">*</span> gridDim<span class=\"token punctuation\">.</span>x<span class=\"token punctuation\">)</span></code></pre></div>\n<p>The CUDA kernel launch together with this TensorFlow macro forces us to think in an abstract yet convenient way: it gives us some index <code class=\"language-text\">i</code> that correspond to the <code class=\"language-text\">i</code>-th element of the output array out. Different block/thread instantiations of this kernel will get their own values for <code class=\"language-text\">i</code>. Now all we have to do is figure out what the indices are of our additional arrays <code class=\"language-text\">in</code> and <code class=\"language-text\">weights</code>. In order to do that, we determine the batch index <code class=\"language-text\">b</code>, the input capsule index <code class=\"language-text\">ci</code>, the output capsule index <code class=\"language-text\">cj</code> and the out capsule element index <code class=\"language-text\">e_out</code>:</p>\n<div class=\"gatsby-highlight\" data-language=\"cpp\"><pre class=\"language-cpp\"><code class=\"language-cpp\"><span class=\"token comment\">// So here we have out[b,ci,cj,e]</span>\n<span class=\"token keyword\">const</span> int64 b     <span class=\"token operator\">=</span> i <span class=\"token operator\">/</span> o_d0<span class=\"token punctuation\">;</span>\n<span class=\"token keyword\">const</span> int64 ci    <span class=\"token operator\">=</span> <span class=\"token punctuation\">(</span>i <span class=\"token operator\">%</span> o_d0<span class=\"token punctuation\">)</span> <span class=\"token operator\">/</span> o_d1<span class=\"token punctuation\">;</span>\n<span class=\"token keyword\">const</span> int64 cj    <span class=\"token operator\">=</span> <span class=\"token punctuation\">(</span>i <span class=\"token operator\">%</span> o_d1<span class=\"token punctuation\">)</span> <span class=\"token operator\">/</span> o_d2<span class=\"token punctuation\">;</span>\n<span class=\"token keyword\">const</span> int64 e_out <span class=\"token operator\">=</span> i <span class=\"token operator\">%</span> o_d2<span class=\"token punctuation\">;</span></code></pre></div>\n<p>Determining these becomes easy once we know the number of elements contained in each axis. In fact, we have given the memory sizes as arguments to the function. For the other arrays, we can then convert <code class=\"language-text\">b</code>, <code class=\"language-text\">ci</code>, <code class=\"language-text\">cj</code> and <code class=\"language-text\">e_out</code> to their respective one-dimensional indices:</p>\n<div class=\"gatsby-highlight\" data-language=\"cpp\"><pre class=\"language-cpp\"><code class=\"language-cpp\"><span class=\"token comment\">// Then, we can have a look at computing the array indices for in and W</span>\nint64 in_idx <span class=\"token operator\">=</span> b <span class=\"token operator\">*</span> x_d0 <span class=\"token operator\">+</span> ci <span class=\"token operator\">*</span> x_d1<span class=\"token punctuation\">;</span>\nint64 w_idx  <span class=\"token operator\">=</span> ci <span class=\"token operator\">*</span> w_d0 <span class=\"token operator\">+</span> cj <span class=\"token operator\">*</span> w_d1 <span class=\"token operator\">+</span> e_out <span class=\"token operator\">*</span> w_d2<span class=\"token punctuation\">;</span></code></pre></div>\n<p>Again, we use the already provided memory sizes for each of the axes to get our one-dimensional indices. These are the indices for the first input capsule element of (i) input capsule <code class=\"language-text\">ci</code> at batch index <code class=\"language-text\">b</code> and (ii) the weights of the input capsule <code class=\"language-text\">ci</code>, the output capsule <code class=\"language-text\">cj</code> and the output capsule element <code class=\"language-text\">e_out</code>. If you’re familiar with Matlab, then perhaps it helps to remind you of the <code class=\"language-text\">sub2ind</code> function that concerns the very same thing.</p>\n<p>We assume that the last axis of both <code class=\"language-text\">in</code> and <code class=\"language-text\">W</code> corresponds to the input capsule elements. This means that they are subsequent in memory and it is therefore straightforward to construct the loop that goes over the individual input capsule elements:</p>\n<div class=\"gatsby-highlight\" data-language=\"cpp\"><pre class=\"language-cpp\"><code class=\"language-cpp\"><span class=\"token comment\">// Initialize result</span>\n<span class=\"token keyword\">float</span> result <span class=\"token operator\">=</span> <span class=\"token number\">0.0</span><span class=\"token punctuation\">;</span>\n<span class=\"token keyword\">for</span> <span class=\"token punctuation\">(</span>int64 v <span class=\"token operator\">=</span> <span class=\"token number\">0</span><span class=\"token punctuation\">;</span> v <span class=\"token operator\">&lt;</span> in_dim<span class=\"token punctuation\">;</span> <span class=\"token operator\">++</span>v<span class=\"token punctuation\">)</span>\n  <span class=\"token comment\">// For both in and weights, the subsequent elements of the forward</span>\n  <span class=\"token comment\">// computation are also subsequent in memory</span>\n  result <span class=\"token operator\">+=</span> <span class=\"token function\">ldg</span><span class=\"token punctuation\">(</span>in <span class=\"token operator\">+</span> in_idx<span class=\"token operator\">++</span><span class=\"token punctuation\">)</span> <span class=\"token operator\">*</span> <span class=\"token function\">ldg</span><span class=\"token punctuation\">(</span>weights <span class=\"token operator\">+</span> w_idx<span class=\"token operator\">++</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">;</span>\n<span class=\"token comment\">// Write result</span>\nout<span class=\"token punctuation\">[</span>i<span class=\"token punctuation\">]</span> <span class=\"token operator\">=</span> result<span class=\"token punctuation\">;</span></code></pre></div>\n<p>The <code class=\"language-text\">ldg</code> function is a <a href=\"https://stackoverflow.com/questions/26603188/what-is-the-difference-between-ldg-intrinsic-and-a-normal-execution\">Read-Only Data Cache Load Function</a>. It just receives a pointer to the actual element to read. Remember that we are computing matrix-vector products, which are just sets of inner products. A potential improvement here is to use shared memory since a single input capsule value is used many times, but we will leave out further optimization for now.</p>\n<h2>Testing</h2>\n<p>I want this post to be an end-to-end showcase of the development of a TensorFlow custom Op for GPU. This includes <em>testing</em> the Op. Here’s the forward computation with <code class=\"language-text\">numpy</code>:</p>\n<div class=\"gatsby-highlight\" data-language=\"python\"><pre class=\"language-python\"><code class=\"language-python\"><span class=\"token keyword\">import</span> tensorflow <span class=\"token keyword\">as</span> tf\n<span class=\"token keyword\">from</span> ops<span class=\"token punctuation\">.</span>capsuleprediction <span class=\"token keyword\">import</span> capsule_prediction\n<span class=\"token keyword\">import</span> numpy <span class=\"token keyword\">as</span> np\n<span class=\"token keyword\">from</span> parameterized <span class=\"token keyword\">import</span> parameterized\n<span class=\"token keyword\">import</span> itertools\n\n\n<span class=\"token keyword\">class</span> <span class=\"token class-name\">CapsulePredictionOpTest</span><span class=\"token punctuation\">(</span>tf<span class=\"token punctuation\">.</span>test<span class=\"token punctuation\">.</span>TestCase<span class=\"token punctuation\">)</span><span class=\"token punctuation\">:</span>\n\n    @<span class=\"token builtin\">staticmethod</span>\n    <span class=\"token keyword\">def</span> <span class=\"token function\">_numpy_capsule_prediction</span><span class=\"token punctuation\">(</span>x<span class=\"token punctuation\">,</span> weights<span class=\"token punctuation\">)</span><span class=\"token punctuation\">:</span>\n        <span class=\"token triple-quoted-string string\">\"\"\" Generate the output for x and weights with numpy \"\"\"</span>\n        batch_size<span class=\"token punctuation\">,</span> in_caps<span class=\"token punctuation\">,</span> in_dim <span class=\"token operator\">=</span> x<span class=\"token punctuation\">.</span>shape\n        _<span class=\"token punctuation\">,</span> out_caps<span class=\"token punctuation\">,</span> out_dim<span class=\"token punctuation\">,</span> _ <span class=\"token operator\">=</span> weights<span class=\"token punctuation\">.</span>shape\n\n        out_shape <span class=\"token operator\">=</span> <span class=\"token punctuation\">(</span>batch_size<span class=\"token punctuation\">,</span> in_caps<span class=\"token punctuation\">,</span> out_caps<span class=\"token punctuation\">,</span> out_dim<span class=\"token punctuation\">)</span>\n        out <span class=\"token operator\">=</span> np<span class=\"token punctuation\">.</span>zeros<span class=\"token punctuation\">(</span>out_shape<span class=\"token punctuation\">)</span>\n\n        <span class=\"token keyword\">for</span> b <span class=\"token keyword\">in</span> <span class=\"token builtin\">range</span><span class=\"token punctuation\">(</span>batch_size<span class=\"token punctuation\">)</span><span class=\"token punctuation\">:</span>\n            <span class=\"token keyword\">for</span> i <span class=\"token keyword\">in</span> <span class=\"token builtin\">range</span><span class=\"token punctuation\">(</span>in_caps<span class=\"token punctuation\">)</span><span class=\"token punctuation\">:</span>\n                <span class=\"token keyword\">for</span> j <span class=\"token keyword\">in</span> <span class=\"token builtin\">range</span><span class=\"token punctuation\">(</span>out_caps<span class=\"token punctuation\">)</span><span class=\"token punctuation\">:</span>\n                    <span class=\"token keyword\">for</span> c <span class=\"token keyword\">in</span> <span class=\"token builtin\">range</span><span class=\"token punctuation\">(</span>out_dim<span class=\"token punctuation\">)</span><span class=\"token punctuation\">:</span>\n                        out<span class=\"token punctuation\">[</span>b<span class=\"token punctuation\">,</span> i<span class=\"token punctuation\">,</span> j<span class=\"token punctuation\">,</span> c<span class=\"token punctuation\">]</span> <span class=\"token operator\">=</span> np<span class=\"token punctuation\">.</span>dot<span class=\"token punctuation\">(</span>x<span class=\"token punctuation\">[</span>b<span class=\"token punctuation\">,</span> i<span class=\"token punctuation\">]</span><span class=\"token punctuation\">,</span> weights<span class=\"token punctuation\">[</span>i<span class=\"token punctuation\">,</span> j<span class=\"token punctuation\">,</span> c<span class=\"token punctuation\">]</span><span class=\"token punctuation\">)</span>\n        <span class=\"token keyword\">return</span> out</code></pre></div>\n<p>The file <code class=\"language-text\">ops/capsuleprediction.py</code> contains the capsule_prediction that actually loads the Op from the shared library file after being compiled. The function above should be straightforward to interpret: we loop over batch, in capsules, out capsules and out capsule elements and compute a dot product for each combination in the output. We’ll use this to verify the forward computation of the Op. Another thing to note is the <code class=\"language-text\">tf.test.TestCase</code> class which we inherit from. It provides some utility functions for testing with TensorFlow.</p>\n<p>Now let’s look at the test for the forward capsule prediction:</p>\n<div class=\"gatsby-highlight\" data-language=\"python\"><pre class=\"language-python\"><code class=\"language-python\"><span class=\"token decorator annotation punctuation\">@parameterized<span class=\"token punctuation\">.</span>expand</span><span class=\"token punctuation\">(</span><span class=\"token punctuation\">[</span>\n    <span class=\"token punctuation\">(</span>batch_size<span class=\"token punctuation\">,</span> in_caps<span class=\"token punctuation\">,</span> out_caps<span class=\"token punctuation\">,</span> in_dim<span class=\"token punctuation\">,</span> out_dim<span class=\"token punctuation\">)</span> <span class=\"token keyword\">for</span>\n    batch_size<span class=\"token punctuation\">,</span> in_caps<span class=\"token punctuation\">,</span> out_caps<span class=\"token punctuation\">,</span> in_dim<span class=\"token punctuation\">,</span> out_dim <span class=\"token keyword\">in</span>\n    itertools<span class=\"token punctuation\">.</span>product<span class=\"token punctuation\">(</span><span class=\"token punctuation\">[</span><span class=\"token number\">4</span><span class=\"token punctuation\">,</span> <span class=\"token number\">8</span><span class=\"token punctuation\">]</span><span class=\"token punctuation\">,</span> <span class=\"token punctuation\">[</span><span class=\"token number\">4</span><span class=\"token punctuation\">,</span> <span class=\"token number\">8</span><span class=\"token punctuation\">]</span><span class=\"token punctuation\">,</span> <span class=\"token punctuation\">[</span><span class=\"token number\">4</span><span class=\"token punctuation\">,</span> <span class=\"token number\">8</span><span class=\"token punctuation\">]</span><span class=\"token punctuation\">,</span> <span class=\"token punctuation\">[</span><span class=\"token number\">4</span><span class=\"token punctuation\">,</span> <span class=\"token number\">8</span><span class=\"token punctuation\">]</span><span class=\"token punctuation\">,</span> <span class=\"token punctuation\">[</span><span class=\"token number\">4</span><span class=\"token punctuation\">,</span> <span class=\"token number\">8</span><span class=\"token punctuation\">]</span><span class=\"token punctuation\">)</span>\n<span class=\"token punctuation\">]</span><span class=\"token punctuation\">)</span>\n<span class=\"token keyword\">def</span> <span class=\"token function\">test_capsule_prediction_op</span><span class=\"token punctuation\">(</span>self<span class=\"token punctuation\">,</span> batch_size<span class=\"token punctuation\">,</span> in_caps<span class=\"token punctuation\">,</span> out_caps<span class=\"token punctuation\">,</span> in_dim<span class=\"token punctuation\">,</span>\n                               out_dim<span class=\"token punctuation\">)</span><span class=\"token punctuation\">:</span>\n    <span class=\"token triple-quoted-string string\">\"\"\" Tests the forward capsmatmul op \"\"\"</span>\n    x <span class=\"token operator\">=</span> np<span class=\"token punctuation\">.</span>random<span class=\"token punctuation\">.</span>rand<span class=\"token punctuation\">(</span>batch_size<span class=\"token punctuation\">,</span> in_caps<span class=\"token punctuation\">,</span> in_dim<span class=\"token punctuation\">)</span>\n    weights <span class=\"token operator\">=</span> np<span class=\"token punctuation\">.</span>random<span class=\"token punctuation\">.</span>rand<span class=\"token punctuation\">(</span>in_caps<span class=\"token punctuation\">,</span> out_caps<span class=\"token punctuation\">,</span> out_dim<span class=\"token punctuation\">,</span> in_dim<span class=\"token punctuation\">)</span>\n\n    truth <span class=\"token operator\">=</span> self<span class=\"token punctuation\">.</span>_numpy_capsule_prediction<span class=\"token punctuation\">(</span>x<span class=\"token punctuation\">,</span> weights<span class=\"token punctuation\">)</span>\n    <span class=\"token keyword\">with</span> self<span class=\"token punctuation\">.</span>test_session<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span> <span class=\"token keyword\">as</span> sess<span class=\"token punctuation\">:</span>\n        x_ph <span class=\"token operator\">=</span> tf<span class=\"token punctuation\">.</span>placeholder<span class=\"token punctuation\">(</span>tf<span class=\"token punctuation\">.</span>float32<span class=\"token punctuation\">,</span> x<span class=\"token punctuation\">.</span>shape<span class=\"token punctuation\">)</span>\n        w_ph <span class=\"token operator\">=</span> tf<span class=\"token punctuation\">.</span>placeholder<span class=\"token punctuation\">(</span>tf<span class=\"token punctuation\">.</span>float32<span class=\"token punctuation\">,</span> weights<span class=\"token punctuation\">.</span>shape<span class=\"token punctuation\">)</span>\n\n        ret <span class=\"token operator\">=</span> capsule_prediction<span class=\"token punctuation\">(</span>x_ph<span class=\"token punctuation\">,</span> w_ph<span class=\"token punctuation\">)</span>\n        out <span class=\"token operator\">=</span> sess<span class=\"token punctuation\">.</span>run<span class=\"token punctuation\">(</span>ret<span class=\"token punctuation\">,</span> <span class=\"token punctuation\">{</span>x_ph<span class=\"token punctuation\">:</span> x<span class=\"token punctuation\">,</span> w_ph<span class=\"token punctuation\">:</span> weights<span class=\"token punctuation\">}</span><span class=\"token punctuation\">)</span>\n    self<span class=\"token punctuation\">.</span>assertAllClose<span class=\"token punctuation\">(</span>truth<span class=\"token punctuation\">,</span> out<span class=\"token punctuation\">)</span></code></pre></div>\n<p>I’ve used quite a few tricks in here. First, the <code class=\"language-text\">parameterized</code> decorator offers a way to invoke the test with different parameters where each test should succeed on its own and will be considered as a separate test by <code class=\"language-text\">pytest</code>. If it fails, the provided input will also be displayed in the test logs so in my experience, using it really speeds up the debugging if needed. The <code class=\"language-text\">parameterized.expand</code> decorator expects a list of tuples. Each tuple will be unpacked as positional function parameters. We can easily generate many tuples to vary the dimension sizes by using itertools.product which takes the Cartesian product of all of its arguments.</p>\n<p>The <code class=\"language-text\">x</code> and <code class=\"language-text\">weights</code> arrays are initialized randomly. The TensorFlow graph that we build is simple: it only holds two placeholders and the <code class=\"language-text\">capsule_prediction</code> Op. The returned value should be the same as that of the <code class=\"language-text\">_numpy_capsule_prediction</code> function. Let’s run the tests:</p>\n<p>A nice feature of <code class=\"language-text\">pytest</code> is that you can add the <code class=\"language-text\">-k</code> flag to select a specific set of tests. Hooray, all tests passed!</p>\n<h2>The backward capsule prediction</h2>\n<p>Next up is the backward computation. You’ll notice that we have visited most of the coming concepts already. We already looked at methods to compute the correct indices for one-dimensional arrays by dimension sizes, we wrote a CUDA kernel, we registered our Op and we set up our tests. Therefore, I will speed things up a bit from here on. The only thing that is in our way is the exact definition of the gradient. It helps to consider a normal dense layer first:</p>\n<span class=\"katex-display\"><span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mi mathvariant=\"bold-italic\">z</mi><mo>=</mo><mi>W</mi><mi mathvariant=\"bold-italic\">x</mi></mrow><annotation encoding=\"application/x-tex\">\\boldsymbol z = W \\boldsymbol x</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.44444em;vertical-align:0em;\"></span><span class=\"mord\"><span class=\"mord boldsymbol\" style=\"margin-right:0.04213em;\">z</span></span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span><span class=\"mrel\">=</span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:0.68333em;vertical-align:0em;\"></span><span class=\"mord mathdefault\" style=\"margin-right:0.13889em;\">W</span><span class=\"mord\"><span class=\"mord boldsymbol\">x</span></span></span></span></span></span>\n<p>Given the gradients of a loss function <span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mi mathvariant=\"script\">L</mi></mrow><annotation encoding=\"application/x-tex\">\\mathcal L</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.68333em;vertical-align:0em;\"></span><span class=\"mord mathcal\">L</span></span></span></span> with respect to <span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mi mathvariant=\"bold-italic\">z</mi></mrow><annotation encoding=\"application/x-tex\">\\boldsymbol z</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.44444em;vertical-align:0em;\"></span><span class=\"mord\"><span class=\"mord boldsymbol\" style=\"margin-right:0.04213em;\">z</span></span></span></span></span> we should be able to get the gradients of <span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mi mathvariant=\"bold-italic\">x</mi></mrow><annotation encoding=\"application/x-tex\">\\boldsymbol x</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.44444em;vertical-align:0em;\"></span><span class=\"mord\"><span class=\"mord boldsymbol\">x</span></span></span></span></span> and <span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mi>W</mi></mrow><annotation encoding=\"application/x-tex\">W</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.68333em;vertical-align:0em;\"></span><span class=\"mord mathdefault\" style=\"margin-right:0.13889em;\">W</span></span></span></span>:</p>\n<span class=\"katex-display\"><span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mfrac><mrow><mi>d</mi><mi mathvariant=\"script\">L</mi></mrow><mrow><mi>d</mi><msub><mi>x</mi><mi>i</mi></msub></mrow></mfrac><mo>=</mo><mfrac><mrow><mi>d</mi><mi mathvariant=\"script\">L</mi></mrow><mrow><mi>d</mi><mi mathvariant=\"bold-italic\">z</mi></mrow></mfrac><mfrac><mrow><mi>d</mi><mi mathvariant=\"bold-italic\">z</mi></mrow><mrow><mi>d</mi><msub><mi>x</mi><mi>i</mi></msub></mrow></mfrac><mo>=</mo><msub><mover accent=\"true\"><mi mathvariant=\"bold-italic\">w</mi><mo>⃗</mo></mover><mi>i</mi></msub><msup><mi mathvariant=\"bold-italic\">z</mi><mo mathvariant=\"normal\">′</mo></msup></mrow><annotation encoding=\"application/x-tex\">\\frac{d \\mathcal L}{ d x_i} = \\frac{d\\mathcal L}{d \\boldsymbol z} \\frac{d\\boldsymbol z}{d x_i} = \\vec{\\boldsymbol w}_i \\boldsymbol z&#x27;</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:2.20744em;vertical-align:-0.8360000000000001em;\"></span><span class=\"mord\"><span class=\"mopen nulldelimiter\"></span><span class=\"mfrac\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:1.37144em;\"><span style=\"top:-2.3139999999999996em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span class=\"mord\"><span class=\"mord mathdefault\">d</span><span class=\"mord\"><span class=\"mord mathdefault\">x</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.31166399999999994em;\"><span style=\"top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mathdefault mtight\">i</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span></span></span><span style=\"top:-3.23em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span class=\"frac-line\" style=\"border-bottom-width:0.04em;\"></span></span><span style=\"top:-3.677em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span class=\"mord\"><span class=\"mord mathdefault\">d</span><span class=\"mord mathcal\">L</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.8360000000000001em;\"><span></span></span></span></span></span><span class=\"mclose nulldelimiter\"></span></span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span><span class=\"mrel\">=</span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:2.20744em;vertical-align:-0.8360000000000001em;\"></span><span class=\"mord\"><span class=\"mopen nulldelimiter\"></span><span class=\"mfrac\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:1.37144em;\"><span style=\"top:-2.314em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span class=\"mord\"><span class=\"mord mathdefault\">d</span><span class=\"mord\"><span class=\"mord boldsymbol\" style=\"margin-right:0.04213em;\">z</span></span></span></span><span style=\"top:-3.23em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span class=\"frac-line\" style=\"border-bottom-width:0.04em;\"></span></span><span style=\"top:-3.677em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span class=\"mord\"><span class=\"mord mathdefault\">d</span><span class=\"mord mathcal\">L</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.686em;\"><span></span></span></span></span></span><span class=\"mclose nulldelimiter\"></span></span><span class=\"mord\"><span class=\"mopen nulldelimiter\"></span><span class=\"mfrac\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:1.37144em;\"><span style=\"top:-2.3139999999999996em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span class=\"mord\"><span class=\"mord mathdefault\">d</span><span class=\"mord\"><span class=\"mord mathdefault\">x</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.31166399999999994em;\"><span style=\"top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mathdefault mtight\">i</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span></span></span><span style=\"top:-3.23em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span class=\"frac-line\" style=\"border-bottom-width:0.04em;\"></span></span><span style=\"top:-3.677em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span class=\"mord\"><span class=\"mord mathdefault\">d</span><span class=\"mord\"><span class=\"mord boldsymbol\" style=\"margin-right:0.04213em;\">z</span></span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.8360000000000001em;\"><span></span></span></span></span></span><span class=\"mclose nulldelimiter\"></span></span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span><span class=\"mrel\">=</span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:0.9518920000000001em;vertical-align:-0.15em;\"></span><span class=\"mord\"><span class=\"mord accent\"><span class=\"vlist-t\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.72744em;\"><span style=\"top:-3em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span class=\"mord\"><span class=\"mord\"><span class=\"mord boldsymbol\" style=\"margin-right:0.02778em;\">w</span></span></span></span><span style=\"top:-3.01344em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span class=\"accent-body\" style=\"left:-0.2355em;\"><span class=\"overlay\" style=\"height:0.714em;width:0.471em;\"><svg width='0.471em' height='0.714em' style='width:0.471em' viewBox='0 0 471 714' preserveAspectRatio='xMinYMin'><path d='M377 20c0-5.333 1.833-10 5.5-14S391 0 397 0c4.667 0 8.667 1.667 12 5\n3.333 2.667 6.667 9 10 19 6.667 24.667 20.333 43.667 41 57 7.333 4.667 11\n10.667 11 18 0 6-1 10-3 12s-6.667 5-14 9c-28.667 14.667-53.667 35.667-75 63\n-1.333 1.333-3.167 3.5-5.5 6.5s-4 4.833-5 5.5c-1 .667-2.5 1.333-4.5 2s-4.333 1\n-7 1c-4.667 0-9.167-1.833-13.5-5.5S337 184 337 178c0-12.667 15.667-32.333 47-59\nH213l-171-1c-8.667-6-13-12.333-13-19 0-4.667 4.333-11.333 13-20h359\nc-16-25.333-24-45-24-59z'/></svg></span></span></span></span></span></span></span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.31166399999999994em;\"><span style=\"top:-2.5500000000000003em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mathdefault mtight\">i</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span><span class=\"mord\"><span class=\"mord\"><span class=\"mord boldsymbol\" style=\"margin-right:0.04213em;\">z</span></span><span class=\"msupsub\"><span class=\"vlist-t\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.801892em;\"><span style=\"top:-3.113em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\"><span class=\"mord mtight\">′</span></span></span></span></span></span></span></span></span></span></span></span></span>\n<p>where <span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><msub><mover accent=\"true\"><mi mathvariant=\"bold-italic\">w</mi><mo>⃗</mo></mover><mi>i</mi></msub></mrow><annotation encoding=\"application/x-tex\">\\vec{\\boldsymbol w}_i</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.87744em;vertical-align:-0.15em;\"></span><span class=\"mord\"><span class=\"mord accent\"><span class=\"vlist-t\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.72744em;\"><span style=\"top:-3em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span class=\"mord\"><span class=\"mord\"><span class=\"mord boldsymbol\" style=\"margin-right:0.02778em;\">w</span></span></span></span><span style=\"top:-3.01344em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span class=\"accent-body\" style=\"left:-0.2355em;\"><span class=\"overlay\" style=\"height:0.714em;width:0.471em;\"><svg width='0.471em' height='0.714em' style='width:0.471em' viewBox='0 0 471 714' preserveAspectRatio='xMinYMin'><path d='M377 20c0-5.333 1.833-10 5.5-14S391 0 397 0c4.667 0 8.667 1.667 12 5\n3.333 2.667 6.667 9 10 19 6.667 24.667 20.333 43.667 41 57 7.333 4.667 11\n10.667 11 18 0 6-1 10-3 12s-6.667 5-14 9c-28.667 14.667-53.667 35.667-75 63\n-1.333 1.333-3.167 3.5-5.5 6.5s-4 4.833-5 5.5c-1 .667-2.5 1.333-4.5 2s-4.333 1\n-7 1c-4.667 0-9.167-1.833-13.5-5.5S337 184 337 178c0-12.667 15.667-32.333 47-59\nH213l-171-1c-8.667-6-13-12.333-13-19 0-4.667 4.333-11.333 13-20h359\nc-16-25.333-24-45-24-59z'/></svg></span></span></span></span></span></span></span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.31166399999999994em;\"><span style=\"top:-2.5500000000000003em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mathdefault mtight\">i</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span></span></span></span> is the i-th row vector and <span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><msup><mi mathvariant=\"bold-italic\">z</mi><mo mathvariant=\"normal\">′</mo></msup></mrow><annotation encoding=\"application/x-tex\">\\boldsymbol z&#x27;</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.751892em;vertical-align:0em;\"></span><span class=\"mord\"><span class=\"mord\"><span class=\"mord boldsymbol\" style=\"margin-right:0.04213em;\">z</span></span><span class=\"msupsub\"><span class=\"vlist-t\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.751892em;\"><span style=\"top:-3.063em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\"><span class=\"mord mtight\">′</span></span></span></span></span></span></span></span></span></span></span></span> is the vector holding the local gradients of the output. If this is the result for <span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mi>i</mi></mrow><annotation encoding=\"application/x-tex\">i</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.65952em;vertical-align:0em;\"></span><span class=\"mord mathdefault\">i</span></span></span></span> then the whole gradient for <span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mi mathvariant=\"bold-italic\">x</mi></mrow><annotation encoding=\"application/x-tex\">\\boldsymbol x</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.44444em;vertical-align:0em;\"></span><span class=\"mord\"><span class=\"mord boldsymbol\">x</span></span></span></span></span> can be computed by:</p>\n<span class=\"katex-display\"><span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mfrac><mrow><mi>d</mi><mi mathvariant=\"script\">L</mi></mrow><mrow><mi>d</mi><msub><mi>w</mi><mrow><mi>r</mi><mi>c</mi></mrow></msub></mrow></mfrac><mo>=</mo><msub><mi>x</mi><mi>r</mi></msub><msubsup><mi>z</mi><mi>c</mi><mo mathvariant=\"normal\">′</mo></msubsup></mrow><annotation encoding=\"application/x-tex\">\\frac{d \\mathcal L}{d w_{rc}} = x_r z&#x27;_c</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:2.20744em;vertical-align:-0.8360000000000001em;\"></span><span class=\"mord\"><span class=\"mopen nulldelimiter\"></span><span class=\"mfrac\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:1.37144em;\"><span style=\"top:-2.3139999999999996em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span class=\"mord\"><span class=\"mord mathdefault\">d</span><span class=\"mord\"><span class=\"mord mathdefault\" style=\"margin-right:0.02691em;\">w</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.151392em;\"><span style=\"top:-2.5500000000000003em;margin-left:-0.02691em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\"><span class=\"mord mathdefault mtight\" style=\"margin-right:0.02778em;\">r</span><span class=\"mord mathdefault mtight\">c</span></span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span></span></span><span style=\"top:-3.23em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span class=\"frac-line\" style=\"border-bottom-width:0.04em;\"></span></span><span style=\"top:-3.677em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span class=\"mord\"><span class=\"mord mathdefault\">d</span><span class=\"mord mathcal\">L</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.8360000000000001em;\"><span></span></span></span></span></span><span class=\"mclose nulldelimiter\"></span></span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span><span class=\"mrel\">=</span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:1.048892em;vertical-align:-0.247em;\"></span><span class=\"mord\"><span class=\"mord mathdefault\">x</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.151392em;\"><span style=\"top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mathdefault mtight\" style=\"margin-right:0.02778em;\">r</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span><span class=\"mord\"><span class=\"mord mathdefault\" style=\"margin-right:0.04398em;\">z</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.8018919999999999em;\"><span style=\"top:-2.4530000000000003em;margin-left:-0.04398em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mathdefault mtight\">c</span></span></span><span style=\"top:-3.113em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\"><span class=\"mord mtight\">′</span></span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.247em;\"><span></span></span></span></span></span></span></span></span></span></span>\n<p>So the matrix holding the gradients becomes an outer product:</p>\n<span class=\"katex-display\"><span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><mfrac><mrow><mi>d</mi><mi mathvariant=\"script\">L</mi></mrow><mrow><mi>d</mi><mi>W</mi></mrow></mfrac><mo>=</mo><mi mathvariant=\"bold-italic\">x</mi><mo stretchy=\"false\">(</mo><msup><mi>z</mi><mo mathvariant=\"normal\">′</mo></msup><msup><mo stretchy=\"false\">)</mo><mi mathvariant=\"normal\">⊤</mi></msup></mrow><annotation encoding=\"application/x-tex\">\\frac{d \\mathcal L}{dW} = \\boldsymbol x (z&#x27;)^{\\top}</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:2.05744em;vertical-align:-0.686em;\"></span><span class=\"mord\"><span class=\"mopen nulldelimiter\"></span><span class=\"mfrac\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:1.37144em;\"><span style=\"top:-2.314em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span class=\"mord\"><span class=\"mord mathdefault\">d</span><span class=\"mord mathdefault\" style=\"margin-right:0.13889em;\">W</span></span></span><span style=\"top:-3.23em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span class=\"frac-line\" style=\"border-bottom-width:0.04em;\"></span></span><span style=\"top:-3.677em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span class=\"mord\"><span class=\"mord mathdefault\">d</span><span class=\"mord mathcal\">L</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.686em;\"><span></span></span></span></span></span><span class=\"mclose nulldelimiter\"></span></span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span><span class=\"mrel\">=</span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:1.149108em;vertical-align:-0.25em;\"></span><span class=\"mord\"><span class=\"mord boldsymbol\">x</span></span><span class=\"mopen\">(</span><span class=\"mord\"><span class=\"mord mathdefault\" style=\"margin-right:0.04398em;\">z</span><span class=\"msupsub\"><span class=\"vlist-t\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.801892em;\"><span style=\"top:-3.113em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\"><span class=\"mord mtight\">′</span></span></span></span></span></span></span></span></span><span class=\"mclose\"><span class=\"mclose\">)</span><span class=\"msupsub\"><span class=\"vlist-t\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.8991079999999999em;\"><span style=\"top:-3.113em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\"><span class=\"mord mtight\">⊤</span></span></span></span></span></span></span></span></span></span></span></span></span>\n<p>Intuitively, this tells us to just pick the two neurons that are connected between these two layers and multiply the local gradient of the output neuron with the input neuron. This means that we can do the same for the capsule prediction layer. The only difference is the dimensionality of the tensors involved.</p>\n<h2>The gradient OpKernel</h2>\n<p>I won’t bother you with the details anymore. Here’s the implementation:</p>\n<div class=\"gatsby-highlight\" data-language=\"cpp\"><pre class=\"language-cpp\"><code class=\"language-cpp\"><span class=\"token keyword\">class</span> <span class=\"token class-name\">CapsulePredictionGradOp</span> <span class=\"token operator\">:</span> <span class=\"token keyword\">public</span> OpKernel\n<span class=\"token punctuation\">{</span>\n <span class=\"token keyword\">public</span><span class=\"token operator\">:</span>\n  <span class=\"token keyword\">explicit</span> <span class=\"token function\">CapsulePredictionGradOp</span><span class=\"token punctuation\">(</span>OpKernelConstruction<span class=\"token operator\">*</span> ctx<span class=\"token punctuation\">)</span> <span class=\"token operator\">:</span> <span class=\"token function\">OpKernel</span><span class=\"token punctuation\">(</span>ctx<span class=\"token punctuation\">)</span> <span class=\"token punctuation\">{</span> <span class=\"token punctuation\">}</span>\n\n  <span class=\"token keyword\">void</span> <span class=\"token function\">Compute</span><span class=\"token punctuation\">(</span>OpKernelContext<span class=\"token operator\">*</span> ctx<span class=\"token punctuation\">)</span> override\n  <span class=\"token punctuation\">{</span>\n    <span class=\"token comment\">// Get the input tensors</span>\n    <span class=\"token keyword\">const</span> Tensor<span class=\"token operator\">&amp;</span> grad <span class=\"token operator\">=</span> ctx<span class=\"token operator\">-></span><span class=\"token function\">input</span><span class=\"token punctuation\">(</span><span class=\"token number\">0</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">;</span>\n    <span class=\"token keyword\">const</span> Tensor<span class=\"token operator\">&amp;</span> input <span class=\"token operator\">=</span> ctx<span class=\"token operator\">-></span><span class=\"token function\">input</span><span class=\"token punctuation\">(</span><span class=\"token number\">1</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">;</span>\n    <span class=\"token keyword\">const</span> Tensor<span class=\"token operator\">&amp;</span> weights <span class=\"token operator\">=</span> ctx<span class=\"token operator\">-></span><span class=\"token function\">input</span><span class=\"token punctuation\">(</span><span class=\"token number\">2</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">;</span>\n\n    <span class=\"token comment\">// Get the shapes so that we can allocate outputs</span>\n    <span class=\"token keyword\">const</span> TensorShape<span class=\"token operator\">&amp;</span> <span class=\"token function\">input_shape</span><span class=\"token punctuation\">(</span>input<span class=\"token punctuation\">.</span><span class=\"token function\">shape</span><span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">;</span>\n    <span class=\"token keyword\">const</span> TensorShape<span class=\"token operator\">&amp;</span> <span class=\"token function\">weights_shape</span><span class=\"token punctuation\">(</span>weights<span class=\"token punctuation\">.</span><span class=\"token function\">shape</span><span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">;</span>\n\n    <span class=\"token comment\">// Allocate outputs</span>\n    Tensor<span class=\"token operator\">*</span> grad_input <span class=\"token operator\">=</span> <span class=\"token keyword\">nullptr</span><span class=\"token punctuation\">;</span>\n    Tensor<span class=\"token operator\">*</span> grad_weights <span class=\"token operator\">=</span> <span class=\"token keyword\">nullptr</span><span class=\"token punctuation\">;</span>\n    <span class=\"token function\">OP_REQUIRES_OK</span><span class=\"token punctuation\">(</span>ctx<span class=\"token punctuation\">,</span> ctx<span class=\"token operator\">-></span><span class=\"token function\">allocate_output</span><span class=\"token punctuation\">(</span><span class=\"token number\">0</span><span class=\"token punctuation\">,</span> input_shape<span class=\"token punctuation\">,</span> <span class=\"token operator\">&amp;</span>grad_input<span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">;</span>\n    <span class=\"token function\">OP_REQUIRES_OK</span><span class=\"token punctuation\">(</span>ctx<span class=\"token punctuation\">,</span> ctx<span class=\"token operator\">-></span><span class=\"token function\">allocate_output</span><span class=\"token punctuation\">(</span><span class=\"token number\">1</span><span class=\"token punctuation\">,</span> weights_shape<span class=\"token punctuation\">,</span> <span class=\"token operator\">&amp;</span>grad_weights<span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">;</span>\n\n    <span class=\"token comment\">// Get the Eigen tensors and pass them on to the launch function</span>\n    <span class=\"token keyword\">auto</span> input_tensor         <span class=\"token operator\">=</span> input<span class=\"token punctuation\">.</span>tensor<span class=\"token operator\">&lt;</span><span class=\"token keyword\">float</span><span class=\"token punctuation\">,</span> <span class=\"token number\">3</span><span class=\"token operator\">></span><span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">;</span>\n    <span class=\"token keyword\">auto</span> weights_tensor       <span class=\"token operator\">=</span> weights<span class=\"token punctuation\">.</span>tensor<span class=\"token operator\">&lt;</span><span class=\"token keyword\">float</span><span class=\"token punctuation\">,</span> <span class=\"token number\">4</span><span class=\"token operator\">></span><span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">;</span>\n    <span class=\"token keyword\">auto</span> grad_tensor          <span class=\"token operator\">=</span> grad<span class=\"token punctuation\">.</span>tensor<span class=\"token operator\">&lt;</span><span class=\"token keyword\">float</span><span class=\"token punctuation\">,</span> <span class=\"token number\">4</span><span class=\"token operator\">></span><span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">;</span>\n    <span class=\"token keyword\">auto</span> grad_input_tensor    <span class=\"token operator\">=</span> grad_input<span class=\"token operator\">-></span>tensor<span class=\"token operator\">&lt;</span><span class=\"token keyword\">float</span><span class=\"token punctuation\">,</span> <span class=\"token number\">3</span><span class=\"token operator\">></span><span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">;</span>\n    <span class=\"token keyword\">auto</span> grad_weights_tensor  <span class=\"token operator\">=</span> grad_weights<span class=\"token operator\">-></span>tensor<span class=\"token operator\">&lt;</span><span class=\"token keyword\">float</span><span class=\"token punctuation\">,</span> <span class=\"token number\">4</span><span class=\"token operator\">></span><span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">;</span>\n    <span class=\"token function\">launchCapsulePredictionGrad</span><span class=\"token punctuation\">(</span>\n      ctx<span class=\"token operator\">-></span>eigen_device<span class=\"token operator\">&lt;</span>GPUDevice<span class=\"token operator\">></span><span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">,</span> input_tensor<span class=\"token punctuation\">,</span> weights_tensor<span class=\"token punctuation\">,</span> grad_tensor<span class=\"token punctuation\">,</span>\n      grad_input_tensor<span class=\"token punctuation\">,</span> grad_weights_tensor\n    <span class=\"token punctuation\">)</span><span class=\"token punctuation\">;</span>\n  <span class=\"token punctuation\">}</span>\n<span class=\"token punctuation\">}</span><span class=\"token punctuation\">;</span></code></pre></div>\n<p>Nothing truly new here. An important difference is that we now have to allocate two output tensors: one for the weight gradient and one for the input gradient. The shape of a tensor’s gradient is identical to the shape of the tensor itself. Thus finding the shape for the allocated tensors is a piece of cake. Let’s check out the <code class=\"language-text\">launchCapsulePredictionGrad</code> function:</p>\n<div class=\"gatsby-highlight\" data-language=\"cpp\"><pre class=\"language-cpp\"><code class=\"language-cpp\"><span class=\"token keyword\">void</span> <span class=\"token function\">launchCapsulePredictionGrad</span><span class=\"token punctuation\">(</span>\n  <span class=\"token keyword\">const</span> GPUDevice<span class=\"token operator\">&amp;</span> d<span class=\"token punctuation\">,</span>\n  <span class=\"token keyword\">typename</span> TTypes<span class=\"token operator\">&lt;</span><span class=\"token keyword\">float</span><span class=\"token punctuation\">,</span> <span class=\"token number\">3</span><span class=\"token operator\">></span><span class=\"token operator\">::</span>ConstTensor input<span class=\"token punctuation\">,</span>\n  <span class=\"token keyword\">typename</span> TTypes<span class=\"token operator\">&lt;</span><span class=\"token keyword\">float</span><span class=\"token punctuation\">,</span> <span class=\"token number\">4</span><span class=\"token operator\">></span><span class=\"token operator\">::</span>ConstTensor weights<span class=\"token punctuation\">,</span>\n  <span class=\"token keyword\">typename</span> TTypes<span class=\"token operator\">&lt;</span><span class=\"token keyword\">float</span><span class=\"token punctuation\">,</span> <span class=\"token number\">4</span><span class=\"token operator\">></span><span class=\"token operator\">::</span>ConstTensor grad<span class=\"token punctuation\">,</span>\n  <span class=\"token keyword\">typename</span> TTypes<span class=\"token operator\">&lt;</span><span class=\"token keyword\">float</span><span class=\"token punctuation\">,</span> <span class=\"token number\">3</span><span class=\"token operator\">></span><span class=\"token operator\">::</span>Tensor grad_input<span class=\"token punctuation\">,</span>\n  <span class=\"token keyword\">typename</span> TTypes<span class=\"token operator\">&lt;</span><span class=\"token keyword\">float</span><span class=\"token punctuation\">,</span> <span class=\"token number\">4</span><span class=\"token operator\">></span><span class=\"token operator\">::</span>Tensor grad_weights<span class=\"token punctuation\">)</span>\n<span class=\"token punctuation\">{</span>\n  <span class=\"token keyword\">const</span> int64 batch_size  <span class=\"token operator\">=</span> input<span class=\"token punctuation\">.</span><span class=\"token function\">dimension</span><span class=\"token punctuation\">(</span><span class=\"token number\">0</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">;</span>\n  <span class=\"token keyword\">const</span> int64 in_caps     <span class=\"token operator\">=</span> input<span class=\"token punctuation\">.</span><span class=\"token function\">dimension</span><span class=\"token punctuation\">(</span><span class=\"token number\">1</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">;</span>\n  <span class=\"token keyword\">const</span> int64 in_dim      <span class=\"token operator\">=</span> input<span class=\"token punctuation\">.</span><span class=\"token function\">dimension</span><span class=\"token punctuation\">(</span><span class=\"token number\">2</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">;</span>\n  <span class=\"token keyword\">const</span> int64 out_dim     <span class=\"token operator\">=</span> weights<span class=\"token punctuation\">.</span><span class=\"token function\">dimension</span><span class=\"token punctuation\">(</span><span class=\"token number\">2</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">;</span>\n  <span class=\"token keyword\">const</span> int64 out_caps    <span class=\"token operator\">=</span> weights<span class=\"token punctuation\">.</span><span class=\"token function\">dimension</span><span class=\"token punctuation\">(</span><span class=\"token number\">1</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">;</span>\n\n  <span class=\"token comment\">// Size first dim</span>\n  <span class=\"token keyword\">const</span> int64 w_d0 <span class=\"token operator\">=</span> out_caps <span class=\"token operator\">*</span> out_dim <span class=\"token operator\">*</span> in_dim<span class=\"token punctuation\">;</span>\n  <span class=\"token keyword\">const</span> int64 x_d0 <span class=\"token operator\">=</span> in_caps <span class=\"token operator\">*</span> in_dim<span class=\"token punctuation\">;</span>\n  <span class=\"token keyword\">const</span> int64 o_d0 <span class=\"token operator\">=</span> in_caps <span class=\"token operator\">*</span> out_caps <span class=\"token operator\">*</span> out_dim<span class=\"token punctuation\">;</span>\n\n  <span class=\"token comment\">// Second dim</span>\n  <span class=\"token keyword\">const</span> int64 w_d1 <span class=\"token operator\">=</span> out_dim <span class=\"token operator\">*</span> in_dim<span class=\"token punctuation\">;</span>\n  <span class=\"token keyword\">const</span> int64 x_d1 <span class=\"token operator\">=</span> in_dim<span class=\"token punctuation\">;</span>\n  <span class=\"token keyword\">const</span> int64 o_d1 <span class=\"token operator\">=</span> out_caps <span class=\"token operator\">*</span> out_dim<span class=\"token punctuation\">;</span>\n\n  <span class=\"token comment\">// Third dim</span>\n  <span class=\"token keyword\">const</span> int64 w_d2 <span class=\"token operator\">=</span> in_dim<span class=\"token punctuation\">;</span>\n  <span class=\"token keyword\">const</span> int64 o_d2 <span class=\"token operator\">=</span> out_dim<span class=\"token punctuation\">;</span>\n\n  <span class=\"token comment\">// Launch input gradient kernel</span>\n  CudaLaunchConfig config <span class=\"token operator\">=</span> <span class=\"token function\">GetCudaLaunchConfig</span><span class=\"token punctuation\">(</span>grad_input<span class=\"token punctuation\">.</span><span class=\"token function\">size</span><span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">,</span> d<span class=\"token punctuation\">)</span><span class=\"token punctuation\">;</span>\n  capsulePredictionInputGradKernel\n    <span class=\"token operator\">&lt;&lt;</span><span class=\"token operator\">&lt;</span>config<span class=\"token punctuation\">.</span>block_count<span class=\"token punctuation\">,</span> config<span class=\"token punctuation\">.</span>thread_per_block<span class=\"token punctuation\">,</span> <span class=\"token number\">0</span><span class=\"token punctuation\">,</span> d<span class=\"token punctuation\">.</span><span class=\"token function\">stream</span><span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span><span class=\"token operator\">>></span><span class=\"token operator\">></span><span class=\"token punctuation\">(</span>\n      grad<span class=\"token punctuation\">.</span><span class=\"token function\">data</span><span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">,</span> weights<span class=\"token punctuation\">.</span><span class=\"token function\">data</span><span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">,</span> grad_input<span class=\"token punctuation\">.</span><span class=\"token function\">data</span><span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">,</span>\n      w_d0<span class=\"token punctuation\">,</span> x_d0<span class=\"token punctuation\">,</span> x_d1<span class=\"token punctuation\">,</span> o_d0<span class=\"token punctuation\">,</span> o_d1<span class=\"token punctuation\">,</span> out_caps<span class=\"token punctuation\">,</span> out_dim<span class=\"token punctuation\">,</span> in_dim<span class=\"token punctuation\">,</span>\n      grad_input<span class=\"token punctuation\">.</span><span class=\"token function\">size</span><span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">;</span>\n\n  <span class=\"token comment\">// Launch weight gradient kernel</span>\n  config <span class=\"token operator\">=</span> <span class=\"token function\">GetCudaLaunchConfig</span><span class=\"token punctuation\">(</span>grad_weights<span class=\"token punctuation\">.</span><span class=\"token function\">size</span><span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">,</span> d<span class=\"token punctuation\">)</span><span class=\"token punctuation\">;</span>\n  capsulePredictionWeightsGradKernel\n    <span class=\"token operator\">&lt;&lt;</span><span class=\"token operator\">&lt;</span>config<span class=\"token punctuation\">.</span>block_count<span class=\"token punctuation\">,</span> config<span class=\"token punctuation\">.</span>thread_per_block<span class=\"token punctuation\">,</span> <span class=\"token number\">0</span><span class=\"token punctuation\">,</span> d<span class=\"token punctuation\">.</span><span class=\"token function\">stream</span><span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span><span class=\"token operator\">>></span><span class=\"token operator\">></span><span class=\"token punctuation\">(</span>\n      grad<span class=\"token punctuation\">.</span><span class=\"token function\">data</span><span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">,</span> input<span class=\"token punctuation\">.</span><span class=\"token function\">data</span><span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">,</span> grad_weights<span class=\"token punctuation\">.</span><span class=\"token function\">data</span><span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">,</span> batch_size<span class=\"token punctuation\">,</span>\n      grad_weights<span class=\"token punctuation\">.</span><span class=\"token function\">size</span><span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">,</span> w_d0<span class=\"token punctuation\">,</span> w_d1<span class=\"token punctuation\">,</span> w_d2<span class=\"token punctuation\">,</span> x_d0<span class=\"token punctuation\">,</span> x_d1<span class=\"token punctuation\">,</span> o_d0<span class=\"token punctuation\">,</span> o_d1<span class=\"token punctuation\">,</span> o_d2<span class=\"token punctuation\">)</span><span class=\"token punctuation\">;</span>\n<span class=\"token punctuation\">}</span></code></pre></div>\n<p>Again, we see a similar code structure. We obtain the dimensions, we determine the memory sizes and finally, we launch not one, but two kernels.</p>\n<p>Glad you’re still there! Now it gets a bit more complicated. Behold the input gradient CUDA kernel:</p>\n<div class=\"gatsby-highlight\" data-language=\"cpp\"><pre class=\"language-cpp\"><code class=\"language-cpp\">__global__ <span class=\"token keyword\">void</span> <span class=\"token function\">capsulePredictionInputGradKernel</span><span class=\"token punctuation\">(</span>\n  <span class=\"token keyword\">const</span> <span class=\"token keyword\">float</span><span class=\"token operator\">*</span> grad<span class=\"token punctuation\">,</span> <span class=\"token keyword\">const</span> <span class=\"token keyword\">float</span><span class=\"token operator\">*</span> weights<span class=\"token punctuation\">,</span> <span class=\"token keyword\">float</span><span class=\"token operator\">*</span> grad_input<span class=\"token punctuation\">,</span>\n  <span class=\"token keyword\">const</span> int64 w_d0<span class=\"token punctuation\">,</span>\n  <span class=\"token keyword\">const</span> int64 x_d0<span class=\"token punctuation\">,</span> <span class=\"token keyword\">const</span> int64 x_d1<span class=\"token punctuation\">,</span>\n  <span class=\"token keyword\">const</span> int64 o_d0<span class=\"token punctuation\">,</span> <span class=\"token keyword\">const</span> int64 o_d1<span class=\"token punctuation\">,</span>\n  <span class=\"token keyword\">const</span> int64 out_caps<span class=\"token punctuation\">,</span>\n  <span class=\"token keyword\">const</span> int64 out_dim<span class=\"token punctuation\">,</span>\n  <span class=\"token keyword\">const</span> int64 in_dim<span class=\"token punctuation\">,</span>\n  <span class=\"token keyword\">const</span> int64 output_size<span class=\"token punctuation\">)</span>\n<span class=\"token punctuation\">{</span>\n  <span class=\"token function\">CUDA_1D_KERNEL_LOOP</span><span class=\"token punctuation\">(</span>i<span class=\"token punctuation\">,</span> output_size<span class=\"token punctuation\">)</span>\n  <span class=\"token punctuation\">{</span>\n    <span class=\"token comment\">// So here we have in_grad[b,ci,e]</span>\n    <span class=\"token keyword\">const</span> int64 b     <span class=\"token operator\">=</span> i <span class=\"token operator\">/</span> x_d0<span class=\"token punctuation\">;</span>\n    <span class=\"token keyword\">const</span> int64 ci    <span class=\"token operator\">=</span> <span class=\"token punctuation\">(</span>i <span class=\"token operator\">%</span> x_d0<span class=\"token punctuation\">)</span> <span class=\"token operator\">/</span> x_d1<span class=\"token punctuation\">;</span>\n    <span class=\"token keyword\">const</span> int64 e_in  <span class=\"token operator\">=</span> i <span class=\"token operator\">%</span> x_d1<span class=\"token punctuation\">;</span>\n\n    <span class=\"token comment\">// Then, we can have a look at computing the array indices for in and W</span>\n    int64 w_idx       <span class=\"token operator\">=</span> ci <span class=\"token operator\">*</span> w_d0 <span class=\"token operator\">+</span> e_in<span class=\"token punctuation\">;</span>\n    int64 grad_idx    <span class=\"token operator\">=</span> b <span class=\"token operator\">*</span> o_d0 <span class=\"token operator\">+</span> ci <span class=\"token operator\">*</span> o_d1<span class=\"token punctuation\">;</span>\n\n    <span class=\"token comment\">// Initialize result</span>\n    <span class=\"token keyword\">float</span> result      <span class=\"token operator\">=</span> <span class=\"token number\">0.0</span><span class=\"token punctuation\">;</span>\n    <span class=\"token comment\">// Iterate over cj and e_out, we already have the other indices</span>\n    <span class=\"token keyword\">for</span> <span class=\"token punctuation\">(</span><span class=\"token keyword\">int</span> cj <span class=\"token operator\">=</span> <span class=\"token number\">0</span><span class=\"token punctuation\">;</span> cj <span class=\"token operator\">&lt;</span> out_caps<span class=\"token punctuation\">;</span> <span class=\"token operator\">++</span>cj<span class=\"token punctuation\">)</span>\n    <span class=\"token punctuation\">{</span>\n      <span class=\"token keyword\">for</span> <span class=\"token punctuation\">(</span><span class=\"token keyword\">int</span> e_out <span class=\"token operator\">=</span> <span class=\"token number\">0</span><span class=\"token punctuation\">;</span> e_out <span class=\"token operator\">&lt;</span> out_dim<span class=\"token punctuation\">;</span> <span class=\"token operator\">++</span>e_out<span class=\"token punctuation\">)</span>\n      <span class=\"token punctuation\">{</span>\n        <span class=\"token comment\">// Next element of grad can be found by incrementing grad_idx</span>\n        result  <span class=\"token operator\">+=</span> <span class=\"token function\">ldg</span><span class=\"token punctuation\">(</span>grad <span class=\"token operator\">+</span> grad_idx<span class=\"token operator\">++</span><span class=\"token punctuation\">)</span> <span class=\"token operator\">*</span> <span class=\"token function\">ldg</span><span class=\"token punctuation\">(</span>weights <span class=\"token operator\">+</span> w_idx<span class=\"token punctuation\">)</span><span class=\"token punctuation\">;</span>\n        <span class=\"token comment\">// Next element of weights can be found by going to the next output</span>\n        <span class=\"token comment\">// capsule element, meaning that we add in_dim to w_idx</span>\n        w_idx   <span class=\"token operator\">+=</span> in_dim<span class=\"token punctuation\">;</span>\n      <span class=\"token punctuation\">}</span>\n    <span class=\"token punctuation\">}</span>\n    <span class=\"token comment\">// Write the result</span>\n    grad_input<span class=\"token punctuation\">[</span>i<span class=\"token punctuation\">]</span> <span class=\"token operator\">=</span> result<span class=\"token punctuation\">;</span>\n  <span class=\"token punctuation\">}</span>\n<span class=\"token punctuation\">}</span></code></pre></div>\n<p>I have added quite some comments to the code to make it more readable. Similar to our previous CUDA kernel, this one determines the axis indices that matter: <code class=\"language-text\">b</code>, <code class=\"language-text\">ci</code> and <code class=\"language-text\">e_in</code>. These are then used to compute the one-dimensional indices for <code class=\"language-text\">w</code> and <code class=\"language-text\">grad</code> (the output gradient). A single input neuron is not just used for one matrix-vector product, but it is involved for all prediction matrices <span class=\"katex\"><span class=\"katex-mathml\"><math><semantics><mrow><msub><mi>W</mi><mrow><mi>i</mi><mi>j</mi></mrow></msub><mo separator=\"true\">,</mo><mi>j</mi><mo>∈</mo><mo stretchy=\"false\">{</mo><mn>1</mn><mo separator=\"true\">,</mo><mo>…</mo><mi mathvariant=\"normal\">#</mi><mtext>output capsules</mtext><mo stretchy=\"false\">}</mo></mrow><annotation encoding=\"application/x-tex\">W_{ij}, j \\in \\{1, \\ldots \\# \\text{output capsules}\\}</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.969438em;vertical-align:-0.286108em;\"></span><span class=\"mord\"><span class=\"mord mathdefault\" style=\"margin-right:0.13889em;\">W</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.311664em;\"><span style=\"top:-2.5500000000000003em;margin-left:-0.13889em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\"><span class=\"mord mathdefault mtight\">i</span><span class=\"mord mathdefault mtight\" style=\"margin-right:0.05724em;\">j</span></span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.286108em;\"><span></span></span></span></span></span></span><span class=\"mpunct\">,</span><span class=\"mspace\" style=\"margin-right:0.16666666666666666em;\"></span><span class=\"mord mathdefault\" style=\"margin-right:0.05724em;\">j</span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span><span class=\"mrel\">∈</span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:1em;vertical-align:-0.25em;\"></span><span class=\"mopen\">{</span><span class=\"mord\">1</span><span class=\"mpunct\">,</span><span class=\"mspace\" style=\"margin-right:0.16666666666666666em;\"></span><span class=\"minner\">…</span><span class=\"mspace\" style=\"margin-right:0.16666666666666666em;\"></span><span class=\"mord\">#</span><span class=\"mord text\"><span class=\"mord\">output capsules</span></span><span class=\"mclose\">}</span></span></span></span>. Therefore, we require two loops, one dealing with the output capsules and another one that deals with the individual capsule elements. Rather than just incrementing the index for our weights, we must now skip to the next output capsule in the inner loop. This means that we should add <code class=\"language-text\">in_dim</code> to the index on each iteration.</p>\n<p>The weight gradient CUDA kernel implementation is:</p>\n<div class=\"gatsby-highlight\" data-language=\"cpp\"><pre class=\"language-cpp\"><code class=\"language-cpp\">__global__ <span class=\"token keyword\">void</span> <span class=\"token function\">capsulePredictionWeightsGradKernel</span><span class=\"token punctuation\">(</span>\n  <span class=\"token keyword\">const</span> <span class=\"token keyword\">float</span><span class=\"token operator\">*</span> grad<span class=\"token punctuation\">,</span> <span class=\"token keyword\">const</span> <span class=\"token keyword\">float</span><span class=\"token operator\">*</span> input<span class=\"token punctuation\">,</span> <span class=\"token keyword\">float</span><span class=\"token operator\">*</span> grad_weights<span class=\"token punctuation\">,</span>\n  <span class=\"token keyword\">const</span> int64 batch_size<span class=\"token punctuation\">,</span> <span class=\"token keyword\">const</span> int64 output_size<span class=\"token punctuation\">,</span>\n  <span class=\"token keyword\">const</span> int64 w_d0<span class=\"token punctuation\">,</span> <span class=\"token keyword\">const</span> int64 w_d1<span class=\"token punctuation\">,</span> <span class=\"token keyword\">const</span> int64 w_d2<span class=\"token punctuation\">,</span>\n  <span class=\"token keyword\">const</span> int64 x_d0<span class=\"token punctuation\">,</span> <span class=\"token keyword\">const</span> int64 x_d1<span class=\"token punctuation\">,</span>\n  <span class=\"token keyword\">const</span> int64 o_d0<span class=\"token punctuation\">,</span> <span class=\"token keyword\">const</span> int64 o_d1<span class=\"token punctuation\">,</span> <span class=\"token keyword\">const</span> int64 o_d2\n<span class=\"token punctuation\">)</span>\n<span class=\"token punctuation\">{</span>\n  <span class=\"token function\">CUDA_1D_KERNEL_LOOP</span><span class=\"token punctuation\">(</span>i<span class=\"token punctuation\">,</span> output_size<span class=\"token punctuation\">)</span>\n  <span class=\"token punctuation\">{</span>\n    <span class=\"token comment\">// So here we have w[ci,cj,e_out,e_in]</span>\n    <span class=\"token keyword\">const</span> int64 ci    <span class=\"token operator\">=</span> i <span class=\"token operator\">/</span> w_d0<span class=\"token punctuation\">;</span>\n    <span class=\"token keyword\">const</span> int64 cj    <span class=\"token operator\">=</span> <span class=\"token punctuation\">(</span>i <span class=\"token operator\">%</span> w_d0<span class=\"token punctuation\">)</span> <span class=\"token operator\">/</span> w_d1<span class=\"token punctuation\">;</span>\n    <span class=\"token keyword\">const</span> int64 e_out <span class=\"token operator\">=</span> <span class=\"token punctuation\">(</span>i <span class=\"token operator\">%</span> w_d1<span class=\"token punctuation\">)</span> <span class=\"token operator\">/</span> w_d2<span class=\"token punctuation\">;</span>\n    <span class=\"token keyword\">const</span> int64 e_in  <span class=\"token operator\">=</span> i <span class=\"token operator\">%</span> w_d2<span class=\"token punctuation\">;</span>\n\n    <span class=\"token comment\">// Then, we can have a look at computing the array indices for</span>\n    <span class=\"token comment\">// in and grad</span>\n    int64 input_idx   <span class=\"token operator\">=</span> ci <span class=\"token operator\">*</span> x_d1 <span class=\"token operator\">+</span> e_in<span class=\"token punctuation\">;</span>               <span class=\"token comment\">// (b == 0)</span>\n    int64 grad_idx    <span class=\"token operator\">=</span> ci <span class=\"token operator\">*</span> o_d1 <span class=\"token operator\">+</span> cj <span class=\"token operator\">*</span> o_d2 <span class=\"token operator\">+</span> e_out<span class=\"token punctuation\">;</span>  <span class=\"token comment\">// (b == 0)</span>\n\n    <span class=\"token comment\">// Initilize result</span>\n    <span class=\"token keyword\">float</span> result      <span class=\"token operator\">=</span> <span class=\"token number\">0.0</span><span class=\"token punctuation\">;</span>\n    <span class=\"token comment\">// We only iterate over b, since we have the other indices already</span>\n    <span class=\"token keyword\">for</span> <span class=\"token punctuation\">(</span>int64 b <span class=\"token operator\">=</span> <span class=\"token number\">0</span><span class=\"token punctuation\">;</span> b <span class=\"token operator\">&lt;</span> batch_size<span class=\"token punctuation\">;</span> b<span class=\"token operator\">++</span><span class=\"token punctuation\">)</span>\n    <span class=\"token punctuation\">{</span>\n      result <span class=\"token operator\">+=</span> <span class=\"token function\">ldg</span><span class=\"token punctuation\">(</span>grad <span class=\"token operator\">+</span> grad_idx<span class=\"token punctuation\">)</span> <span class=\"token operator\">*</span> <span class=\"token function\">ldg</span><span class=\"token punctuation\">(</span>input <span class=\"token operator\">+</span> input_idx<span class=\"token punctuation\">)</span><span class=\"token punctuation\">;</span>\n      <span class=\"token comment\">// Next elements can be found by jumping to the next batch</span>\n      input_idx <span class=\"token operator\">+=</span> x_d0<span class=\"token punctuation\">;</span>\n      grad_idx  <span class=\"token operator\">+=</span> o_d0<span class=\"token punctuation\">;</span>\n    <span class=\"token punctuation\">}</span>\n    grad_weights<span class=\"token punctuation\">[</span>i<span class=\"token punctuation\">]</span> <span class=\"token operator\">=</span> result<span class=\"token punctuation\">;</span>\n  <span class=\"token punctuation\">}</span>\n<span class=\"token punctuation\">}</span></code></pre></div>\n<p>The same tricks apply here again: we compute the tensor axis indices, we compute the one-dimensional indices and we loop over the required elements to obtain our output. What I didn’t mention before is that we have to sum the gradients for each weight over all samples in the batch. Given the memory size for the input and output tensors along the 0-th axis, this is a straightforward thing to do.</p>\n<p>Before our gradient implementation nicely integrates with TensorFlow, we have to register it as being the gradient of our <code class=\"language-text\">CapsulePrediction</code> Op:</p>\n<div class=\"gatsby-highlight\" data-language=\"python\"><pre class=\"language-python\"><code class=\"language-python\"><span class=\"token decorator annotation punctuation\">@ops<span class=\"token punctuation\">.</span>RegisterGradient</span><span class=\"token punctuation\">(</span><span class=\"token string\">\"CapsulePrediction\"</span><span class=\"token punctuation\">)</span>\n<span class=\"token keyword\">def</span> <span class=\"token function\">_capsule_prediction_grad</span><span class=\"token punctuation\">(</span>op<span class=\"token punctuation\">,</span> grad<span class=\"token punctuation\">)</span><span class=\"token punctuation\">:</span>\n    <span class=\"token triple-quoted-string string\">\"\"\" Computes gradient for capsule prediction operation \"\"\"</span>\n    <span class=\"token keyword\">return</span> op_module<span class=\"token punctuation\">.</span>capsule_prediction_grad<span class=\"token punctuation\">(</span>grad<span class=\"token punctuation\">,</span> op<span class=\"token punctuation\">.</span>inputs<span class=\"token punctuation\">[</span><span class=\"token number\">0</span><span class=\"token punctuation\">]</span><span class=\"token punctuation\">,</span> op<span class=\"token punctuation\">.</span>inputs<span class=\"token punctuation\">[</span><span class=\"token number\">1</span><span class=\"token punctuation\">]</span><span class=\"token punctuation\">)</span></code></pre></div>\n<p>Now we can just use tf.gradients after which the computation graph for the gradient should include our gradient op. Awesome!</p>\n<h2>Testing The Gradients</h2>\n<p>We have arrived at one of the last stages: testing the gradient. This really sounds harder than it is. TensorFlow already has a gradient testing utility, and we’ll use it here. We add the following methods to our <code class=\"language-text\">CapsulePredictionOpTest</code> class:</p>\n<div class=\"gatsby-highlight\" data-language=\"python\"><pre class=\"language-python\"><code class=\"language-python\"><span class=\"token decorator annotation punctuation\">@parameterized<span class=\"token punctuation\">.</span>expand</span><span class=\"token punctuation\">(</span><span class=\"token punctuation\">[</span>\n    <span class=\"token punctuation\">(</span>batch_size<span class=\"token punctuation\">,</span> in_caps<span class=\"token punctuation\">,</span> out_caps<span class=\"token punctuation\">,</span> in_dim<span class=\"token punctuation\">,</span> out_dim<span class=\"token punctuation\">)</span> <span class=\"token keyword\">for</span>\n    batch_size<span class=\"token punctuation\">,</span> in_caps<span class=\"token punctuation\">,</span> out_caps<span class=\"token punctuation\">,</span> in_dim<span class=\"token punctuation\">,</span> out_dim <span class=\"token keyword\">in</span>\n    itertools<span class=\"token punctuation\">.</span>product<span class=\"token punctuation\">(</span><span class=\"token punctuation\">[</span><span class=\"token number\">4</span><span class=\"token punctuation\">,</span> <span class=\"token number\">8</span><span class=\"token punctuation\">]</span><span class=\"token punctuation\">,</span> <span class=\"token punctuation\">[</span><span class=\"token number\">4</span><span class=\"token punctuation\">,</span> <span class=\"token number\">8</span><span class=\"token punctuation\">]</span><span class=\"token punctuation\">,</span> <span class=\"token punctuation\">[</span><span class=\"token number\">4</span><span class=\"token punctuation\">,</span> <span class=\"token number\">8</span><span class=\"token punctuation\">]</span><span class=\"token punctuation\">,</span> <span class=\"token punctuation\">[</span><span class=\"token number\">4</span><span class=\"token punctuation\">,</span> <span class=\"token number\">8</span><span class=\"token punctuation\">]</span><span class=\"token punctuation\">,</span> <span class=\"token punctuation\">[</span><span class=\"token number\">4</span><span class=\"token punctuation\">,</span> <span class=\"token number\">8</span><span class=\"token punctuation\">]</span><span class=\"token punctuation\">)</span>\n<span class=\"token punctuation\">]</span><span class=\"token punctuation\">)</span>\n<span class=\"token keyword\">def</span> <span class=\"token function\">test_capsule_prediction_weights_grad</span><span class=\"token punctuation\">(</span>self<span class=\"token punctuation\">,</span> batch_size<span class=\"token punctuation\">,</span> in_caps<span class=\"token punctuation\">,</span> out_caps<span class=\"token punctuation\">,</span>\n                                         in_dim<span class=\"token punctuation\">,</span> out_dim<span class=\"token punctuation\">)</span><span class=\"token punctuation\">:</span>\n    <span class=\"token triple-quoted-string string\">\"\"\" Tests gradient of output w.r.t. weights \"\"\"</span>\n    x <span class=\"token operator\">=</span> np<span class=\"token punctuation\">.</span>random<span class=\"token punctuation\">.</span>rand<span class=\"token punctuation\">(</span>batch_size<span class=\"token punctuation\">,</span> in_caps<span class=\"token punctuation\">,</span> in_dim<span class=\"token punctuation\">)</span>\n    weights <span class=\"token operator\">=</span> np<span class=\"token punctuation\">.</span>random<span class=\"token punctuation\">.</span>rand<span class=\"token punctuation\">(</span>in_caps<span class=\"token punctuation\">,</span> out_caps<span class=\"token punctuation\">,</span> out_dim<span class=\"token punctuation\">,</span> in_dim<span class=\"token punctuation\">)</span>\n    out_shape <span class=\"token operator\">=</span> <span class=\"token punctuation\">(</span>batch_size<span class=\"token punctuation\">,</span> in_caps<span class=\"token punctuation\">,</span> out_caps<span class=\"token punctuation\">,</span> out_dim<span class=\"token punctuation\">)</span>\n\n    <span class=\"token keyword\">with</span> self<span class=\"token punctuation\">.</span>test_session<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">:</span>\n        x_ph <span class=\"token operator\">=</span> tf<span class=\"token punctuation\">.</span>placeholder<span class=\"token punctuation\">(</span>tf<span class=\"token punctuation\">.</span>float32<span class=\"token punctuation\">,</span> x<span class=\"token punctuation\">.</span>shape<span class=\"token punctuation\">)</span>\n        w_ph <span class=\"token operator\">=</span> tf<span class=\"token punctuation\">.</span>placeholder<span class=\"token punctuation\">(</span>tf<span class=\"token punctuation\">.</span>float32<span class=\"token punctuation\">,</span> weights<span class=\"token punctuation\">.</span>shape<span class=\"token punctuation\">)</span>\n        fd <span class=\"token operator\">=</span> <span class=\"token punctuation\">{</span>x_ph<span class=\"token punctuation\">:</span> x<span class=\"token punctuation\">,</span> w_ph<span class=\"token punctuation\">:</span> weights<span class=\"token punctuation\">}</span>\n\n        caps_out <span class=\"token operator\">=</span> capsule_prediction<span class=\"token punctuation\">(</span>x_ph<span class=\"token punctuation\">,</span> w_ph<span class=\"token punctuation\">)</span>\n        grad_w <span class=\"token operator\">=</span> tf<span class=\"token punctuation\">.</span>test<span class=\"token punctuation\">.</span>compute_gradient<span class=\"token punctuation\">(</span>\n            w_ph<span class=\"token punctuation\">,</span> weights<span class=\"token punctuation\">.</span>shape<span class=\"token punctuation\">,</span> caps_out<span class=\"token punctuation\">,</span> out_shape<span class=\"token punctuation\">,</span> extra_feed_dict<span class=\"token operator\">=</span>fd\n        <span class=\"token punctuation\">)</span>\n\n    self<span class=\"token punctuation\">.</span>assertAllClose<span class=\"token punctuation\">(</span>grad_w<span class=\"token punctuation\">[</span><span class=\"token number\">0</span><span class=\"token punctuation\">]</span><span class=\"token punctuation\">,</span> grad_w<span class=\"token punctuation\">[</span><span class=\"token number\">1</span><span class=\"token punctuation\">]</span><span class=\"token punctuation\">,</span> atol<span class=\"token operator\">=</span><span class=\"token number\">1e</span><span class=\"token operator\">-</span><span class=\"token number\">3</span><span class=\"token punctuation\">,</span> rtol<span class=\"token operator\">=</span><span class=\"token number\">1e</span><span class=\"token operator\">-</span><span class=\"token number\">3</span><span class=\"token punctuation\">)</span>\n\n@parameterized<span class=\"token punctuation\">.</span>expand<span class=\"token punctuation\">(</span><span class=\"token punctuation\">[</span>\n    <span class=\"token punctuation\">(</span>batch_size<span class=\"token punctuation\">,</span> in_caps<span class=\"token punctuation\">,</span> out_caps<span class=\"token punctuation\">,</span> in_dim<span class=\"token punctuation\">,</span> out_dim<span class=\"token punctuation\">)</span> <span class=\"token keyword\">for</span>\n    batch_size<span class=\"token punctuation\">,</span> in_caps<span class=\"token punctuation\">,</span> out_caps<span class=\"token punctuation\">,</span> in_dim<span class=\"token punctuation\">,</span> out_dim <span class=\"token keyword\">in</span>\n    itertools<span class=\"token punctuation\">.</span>product<span class=\"token punctuation\">(</span><span class=\"token punctuation\">[</span><span class=\"token number\">4</span><span class=\"token punctuation\">,</span> <span class=\"token number\">8</span><span class=\"token punctuation\">]</span><span class=\"token punctuation\">,</span> <span class=\"token punctuation\">[</span><span class=\"token number\">4</span><span class=\"token punctuation\">,</span> <span class=\"token number\">8</span><span class=\"token punctuation\">]</span><span class=\"token punctuation\">,</span> <span class=\"token punctuation\">[</span><span class=\"token number\">4</span><span class=\"token punctuation\">,</span> <span class=\"token number\">8</span><span class=\"token punctuation\">]</span><span class=\"token punctuation\">,</span> <span class=\"token punctuation\">[</span><span class=\"token number\">4</span><span class=\"token punctuation\">,</span> <span class=\"token number\">8</span><span class=\"token punctuation\">]</span><span class=\"token punctuation\">,</span> <span class=\"token punctuation\">[</span><span class=\"token number\">4</span><span class=\"token punctuation\">,</span> <span class=\"token number\">8</span><span class=\"token punctuation\">]</span><span class=\"token punctuation\">)</span>\n<span class=\"token punctuation\">]</span><span class=\"token punctuation\">)</span>\n<span class=\"token keyword\">def</span> <span class=\"token function\">test_capsule_prediction_input_grad</span><span class=\"token punctuation\">(</span>self<span class=\"token punctuation\">,</span> batch_size<span class=\"token punctuation\">,</span> in_caps<span class=\"token punctuation\">,</span> out_caps<span class=\"token punctuation\">,</span>\n                                       in_dim<span class=\"token punctuation\">,</span> out_dim<span class=\"token punctuation\">)</span><span class=\"token punctuation\">:</span>\n    <span class=\"token triple-quoted-string string\">\"\"\" Tests gradient of output w.r.t. x \"\"\"</span>\n    x <span class=\"token operator\">=</span> np<span class=\"token punctuation\">.</span>random<span class=\"token punctuation\">.</span>rand<span class=\"token punctuation\">(</span>batch_size<span class=\"token punctuation\">,</span> in_caps<span class=\"token punctuation\">,</span> in_dim<span class=\"token punctuation\">)</span>\n    weights <span class=\"token operator\">=</span> np<span class=\"token punctuation\">.</span>random<span class=\"token punctuation\">.</span>rand<span class=\"token punctuation\">(</span>in_caps<span class=\"token punctuation\">,</span> out_caps<span class=\"token punctuation\">,</span> out_dim<span class=\"token punctuation\">,</span> in_dim<span class=\"token punctuation\">)</span>\n    out_shape <span class=\"token operator\">=</span> <span class=\"token punctuation\">(</span>batch_size<span class=\"token punctuation\">,</span> in_caps<span class=\"token punctuation\">,</span> out_caps<span class=\"token punctuation\">,</span> out_dim<span class=\"token punctuation\">)</span>\n\n    <span class=\"token keyword\">with</span> self<span class=\"token punctuation\">.</span>test_session<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">:</span>\n        x_ph <span class=\"token operator\">=</span> tf<span class=\"token punctuation\">.</span>placeholder<span class=\"token punctuation\">(</span>tf<span class=\"token punctuation\">.</span>float32<span class=\"token punctuation\">,</span> x<span class=\"token punctuation\">.</span>shape<span class=\"token punctuation\">)</span>\n        w_ph <span class=\"token operator\">=</span> tf<span class=\"token punctuation\">.</span>placeholder<span class=\"token punctuation\">(</span>tf<span class=\"token punctuation\">.</span>float32<span class=\"token punctuation\">,</span> weights<span class=\"token punctuation\">.</span>shape<span class=\"token punctuation\">)</span>\n        fd <span class=\"token operator\">=</span> <span class=\"token punctuation\">{</span>x_ph<span class=\"token punctuation\">:</span> x<span class=\"token punctuation\">,</span> w_ph<span class=\"token punctuation\">:</span> weights<span class=\"token punctuation\">}</span>\n        caps_out <span class=\"token operator\">=</span> capsule_prediction<span class=\"token punctuation\">(</span>x_ph<span class=\"token punctuation\">,</span> w_ph<span class=\"token punctuation\">)</span>\n        grad_x <span class=\"token operator\">=</span> tf<span class=\"token punctuation\">.</span>test<span class=\"token punctuation\">.</span>compute_gradient<span class=\"token punctuation\">(</span>\n            x_ph<span class=\"token punctuation\">,</span> x<span class=\"token punctuation\">.</span>shape<span class=\"token punctuation\">,</span> caps_out<span class=\"token punctuation\">,</span> out_shape<span class=\"token punctuation\">,</span> extra_feed_dict<span class=\"token operator\">=</span>fd\n        <span class=\"token punctuation\">)</span>\n\n    self<span class=\"token punctuation\">.</span>assertAllClose<span class=\"token punctuation\">(</span>grad_x<span class=\"token punctuation\">[</span><span class=\"token number\">0</span><span class=\"token punctuation\">]</span><span class=\"token punctuation\">,</span> grad_x<span class=\"token punctuation\">[</span><span class=\"token number\">1</span><span class=\"token punctuation\">]</span><span class=\"token punctuation\">,</span> atol<span class=\"token operator\">=</span><span class=\"token number\">1e</span><span class=\"token operator\">-</span><span class=\"token number\">3</span><span class=\"token punctuation\">,</span> rtol<span class=\"token operator\">=</span><span class=\"token number\">1e</span><span class=\"token operator\">-</span><span class=\"token number\">3</span><span class=\"token punctuation\">)</span></code></pre></div>\n<p>The <code class=\"language-text\">tf.test.compute_gradient</code> function determines the ‘theoretical’ and numerical gradient respectively. The numerical gradient is computed by finite differences whereas the theoretical gradient is computed by our Op’s registered gradient. They should be nearly equal, so we assert they are close by using the <code class=\"language-text\">assertAllClose</code> method that is inherited from <code class=\"language-text\">tf.test.TestCase</code>. Here is the resulting output:</p>\n<p><figure class=\"gatsby-resp-image-figure\" style=\"\">\n    <a\n    class=\"gatsby-resp-image-link\"\n    href=\"/static/fa40c81157c033dbb558aac1670f1e9d/2aa68/test-output.jpg\"\n    style=\"display: block\"\n    target=\"_blank\"\n    rel=\"noopener\"\n  >\n    <span\n    class=\"gatsby-resp-image-wrapper\"\n    style=\"position: relative; display: block; margin-left: auto; margin-right: auto;  max-width: 800px;\"\n  >\n    <span\n      class=\"gatsby-resp-image-background-image\"\n      style=\"padding-bottom: 18.75%; position: relative; bottom: 0; left: 0; background-image: url('data:image/jpeg;base64,/9j/2wBDABALDA4MChAODQ4SERATGCgaGBYWGDEjJR0oOjM9PDkzODdASFxOQERXRTc4UG1RV19iZ2hnPk1xeXBkeFxlZ2P/2wBDARESEhgVGC8aGi9jQjhCY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2P/wgARCAAEABQDASIAAhEBAxEB/8QAFwABAAMAAAAAAAAAAAAAAAAAAAECBf/EABUBAQEAAAAAAAAAAAAAAAAAAAAB/9oADAMBAAIQAxAAAAHGqSQD/8QAFBABAAAAAAAAAAAAAAAAAAAAEP/aAAgBAQABBQJ//8QAFBEBAAAAAAAAAAAAAAAAAAAAEP/aAAgBAwEBPwE//8QAFBEBAAAAAAAAAAAAAAAAAAAAEP/aAAgBAgEBPwE//8QAFBABAAAAAAAAAAAAAAAAAAAAEP/aAAgBAQAGPwJ//8QAGRAAAgMBAAAAAAAAAAAAAAAAAAERMUFx/9oACAEBAAE/Ib1j6yD/2gAMAwEAAgADAAAAEIwv/8QAFBEBAAAAAAAAAAAAAAAAAAAAEP/aAAgBAwEBPxA//8QAFBEBAAAAAAAAAAAAAAAAAAAAEP/aAAgBAgEBPxA//8QAGxAAAgEFAAAAAAAAAAAAAAAAAAERQVFxkeH/2gAIAQEAAT8Qh0GBH3I3ez//2Q=='); background-size: cover; display: block;\"\n    ></span>\n    <img\n        class=\"gatsby-resp-image-image\"\n        style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;box-shadow:inset 0px 0px 0px 400px white;\"\n        alt=\"<i><center>Console output for gradient tests.</center></i>\"\n        title=\"\"\n        src=\"/static/fa40c81157c033dbb558aac1670f1e9d/0ae04/test-output.jpg\"\n        srcset=\"/static/fa40c81157c033dbb558aac1670f1e9d/122c5/test-output.jpg 200w,\n/static/fa40c81157c033dbb558aac1670f1e9d/b9af0/test-output.jpg 400w,\n/static/fa40c81157c033dbb558aac1670f1e9d/0ae04/test-output.jpg 800w,\n/static/fa40c81157c033dbb558aac1670f1e9d/2aa68/test-output.jpg 928w\"\n        sizes=\"(max-width: 800px) 100vw, 800px\"\n      />\n  </span>\n  </a>\n    <figcaption class=\"gatsby-resp-image-figcaption\"><i><center>Console output for gradient tests.</center></i></figcaption>\n  </figure></p>\n<p>In my previous blog post, I have already discussed a capsule network for MNIST classification. We can now insert our <code class=\"language-text\">capsule_prediction</code> function in the code:</p>\n<div class=\"gatsby-highlight has-highlighted-lines\" data-language=\"python\"><pre class=\"language-python\"><code class=\"language-python\"><span class=\"token keyword\">def</span> <span class=\"token function\">digit_caps</span><span class=\"token punctuation\">(</span>incoming<span class=\"token punctuation\">,</span> n_digit_caps<span class=\"token punctuation\">,</span> dim_digit_caps<span class=\"token punctuation\">,</span> name<span class=\"token operator\">=</span><span class=\"token string\">\"DigitCaps\"</span><span class=\"token punctuation\">,</span>\n               neuron_axis<span class=\"token operator\">=</span><span class=\"token operator\">-</span><span class=\"token number\">1</span><span class=\"token punctuation\">,</span> capsule_axis<span class=\"token operator\">=</span><span class=\"token operator\">-</span><span class=\"token number\">2</span><span class=\"token punctuation\">,</span> routing_iters<span class=\"token operator\">=</span><span class=\"token number\">3</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">:</span>\n    <span class=\"token triple-quoted-string string\">\"\"\" Digit capsule layer \"\"\"</span>\n    <span class=\"token keyword\">with</span> tf<span class=\"token punctuation\">.</span>variable_scope<span class=\"token punctuation\">(</span>name<span class=\"token punctuation\">)</span><span class=\"token punctuation\">:</span>\n        <span class=\"token comment\"># Get number of capsules and dimensionality of previous layer</span>\n        in_shape <span class=\"token operator\">=</span> incoming<span class=\"token punctuation\">.</span>shape<span class=\"token punctuation\">.</span>as_list<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span>\n        n_primary_caps <span class=\"token operator\">=</span> in_shape<span class=\"token punctuation\">[</span>capsule_axis<span class=\"token punctuation\">]</span>\n        dim_primary_caps <span class=\"token operator\">=</span> in_shape<span class=\"token punctuation\">[</span>neuron_axis<span class=\"token punctuation\">]</span>\n        <span class=\"token comment\"># Initialize all weight matrices</span>\n        w_shape <span class=\"token operator\">=</span> <span class=\"token punctuation\">[</span>n_primary_caps<span class=\"token punctuation\">,</span> n_digit_caps<span class=\"token punctuation\">,</span> dim_digit_caps<span class=\"token punctuation\">,</span> dim_primary_caps<span class=\"token punctuation\">]</span>\\\n            <span class=\"token keyword\">if</span> args<span class=\"token punctuation\">.</span>custom_op \\\n            <span class=\"token keyword\">else</span> <span class=\"token punctuation\">[</span>n_primary_caps<span class=\"token punctuation\">,</span> n_digit_caps <span class=\"token operator\">*</span> dim_digit_caps<span class=\"token punctuation\">,</span> dim_primary_caps<span class=\"token punctuation\">]</span>\n\n        W_ij <span class=\"token operator\">=</span> tf<span class=\"token punctuation\">.</span>get_variable<span class=\"token punctuation\">(</span>\n            <span class=\"token string\">\"weights\"</span><span class=\"token punctuation\">,</span> shape<span class=\"token operator\">=</span>w_shape<span class=\"token punctuation\">,</span>\n            initializer<span class=\"token operator\">=</span>tf<span class=\"token punctuation\">.</span>keras<span class=\"token punctuation\">.</span>initializers<span class=\"token punctuation\">.</span>glorot_uniform<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span>\n        <span class=\"token punctuation\">)</span>\n        <span class=\"token comment\"># Initialize routing logits, the leading axis with size 1 is added for</span>\n        <span class=\"token comment\"># convenience.</span>\n        b_ij <span class=\"token operator\">=</span> tf<span class=\"token punctuation\">.</span>get_variable<span class=\"token punctuation\">(</span>\n            <span class=\"token string\">\"logits\"</span><span class=\"token punctuation\">,</span> shape<span class=\"token operator\">=</span><span class=\"token punctuation\">[</span><span class=\"token number\">1</span><span class=\"token punctuation\">,</span> n_primary_caps<span class=\"token punctuation\">,</span> n_digit_caps<span class=\"token punctuation\">]</span><span class=\"token punctuation\">,</span>\n            initializer<span class=\"token operator\">=</span>tf<span class=\"token punctuation\">.</span>zeros_initializer<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">,</span> trainable<span class=\"token operator\">=</span>args<span class=\"token punctuation\">.</span>logits_trainable\n        <span class=\"token punctuation\">)</span>\n<span class=\"gatsby-highlight-code-line\">        <span class=\"token keyword\">if</span> args<span class=\"token punctuation\">.</span>custom_op<span class=\"token punctuation\">:</span></span><span class=\"gatsby-highlight-code-line\">            <span class=\"token comment\"># Custom op</span></span><span class=\"gatsby-highlight-code-line\">            u_hat <span class=\"token operator\">=</span> capsule_prediction<span class=\"token punctuation\">(</span>incoming<span class=\"token punctuation\">,</span> W_ij<span class=\"token punctuation\">)</span></span>        <span class=\"token keyword\">else</span><span class=\"token punctuation\">:</span>\n            <span class=\"token comment\"># Reshape and transpose hacking</span>\n            u_i <span class=\"token operator\">=</span> tf<span class=\"token punctuation\">.</span>transpose<span class=\"token punctuation\">(</span>incoming<span class=\"token punctuation\">,</span> <span class=\"token punctuation\">(</span><span class=\"token number\">1</span><span class=\"token punctuation\">,</span> <span class=\"token number\">2</span><span class=\"token punctuation\">,</span> <span class=\"token number\">0</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span>\n            u_hat <span class=\"token operator\">=</span> tf<span class=\"token punctuation\">.</span>matmul<span class=\"token punctuation\">(</span>W_ij<span class=\"token punctuation\">,</span> u_i<span class=\"token punctuation\">)</span>\n            u_hat <span class=\"token operator\">=</span> tf<span class=\"token punctuation\">.</span>reshape<span class=\"token punctuation\">(</span>\n                tf<span class=\"token punctuation\">.</span>transpose<span class=\"token punctuation\">(</span>u_hat<span class=\"token punctuation\">,</span> <span class=\"token punctuation\">(</span><span class=\"token number\">2</span><span class=\"token punctuation\">,</span> <span class=\"token number\">0</span><span class=\"token punctuation\">,</span> <span class=\"token number\">1</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">,</span>\n                <span class=\"token punctuation\">(</span><span class=\"token operator\">-</span><span class=\"token number\">1</span><span class=\"token punctuation\">,</span> n_primary_caps<span class=\"token punctuation\">,</span> n_digit_caps<span class=\"token punctuation\">,</span> dim_digit_caps<span class=\"token punctuation\">)</span>\n            <span class=\"token punctuation\">)</span>\n\n        <span class=\"token keyword\">def</span> <span class=\"token function\">capsule_out</span><span class=\"token punctuation\">(</span>b_ij<span class=\"token punctuation\">)</span><span class=\"token punctuation\">:</span>\n            <span class=\"token triple-quoted-string string\">\"\"\" Given the logits b_ij, computes the output of this layer. \"\"\"</span>\n            c_ij <span class=\"token operator\">=</span> tf<span class=\"token punctuation\">.</span>nn<span class=\"token punctuation\">.</span>softmax<span class=\"token punctuation\">(</span>b_ij<span class=\"token punctuation\">,</span> axis<span class=\"token operator\">=</span><span class=\"token number\">2</span><span class=\"token punctuation\">)</span>\n            s_j <span class=\"token operator\">=</span> tf<span class=\"token punctuation\">.</span>reduce_sum<span class=\"token punctuation\">(</span>\n                tf<span class=\"token punctuation\">.</span>reshape<span class=\"token punctuation\">(</span>c_ij<span class=\"token punctuation\">,</span> <span class=\"token punctuation\">(</span><span class=\"token operator\">-</span><span class=\"token number\">1</span><span class=\"token punctuation\">,</span> n_primary_caps<span class=\"token punctuation\">,</span> n_digit_caps<span class=\"token punctuation\">,</span> <span class=\"token number\">1</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span> <span class=\"token operator\">*</span> u_hat<span class=\"token punctuation\">,</span>\n                axis<span class=\"token operator\">=</span><span class=\"token number\">1</span>\n            <span class=\"token punctuation\">)</span>\n            v_j <span class=\"token operator\">=</span> squash<span class=\"token punctuation\">(</span>s_j<span class=\"token punctuation\">)</span>\n            <span class=\"token keyword\">return</span> v_j\n\n        <span class=\"token keyword\">def</span> <span class=\"token function\">routing_iteration</span><span class=\"token punctuation\">(</span><span class=\"token builtin\">iter</span><span class=\"token punctuation\">,</span> logits<span class=\"token punctuation\">)</span><span class=\"token punctuation\">:</span>\n            <span class=\"token triple-quoted-string string\">\"\"\"\n            Given a set of logits, computes the new logits using the routing\n            definition from the paper.\n            \"\"\"</span>\n            v_j <span class=\"token operator\">=</span> capsule_out<span class=\"token punctuation\">(</span>logits<span class=\"token punctuation\">)</span>\n            a_ij <span class=\"token operator\">=</span> tf<span class=\"token punctuation\">.</span>reduce_sum<span class=\"token punctuation\">(</span>tf<span class=\"token punctuation\">.</span>expand_dims<span class=\"token punctuation\">(</span>v_j<span class=\"token punctuation\">,</span> axis<span class=\"token operator\">=</span><span class=\"token number\">1</span><span class=\"token punctuation\">)</span> <span class=\"token operator\">*</span> u_hat<span class=\"token punctuation\">,</span> axis<span class=\"token operator\">=</span><span class=\"token number\">3</span><span class=\"token punctuation\">)</span>\n            logits <span class=\"token operator\">=</span> tf<span class=\"token punctuation\">.</span>reshape<span class=\"token punctuation\">(</span>logits <span class=\"token operator\">+</span> a_ij<span class=\"token punctuation\">,</span> <span class=\"token punctuation\">(</span><span class=\"token operator\">-</span><span class=\"token number\">1</span><span class=\"token punctuation\">,</span> n_primary_caps<span class=\"token punctuation\">,</span> n_digit_caps<span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span>\n            <span class=\"token keyword\">return</span> <span class=\"token punctuation\">[</span><span class=\"token builtin\">iter</span> <span class=\"token operator\">+</span> <span class=\"token number\">1</span><span class=\"token punctuation\">,</span> logits<span class=\"token punctuation\">]</span>\n\n        <span class=\"token comment\"># Compute routing</span>\n        i <span class=\"token operator\">=</span> tf<span class=\"token punctuation\">.</span>constant<span class=\"token punctuation\">(</span><span class=\"token number\">0</span><span class=\"token punctuation\">)</span>\n        routing_result <span class=\"token operator\">=</span> tf<span class=\"token punctuation\">.</span>while_loop<span class=\"token punctuation\">(</span>\n            <span class=\"token keyword\">lambda</span> i<span class=\"token punctuation\">,</span> logits<span class=\"token punctuation\">:</span> tf<span class=\"token punctuation\">.</span>less<span class=\"token punctuation\">(</span>i<span class=\"token punctuation\">,</span> routing_iters<span class=\"token punctuation\">)</span><span class=\"token punctuation\">,</span>\n            routing_iteration<span class=\"token punctuation\">,</span>\n            <span class=\"token punctuation\">[</span>i<span class=\"token punctuation\">,</span> tf<span class=\"token punctuation\">.</span>tile<span class=\"token punctuation\">(</span>b_ij<span class=\"token punctuation\">,</span> tf<span class=\"token punctuation\">.</span>stack<span class=\"token punctuation\">(</span><span class=\"token punctuation\">[</span>tf<span class=\"token punctuation\">.</span>shape<span class=\"token punctuation\">(</span>incoming<span class=\"token punctuation\">)</span><span class=\"token punctuation\">[</span><span class=\"token number\">0</span><span class=\"token punctuation\">]</span><span class=\"token punctuation\">,</span> <span class=\"token number\">1</span><span class=\"token punctuation\">,</span> <span class=\"token number\">1</span><span class=\"token punctuation\">]</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">]</span>\n        <span class=\"token punctuation\">)</span>\n        <span class=\"token comment\"># Second element of the result contains our final logits</span>\n        v_j <span class=\"token operator\">=</span> capsule_out<span class=\"token punctuation\">(</span>routing_result<span class=\"token punctuation\">[</span><span class=\"token number\">1</span><span class=\"token punctuation\">]</span><span class=\"token punctuation\">)</span>\n\n    <span class=\"token keyword\">return</span> v_j</code></pre></div>\n<p>So what about performance? Well, it turns out that training with the custom op runs somewhat slower than training with the transpose and reshape hacking. As I’ve stated before, the code could be optimized even more. Thanks for staying around till the end and I would love to hear suggestions and feedback!</p>","frontmatter":{"title":"Cuda, TensorFlow and capsule networks","date":"February 11, 2018","description":"A complete demonstration of developing a custom TensorFlow Op with GPU support."}}},"pageContext":{"isCreatedByStatefulCreatePages":false,"slug":"/capsnet/","previous":null,"next":{"fields":{"slug":"/spn01/"},"frontmatter":{"title":"Sum-Product Networks"}}}}